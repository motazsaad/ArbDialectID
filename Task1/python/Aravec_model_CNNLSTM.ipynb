{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "import loading\n",
    "import process_aravec\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop,Adam\n",
    "from keras.layers import Input,Embedding, Conv1D,MaxPooling1D,GlobalMaxPooling1D, Dense,Dropout,LSTM,Flatten,GRU,Bidirectional,GlobalAveragePooling1D\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns             \n",
    "from keras import regularizers\n",
    "from keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 5000\n",
    "max_len = 200\n",
    "training_samples = 8000  # We will be training on 200 samples\n",
    "validation_samples = 2000  # We will be validating on 10000 samples\n",
    "# Convolution\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 2\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 70\n",
    "#keras model\n",
    "epochs = 10\n",
    "batch_size=50\n",
    "\n",
    "#data directory\n",
    "#data_dir = '../data/SplitedPalSent'\n",
    "#data_dir = '../data/ASTD'\n",
    "#data_dir = '../data/labr5/clean'\n",
    "#data_dir = '../data/labr3'\n",
    "#data_dir = '../data/labr2'\n",
    "#data_dir = '../data/Shami'\n",
    "#train_dir = '../data/D6_26/6dialects/splited_train/BEI'\n",
    "#test_dir = '../data/D6_26/6dialects/splited_dev/BEI'\n",
    "train_dir = '../data/Dialect6/Multi_data/train/pre_clean'\n",
    "test_dir = '../data/Dialect6/Multi_data/dev/pre_clean'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/Dialect6/Multi_data/train/pre_clean/train\n",
      "Found 39818 unique tokens.\n",
      "Shape of data tensor: (54000, 200)\n",
      "Shape of label tensor: (54000,)\n",
      "54000\n",
      "[0. 0. 1. 0. 0. 0.]\n",
      "[  48  795 2375    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "import loading\n",
    "input_train, y_train  = loading.load_train(train_dir,max_len,training_samples,validation_samples,max_features, Validation = False, binary = False )\n",
    "input_test, y_test = loading.load_test(test_dir,max_len,max_features,binary = False )\n",
    "print(len(input_train))\n",
    "print((y_train[10]))\n",
    "print(input_test[1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 331679 word vectors.\n",
      "0.987\n"
     ]
    }
   ],
   "source": [
    "def embedding_matrix(data_dir,max_len,max_features):\n",
    "    ara_dir = '../data/tweet_cbow_300/'\n",
    "    word_index = loading.word_index(data_dir,max_len,max_features)\n",
    "    t_model = Word2Vec.load(os.path.join(ara_dir,'tweets_cbow_300'))\n",
    "\n",
    "    print('Found %s word vectors.' % len(t_model.wv.index2word))# how many words in aravec this model\n",
    "    embeddings_index = t_model.wv\n",
    "\n",
    "    embedding_dim = embeddings_index.vector_size #300\n",
    "    embedding_matrix = np.zeros((max_features, embedding_dim))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        word = process_aravec.clean_str(word).replace(\" \", \"_\")\n",
    "        check = False\n",
    "        if word in embeddings_index:\n",
    "            w = word\n",
    "            check = True\n",
    "        elif word[1:] in embeddings_index:\n",
    "            w= word[1:]\n",
    "            check = True\n",
    "        elif word[:-2] in embeddings_index:\n",
    "            w= word[:-2]\n",
    "            check = True\n",
    "        if check:\n",
    "            embedding_vector = embeddings_index[w]\n",
    "            if i < max_features:\n",
    "                if embedding_vector is not None:\n",
    "                    # Words not found in embedding index will be all-zeros.\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "                    #print(len(embedding_vector))\n",
    "       \n",
    "        \n",
    "    return  embedding_matrix  \n",
    "embedding_matrix = embedding_matrix(train_dir,max_len,max_features)\n",
    "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
    "print(nonzero_elements / max_features)\n",
    "#for dialetct PAl it only covers 35% of the vocabulary\n",
    "# for MSA it covers 98%\n",
    "#ASTD . 90%\n",
    "#shami 67%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#building model CNN + LSTM\n",
    "#tried on ASTD \n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 100,weights=[embedding_matrix], trainable=True, input_length=max_len))\n",
    "model.add(LSTM(70,dropout=0.5, recurrent_dropout=0.5))#,return_sequences=True)))\n",
    "#model.add(LSTM(64,dropout=0.5, recurrent_dropout=0.5,return_sequences=False))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(30))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(3,activation='sigmoid'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics =['acc'])\n",
    "#model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 100,weights=[embedding_matrix], trainable=True, input_length=max_len))\n",
    "#model.add(Flatten())\n",
    "#model.add(Conv1D(100, 10, activation='relu'))\n",
    "model.add(Conv1D(100, 10, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Conv1D(160, 10, activation='relu'))\n",
    "#model.add(Conv1D(160, 10, activation='relu'))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics =['acc'])\n",
    "#model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_71 (Dense)             (None, 500)               100500    \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 300)               150300    \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 200)               60200     \n",
      "_________________________________________________________________\n",
      "dropout_55 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 6)                 1206      \n",
      "=================================================================\n",
      "Total params: 312,206\n",
      "Trainable params: 312,206\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# Note: glorot_uniform is the Xavier uniform initializer.\n",
    "#model.add(Embedding(max_features, 300,weights=[embedding_matrix], trainable=True, input_length=max_len)) \n",
    "model.add(Dense(500,input_dim=max_len, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(300, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(200, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\"))\n",
    "model.add(Dropout(0.5))\n",
    "#model.add(Flatten())\n",
    "model.add(Dense(6, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\"))\n",
    "model_optimizer = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=model_optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MADAR\n",
    "model=Sequential()\n",
    "model.add(Embedding(max_features, 300,weights=[embedding_matrix], trainable=True, input_length=max_len)) \n",
    "model.add(Bidirectional(LSTM(128,dropout=0.5, recurrent_dropout=0.5,return_sequences=False)))\n",
    "#model.add(LSTM(64,dropout=0.5, recurrent_dropout=0.5,return_sequences=False))\n",
    "model.add(Dense(50,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(6,activation='sigmoid'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics =['acc'])\n",
    "#model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 300,weights=[embedding_matrix], trainable=True, input_length=max_len)) \n",
    "model.add(Dense(500,kernel_initializer=\"glorot_uniform\",activation=\"sigmoid\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(300,kernel_initializer=\"glorot_uniform\",activation=\"sigmoid\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100,kernel_initializer=\"glorot_uniform\",activation=\"sigmoid\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(6,kernel_initializer=\"glorot_uniform\",activation=\"sigmoid\"))\n",
    "model_optimizer = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=model_optimizer,\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM  EXP 16\n",
    "model=Sequential()\n",
    "model.add(Embedding(max_features, 300,weights=[embedding_matrix], trainable=True, input_length=max_len)) \n",
    "model.add(Bidirectional(LSTM(128,dropout=0.5, recurrent_dropout=0.5,return_sequences=True)))\n",
    "model.add(LSTM(64,dropout=0.5, recurrent_dropout=0.5,return_sequences=False))\n",
    "model.add(Dense(100,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(6,activation='sigmoid'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics =['acc'])\n",
    "#model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN_19,2\n",
    "\n",
    "\n",
    "def create_convnet(max_features, vectore_d):\n",
    "    input_shape = Input(shape=(max_features, vectore_d))\n",
    "    conv = []\n",
    "    for filters in [32,64,128]:\n",
    "        for k_size in [2,3,4,5,6]:\n",
    "            tower = Conv1D(filters, k_size, activation='relu')(input_shape)\n",
    "            tower = GlobalMaxPooling1D()(tower)\n",
    "            conv.append(tower)\n",
    "    \n",
    "    merged = layers.concatenate([tower for tower in conv], axis=1)\n",
    "   \n",
    "    \n",
    "    out = Dense(10,activation='relu')(merged)\n",
    "    #out = Dropout(0.3)(out)\n",
    "    #out = Dense(30)(out)\n",
    "    out = Dense(6, activation='sigmoid')(out)\n",
    "\n",
    "    model = Model(input_shape, out)\n",
    "    #print(model.summary())\n",
    "    from keras.utils.vis_utils import plot_model\n",
    "    #plot_model(model, to_file=filepath+'.png', show_shapes=True, show_layer_names=True)\n",
    "    return model\n",
    "\n",
    "\n",
    "input_shape =(max_features, )\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 300,weights=[embedding_matrix], trainable=True, input_length=max_len)) \n",
    "model.add(Bidirectional(LSTM(128,dropout=0.5, recurrent_dropout=0.5,return_sequences= True)))\n",
    "model.add(Bidirectional(LSTM(64,dropout=0.5, recurrent_dropout=0.5,return_sequences= True)))\n",
    "model.add(create_convnet(max_len, 128))\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics =['acc'])\n",
    "#model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#EXP LSTM 9+17\n",
    "model=Sequential()\n",
    "model.add(Embedding(max_features, 300,weights=[embedding_matrix], trainable=True, input_length=max_len)) \n",
    "model.add(Bidirectional(LSTM(128,dropout=0.5, recurrent_dropout=0.5,return_sequences=True)))\n",
    "model.add(LSTM(64,dropout=0.5, recurrent_dropout=0.5,return_sequences=False))\n",
    "model.add(Dense(100,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#building model CNN + LSTM kagg\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 300,weights=[embedding_matrix], trainable=True, input_length=max_len)) \n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1))\n",
    "model.add(MaxPooling1D(pool_size=pool_size))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(32, 3, padding='valid', activation='relu', strides=1))\n",
    "#model.add(Flatten())\n",
    "model.add(LSTM(lstm_output_size, dropout = 0.3, recurrent_dropout=0.3))\n",
    "model.add(Dense(3,activation='sigmoid'))\n",
    "#model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics =['acc'])\n",
    "print(model.summary())   \n",
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, to_file='weights/kaggle.png', show_shapes=True, show_layer_names=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 43200 samples, validate on 10800 samples\n",
      "Epoch 1/100\n",
      "43200/43200 [==============================] - 10s 231us/step - loss: 1.7921 - acc: 0.1761 - val_loss: 1.7883 - val_acc: 0.1888\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.78834, saving model to weights.best.madar_embediing.hdf5\n",
      "Epoch 2/100\n",
      "43200/43200 [==============================] - 7s 161us/step - loss: 1.7880 - acc: 0.1878 - val_loss: 1.7855 - val_acc: 0.1965\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.78834 to 1.78545, saving model to weights.best.madar_embediing.hdf5\n",
      "Epoch 3/100\n",
      "43200/43200 [==============================] - 7s 169us/step - loss: 1.7867 - acc: 0.1912 - val_loss: 1.7845 - val_acc: 0.1918\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.78545 to 1.78447, saving model to weights.best.madar_embediing.hdf5\n",
      "Epoch 4/100\n",
      "43200/43200 [==============================] - 7s 173us/step - loss: 1.7850 - acc: 0.1966 - val_loss: 1.7834 - val_acc: 0.1934\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.78447 to 1.78341, saving model to weights.best.madar_embediing.hdf5\n",
      "Epoch 5/100\n",
      "43200/43200 [==============================] - 7s 166us/step - loss: 1.7842 - acc: 0.1979 - val_loss: 1.7836 - val_acc: 0.1923\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.78341\n",
      "Epoch 6/100\n",
      "43200/43200 [==============================] - 7s 163us/step - loss: 1.7838 - acc: 0.1968 - val_loss: 1.7831 - val_acc: 0.1952\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.78341 to 1.78314, saving model to weights.best.madar_embediing.hdf5\n",
      "Epoch 7/100\n",
      "43200/43200 [==============================] - 8s 188us/step - loss: 1.7829 - acc: 0.1978 - val_loss: 1.7824 - val_acc: 0.1992\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.78314 to 1.78241, saving model to weights.best.madar_embediing.hdf5\n",
      "Epoch 8/100\n",
      "43200/43200 [==============================] - 8s 175us/step - loss: 1.7828 - acc: 0.1996 - val_loss: 1.7824 - val_acc: 0.1970\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.78241 to 1.78236, saving model to weights.best.madar_embediing.hdf5\n",
      "Epoch 9/100\n",
      "43200/43200 [==============================] - 8s 177us/step - loss: 1.7821 - acc: 0.1991 - val_loss: 1.7822 - val_acc: 0.1980\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.78236 to 1.78224, saving model to weights.best.madar_embediing.hdf5\n",
      "Epoch 10/100\n",
      "43200/43200 [==============================] - 8s 179us/step - loss: 1.7821 - acc: 0.1990 - val_loss: 1.7831 - val_acc: 0.2013\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.78224\n",
      "Epoch 11/100\n",
      "43200/43200 [==============================] - 8s 175us/step - loss: 1.7820 - acc: 0.1975 - val_loss: 1.7823 - val_acc: 0.1988\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.78224\n",
      "Epoch 12/100\n",
      "43200/43200 [==============================] - 8s 180us/step - loss: 1.7818 - acc: 0.1967 - val_loss: 1.7813 - val_acc: 0.1957\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.78224 to 1.78128, saving model to weights.best.madar_embediing.hdf5\n",
      "Epoch 13/100\n",
      "43200/43200 [==============================] - 8s 184us/step - loss: 1.7822 - acc: 0.1988 - val_loss: 1.7813 - val_acc: 0.2010\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.78128\n",
      "Epoch 14/100\n",
      "43200/43200 [==============================] - 7s 157us/step - loss: 1.7810 - acc: 0.1992 - val_loss: 1.7821 - val_acc: 0.1996\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.78128\n",
      "Epoch 15/100\n",
      "43200/43200 [==============================] - 7s 158us/step - loss: 1.7817 - acc: 0.1997 - val_loss: 1.7816 - val_acc: 0.2003\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.78128\n",
      "Epoch 16/100\n",
      "43200/43200 [==============================] - 6s 144us/step - loss: 1.7813 - acc: 0.1995 - val_loss: 1.7814 - val_acc: 0.2006\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.78128\n",
      "Epoch 17/100\n",
      "43200/43200 [==============================] - 7s 151us/step - loss: 1.7811 - acc: 0.2007 - val_loss: 1.7810 - val_acc: 0.1985\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.78128 to 1.78103, saving model to weights.best.madar_embediing.hdf5\n",
      "Epoch 18/100\n",
      "43200/43200 [==============================] - 8s 196us/step - loss: 1.7806 - acc: 0.1998 - val_loss: 1.7810 - val_acc: 0.2000\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.78103 to 1.78099, saving model to weights.best.madar_embediing.hdf5\n",
      "Epoch 19/100\n",
      "43200/43200 [==============================] - 7s 159us/step - loss: 1.7806 - acc: 0.2022 - val_loss: 1.7814 - val_acc: 0.2009\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.78099\n",
      "Epoch 20/100\n",
      "43200/43200 [==============================] - 6s 137us/step - loss: 1.7806 - acc: 0.2014 - val_loss: 1.7820 - val_acc: 0.2000\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.78099\n",
      "Epoch 21/100\n",
      "43200/43200 [==============================] - 6s 136us/step - loss: 1.7809 - acc: 0.1991 - val_loss: 1.7809 - val_acc: 0.2012\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.78099 to 1.78090, saving model to weights.best.madar_embediing.hdf5\n",
      "Epoch 22/100\n",
      "43200/43200 [==============================] - 11s 246us/step - loss: 1.7794 - acc: 0.2036 - val_loss: 1.7803 - val_acc: 0.2005\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.78090 to 1.78028, saving model to weights.best.madar_embediing.hdf5\n",
      "Epoch 23/100\n",
      "43200/43200 [==============================] - 11s 263us/step - loss: 1.7799 - acc: 0.1980 - val_loss: 1.7812 - val_acc: 0.1993\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.78028\n",
      "Epoch 24/100\n",
      "43200/43200 [==============================] - 9s 218us/step - loss: 1.7798 - acc: 0.2000 - val_loss: 1.7812 - val_acc: 0.2011\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.78028\n",
      "Epoch 25/100\n",
      "43200/43200 [==============================] - 10s 235us/step - loss: 1.7805 - acc: 0.1999 - val_loss: 1.7802 - val_acc: 0.2039\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.78028 to 1.78025, saving model to weights.best.madar_embediing.hdf5\n",
      "Epoch 26/100\n",
      "43200/43200 [==============================] - 7s 167us/step - loss: 1.7795 - acc: 0.2044 - val_loss: 1.7799 - val_acc: 0.2007\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.78025 to 1.77990, saving model to weights.best.madar_embediing.hdf5\n",
      "Epoch 27/100\n",
      "43200/43200 [==============================] - 8s 177us/step - loss: 1.7795 - acc: 0.2018 - val_loss: 1.7801 - val_acc: 0.2031\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.77990\n",
      "Epoch 28/100\n",
      "43200/43200 [==============================] - 7s 169us/step - loss: 1.7796 - acc: 0.2004 - val_loss: 1.7793 - val_acc: 0.2033\n",
      "\n",
      "Epoch 00028: val_loss improved from 1.77990 to 1.77933, saving model to weights.best.madar_embediing.hdf5\n",
      "Epoch 29/100\n",
      "43200/43200 [==============================] - 9s 215us/step - loss: 1.7799 - acc: 0.2025 - val_loss: 1.7790 - val_acc: 0.2026\n",
      "\n",
      "Epoch 00029: val_loss improved from 1.77933 to 1.77899, saving model to weights.best.madar_embediing.hdf5\n",
      "Epoch 30/100\n",
      "43200/43200 [==============================] - 11s 244us/step - loss: 1.7795 - acc: 0.2017 - val_loss: 1.7797 - val_acc: 0.1997\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.77899\n",
      "Epoch 31/100\n",
      "43200/43200 [==============================] - 9s 201us/step - loss: 1.7793 - acc: 0.2018 - val_loss: 1.7789 - val_acc: 0.2018\n",
      "\n",
      "Epoch 00031: val_loss improved from 1.77899 to 1.77893, saving model to weights.best.madar_embediing.hdf5\n",
      "Epoch 32/100\n",
      "43200/43200 [==============================] - 8s 182us/step - loss: 1.7801 - acc: 0.2000 - val_loss: 1.7797 - val_acc: 0.2002\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.77893\n",
      "Epoch 33/100\n",
      "43200/43200 [==============================] - 8s 184us/step - loss: 1.7792 - acc: 0.2040 - val_loss: 1.7790 - val_acc: 0.2002\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.77893\n",
      "Epoch 34/100\n",
      "43200/43200 [==============================] - 9s 200us/step - loss: 1.7794 - acc: 0.2043 - val_loss: 1.7791 - val_acc: 0.2061\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.77893\n",
      "Epoch 35/100\n",
      "43200/43200 [==============================] - 10s 243us/step - loss: 1.7785 - acc: 0.2026 - val_loss: 1.7783 - val_acc: 0.2045\n",
      "\n",
      "Epoch 00035: val_loss improved from 1.77893 to 1.77832, saving model to weights.best.madar_embediing.hdf5\n",
      "Epoch 36/100\n",
      "43200/43200 [==============================] - 9s 219us/step - loss: 1.7786 - acc: 0.2032 - val_loss: 1.7789 - val_acc: 0.2015\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1.77832\n",
      "Epoch 37/100\n",
      "43200/43200 [==============================] - 6s 143us/step - loss: 1.7786 - acc: 0.2005 - val_loss: 1.7784 - val_acc: 0.2047\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.77832\n",
      "Epoch 38/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43200/43200 [==============================] - 6s 149us/step - loss: 1.7788 - acc: 0.2038 - val_loss: 1.7775 - val_acc: 0.2034\n",
      "\n",
      "Epoch 00038: val_loss improved from 1.77832 to 1.77753, saving model to weights.best.madar_embediing.hdf5\n",
      "Epoch 39/100\n",
      "43200/43200 [==============================] - 9s 198us/step - loss: 1.7782 - acc: 0.2056 - val_loss: 1.7783 - val_acc: 0.2010\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.77753\n",
      "Epoch 40/100\n",
      "43200/43200 [==============================] - 6s 144us/step - loss: 1.7781 - acc: 0.2046 - val_loss: 1.7786 - val_acc: 0.2057\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1.77753\n",
      "Epoch 41/100\n",
      "43200/43200 [==============================] - 8s 175us/step - loss: 1.7786 - acc: 0.2026 - val_loss: 1.7781 - val_acc: 0.2037\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 1.77753\n",
      "Epoch 42/100\n",
      "43200/43200 [==============================] - 12s 288us/step - loss: 1.7783 - acc: 0.2032 - val_loss: 1.7789 - val_acc: 0.2032\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1.77753\n",
      "Epoch 43/100\n",
      "43200/43200 [==============================] - 7s 168us/step - loss: 1.7785 - acc: 0.2004 - val_loss: 1.7778 - val_acc: 0.2030\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 1.77753\n",
      "Epoch 44/100\n",
      "43200/43200 [==============================] - 8s 194us/step - loss: 1.7794 - acc: 0.2029 - val_loss: 1.7781 - val_acc: 0.2077\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1.77753\n",
      "Epoch 45/100\n",
      "43200/43200 [==============================] - 9s 198us/step - loss: 1.7780 - acc: 0.2068 - val_loss: 1.7769 - val_acc: 0.2049\n",
      "\n",
      "Epoch 00045: val_loss improved from 1.77753 to 1.77690, saving model to weights.best.madar_embediing.hdf5\n",
      "Epoch 46/100\n",
      "43200/43200 [==============================] - 6s 145us/step - loss: 1.7778 - acc: 0.2051 - val_loss: 1.7777 - val_acc: 0.2055\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1.77690\n",
      "Epoch 47/100\n",
      "43200/43200 [==============================] - 7s 166us/step - loss: 1.7781 - acc: 0.2041 - val_loss: 1.7776 - val_acc: 0.2038\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1.77690\n",
      "Epoch 48/100\n",
      "43200/43200 [==============================] - 7s 167us/step - loss: 1.7773 - acc: 0.2049 - val_loss: 1.7776 - val_acc: 0.2091\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1.77690\n",
      "Epoch 49/100\n",
      "43200/43200 [==============================] - 9s 211us/step - loss: 1.7785 - acc: 0.2050 - val_loss: 1.7774 - val_acc: 0.2091\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 1.77690\n",
      "Epoch 50/100\n",
      "43200/43200 [==============================] - 7s 153us/step - loss: 1.7775 - acc: 0.2060 - val_loss: 1.7775 - val_acc: 0.2030\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 1.77690\n",
      "Epoch 51/100\n",
      "43200/43200 [==============================] - 6s 135us/step - loss: 1.7774 - acc: 0.2040 - val_loss: 1.7772 - val_acc: 0.2089\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 1.77690\n",
      "Epoch 52/100\n",
      "43200/43200 [==============================] - 8s 192us/step - loss: 1.7779 - acc: 0.2025 - val_loss: 1.7776 - val_acc: 0.2073\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 1.77690\n",
      "Epoch 53/100\n",
      "43200/43200 [==============================] - 9s 197us/step - loss: 1.7779 - acc: 0.2055 - val_loss: 1.7778 - val_acc: 0.2042\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 1.77690\n",
      "Epoch 54/100\n",
      "43200/43200 [==============================] - 8s 174us/step - loss: 1.7773 - acc: 0.2055 - val_loss: 1.7771 - val_acc: 0.2062\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 1.77690\n",
      "Epoch 55/100\n",
      "43200/43200 [==============================] - 8s 186us/step - loss: 1.7780 - acc: 0.2078 - val_loss: 1.7782 - val_acc: 0.2036\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 1.77690\n",
      "Epoch 56/100\n",
      "43200/43200 [==============================] - 7s 164us/step - loss: 1.7773 - acc: 0.2060 - val_loss: 1.7773 - val_acc: 0.2060\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 1.77690\n",
      "Epoch 57/100\n",
      "43200/43200 [==============================] - 8s 183us/step - loss: 1.7769 - acc: 0.2059 - val_loss: 1.7782 - val_acc: 0.2028\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 1.77690\n",
      "Epoch 58/100\n",
      "43200/43200 [==============================] - 7s 164us/step - loss: 1.7781 - acc: 0.2056 - val_loss: 1.7778 - val_acc: 0.2051\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 1.77690\n",
      "Epoch 59/100\n",
      "43200/43200 [==============================] - 7s 161us/step - loss: 1.7767 - acc: 0.2063 - val_loss: 1.7772 - val_acc: 0.2052\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 1.77690\n",
      "Epoch 60/100\n",
      "43200/43200 [==============================] - 8s 188us/step - loss: 1.7778 - acc: 0.2054 - val_loss: 1.7776 - val_acc: 0.2059\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 1.77690\n",
      "Epoch 61/100\n",
      "43200/43200 [==============================] - 7s 163us/step - loss: 1.7776 - acc: 0.2050 - val_loss: 1.7761 - val_acc: 0.2071\n",
      "\n",
      "Epoch 00061: val_loss improved from 1.77690 to 1.77612, saving model to weights.best.madar_embediing.hdf5\n",
      "Epoch 62/100\n",
      "43200/43200 [==============================] - 7s 155us/step - loss: 1.7774 - acc: 0.2062 - val_loss: 1.7762 - val_acc: 0.2072\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 1.77612\n",
      "Epoch 63/100\n",
      "43200/43200 [==============================] - 7s 156us/step - loss: 1.7777 - acc: 0.2056 - val_loss: 1.7772 - val_acc: 0.2063\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 1.77612\n",
      "Epoch 64/100\n",
      "43200/43200 [==============================] - 7s 159us/step - loss: 1.7769 - acc: 0.2080 - val_loss: 1.7762 - val_acc: 0.2087\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 1.77612\n",
      "Epoch 65/100\n",
      "43200/43200 [==============================] - 6s 141us/step - loss: 1.7767 - acc: 0.2052 - val_loss: 1.7767 - val_acc: 0.2069\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 1.77612\n",
      "Epoch 66/100\n",
      "43200/43200 [==============================] - 6s 148us/step - loss: 1.7776 - acc: 0.2055 - val_loss: 1.7762 - val_acc: 0.2077\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 1.77612\n",
      "Epoch 67/100\n",
      "43200/43200 [==============================] - 6s 142us/step - loss: 1.7771 - acc: 0.2046 - val_loss: 1.7753 - val_acc: 0.2059\n",
      "\n",
      "Epoch 00067: val_loss improved from 1.77612 to 1.77525, saving model to weights.best.madar_embediing.hdf5\n",
      "Epoch 68/100\n",
      "43200/43200 [==============================] - 6s 142us/step - loss: 1.7766 - acc: 0.2066 - val_loss: 1.7757 - val_acc: 0.2065\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 1.77525\n",
      "Epoch 69/100\n",
      "43200/43200 [==============================] - 6s 136us/step - loss: 1.7757 - acc: 0.2067 - val_loss: 1.7749 - val_acc: 0.2052\n",
      "\n",
      "Epoch 00069: val_loss improved from 1.77525 to 1.77493, saving model to weights.best.madar_embediing.hdf5\n",
      "Epoch 70/100\n",
      "43200/43200 [==============================] - 6s 145us/step - loss: 1.7768 - acc: 0.2064 - val_loss: 1.7751 - val_acc: 0.2066\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 1.77493\n",
      "Epoch 71/100\n",
      "43200/43200 [==============================] - 6s 142us/step - loss: 1.7770 - acc: 0.2054 - val_loss: 1.7761 - val_acc: 0.2045\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 1.77493\n",
      "Epoch 72/100\n",
      "43200/43200 [==============================] - 6s 138us/step - loss: 1.7764 - acc: 0.2096 - val_loss: 1.7754 - val_acc: 0.2073\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 1.77493\n",
      "Epoch 73/100\n",
      "43200/43200 [==============================] - 6s 139us/step - loss: 1.7762 - acc: 0.2084 - val_loss: 1.7755 - val_acc: 0.2088\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 1.77493\n",
      "Epoch 74/100\n",
      "43200/43200 [==============================] - 6s 138us/step - loss: 1.7762 - acc: 0.2060 - val_loss: 1.7759 - val_acc: 0.2041\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 1.77493\n",
      "Epoch 75/100\n",
      "43200/43200 [==============================] - 6s 145us/step - loss: 1.7771 - acc: 0.2049 - val_loss: 1.7757 - val_acc: 0.2061\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 1.77493\n",
      "Epoch 76/100\n",
      "43200/43200 [==============================] - 7s 173us/step - loss: 1.7762 - acc: 0.2054 - val_loss: 1.7765 - val_acc: 0.2101\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 1.77493\n",
      "Epoch 77/100\n",
      "43200/43200 [==============================] - 7s 169us/step - loss: 1.7758 - acc: 0.2084 - val_loss: 1.7751 - val_acc: 0.2064\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 1.77493\n",
      "Epoch 78/100\n",
      "43200/43200 [==============================] - 6s 138us/step - loss: 1.7768 - acc: 0.2091 - val_loss: 1.7760 - val_acc: 0.2031\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 1.77493\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43200/43200 [==============================] - 7s 158us/step - loss: 1.7761 - acc: 0.2071 - val_loss: 1.7760 - val_acc: 0.2080\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 1.77493\n",
      "Epoch 80/100\n",
      "43200/43200 [==============================] - 7s 152us/step - loss: 1.7759 - acc: 0.2060 - val_loss: 1.7756 - val_acc: 0.2089\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 1.77493\n",
      "Epoch 81/100\n",
      "43200/43200 [==============================] - 7s 156us/step - loss: 1.7774 - acc: 0.2051 - val_loss: 1.7761 - val_acc: 0.2059\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 1.77493\n",
      "Epoch 82/100\n",
      "43200/43200 [==============================] - 7s 154us/step - loss: 1.7765 - acc: 0.2088 - val_loss: 1.7764 - val_acc: 0.2093\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 1.77493\n",
      "Epoch 83/100\n",
      "43200/43200 [==============================] - 7s 153us/step - loss: 1.7764 - acc: 0.2076 - val_loss: 1.7763 - val_acc: 0.2056\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 1.77493\n",
      "Epoch 84/100\n",
      "43200/43200 [==============================] - 6s 143us/step - loss: 1.7762 - acc: 0.2068 - val_loss: 1.7757 - val_acc: 0.2074\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 1.77493\n",
      "Epoch 85/100\n",
      "43200/43200 [==============================] - 7s 157us/step - loss: 1.7769 - acc: 0.2090 - val_loss: 1.7762 - val_acc: 0.2081\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 1.77493\n",
      "Epoch 86/100\n",
      "43200/43200 [==============================] - 8s 190us/step - loss: 1.7761 - acc: 0.2091 - val_loss: 1.7755 - val_acc: 0.2060\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 1.77493\n",
      "Epoch 87/100\n",
      "43200/43200 [==============================] - 10s 228us/step - loss: 1.7765 - acc: 0.2083 - val_loss: 1.7759 - val_acc: 0.2090\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 1.77493\n",
      "Epoch 88/100\n",
      "43200/43200 [==============================] - 8s 176us/step - loss: 1.7760 - acc: 0.2073 - val_loss: 1.7751 - val_acc: 0.2067\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 1.77493\n",
      "Epoch 89/100\n",
      "43200/43200 [==============================] - 8s 195us/step - loss: 1.7762 - acc: 0.2094 - val_loss: 1.7747 - val_acc: 0.2065\n",
      "\n",
      "Epoch 00089: val_loss improved from 1.77493 to 1.77471, saving model to weights.best.madar_embediing.hdf5\n",
      "Epoch 90/100\n",
      "43200/43200 [==============================] - 7s 161us/step - loss: 1.7755 - acc: 0.2102 - val_loss: 1.7740 - val_acc: 0.2094\n",
      "\n",
      "Epoch 00090: val_loss improved from 1.77471 to 1.77397, saving model to weights.best.madar_embediing.hdf5\n",
      "Epoch 91/100\n",
      "43200/43200 [==============================] - 6s 143us/step - loss: 1.7755 - acc: 0.2096 - val_loss: 1.7754 - val_acc: 0.2100\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 1.77397\n",
      "Epoch 92/100\n",
      "43200/43200 [==============================] - 11s 246us/step - loss: 1.7757 - acc: 0.2069 - val_loss: 1.7750 - val_acc: 0.2072\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 1.77397\n",
      "Epoch 93/100\n",
      "43200/43200 [==============================] - 13s 309us/step - loss: 1.7750 - acc: 0.2081 - val_loss: 1.7750 - val_acc: 0.2074\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 1.77397\n",
      "Epoch 94/100\n",
      "43200/43200 [==============================] - 8s 188us/step - loss: 1.7755 - acc: 0.2084 - val_loss: 1.7759 - val_acc: 0.2060\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 1.77397\n",
      "Epoch 95/100\n",
      "43200/43200 [==============================] - 8s 175us/step - loss: 1.7750 - acc: 0.2091 - val_loss: 1.7746 - val_acc: 0.2082\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 1.77397\n",
      "Epoch 96/100\n",
      "43200/43200 [==============================] - 7s 159us/step - loss: 1.7753 - acc: 0.2052 - val_loss: 1.7739 - val_acc: 0.2104\n",
      "\n",
      "Epoch 00096: val_loss improved from 1.77397 to 1.77389, saving model to weights.best.madar_embediing.hdf5\n",
      "Epoch 97/100\n",
      "43200/43200 [==============================] - 7s 159us/step - loss: 1.7751 - acc: 0.2068 - val_loss: 1.7744 - val_acc: 0.2100\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 1.77389\n",
      "Epoch 98/100\n",
      "43200/43200 [==============================] - 7s 152us/step - loss: 1.7747 - acc: 0.2102 - val_loss: 1.7749 - val_acc: 0.2081\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 1.77389\n",
      "Epoch 99/100\n",
      "43200/43200 [==============================] - 7s 153us/step - loss: 1.7753 - acc: 0.2081 - val_loss: 1.7742 - val_acc: 0.2125\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 1.77389\n",
      "Epoch 100/100\n",
      "43200/43200 [==============================] - 6s 150us/step - loss: 1.7748 - acc: 0.2081 - val_loss: 1.7744 - val_acc: 0.2105\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 1.77389\n"
     ]
    }
   ],
   "source": [
    "#filepath=\"weights/weights.best.exp_16.hdf5\"\n",
    "\n",
    "#model.load_weights(filepath)\n",
    "filepath=\"weights.best.madar_embediing.hdf5\"\n",
    "\n",
    "#saved_model = filepath\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "#callbacks_list = [checkpoint,early_stopping]\n",
    "history = model.fit(input_train, y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=50,\n",
    "                    #validation_data=(x_val, y_val),\n",
    "                    validation_split= 0.2,\n",
    "                    callbacks=callbacks_list)#[checkpoint])#[early_stopping])\n",
    "\n",
    "\n",
    "model.save_weights(filepath)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 'Testing And Evalaution'\n",
    "1- classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 22.73%\n"
     ]
    }
   ],
   "source": [
    "def plot_show(history):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "#plot_show(history)\n",
    "#filepath=\"weights/weights.best.exp16.hdf5\"\n",
    "\n",
    "model.load_weights(filepath)\n",
    "#y_test.argmax(axis=1), predicte.argmax(axis=1)\n",
    "scores= model.evaluate(input_test, y_test,verbose=0)\n",
    "predicte = model.predict(input_test, verbose = 2, batch_size = batch_size)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "\n",
    "\n",
    "#from keras.utils.vis_utils import plot_model\n",
    "#plot_model(model, to_file=filepath+'.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.49      0.34      1000\n",
      "           1       0.20      0.45      0.27      1000\n",
      "           2       0.17      0.01      0.03      1000\n",
      "           3       0.17      0.02      0.04      1000\n",
      "           4       0.17      0.02      0.03      1000\n",
      "           5       0.20      0.32      0.25      1000\n",
      "\n",
      "   micro avg       0.22      0.22      0.22      6000\n",
      "   macro avg       0.20      0.22      0.16      6000\n",
      "weighted avg       0.20      0.22      0.16      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test and Evaluation\n",
    "#model.load_weights(filepath)\n",
    "#predicted = model.predict_proba(input_test, verbose = 2, batch_size = batch_size)\n",
    "#for binary\n",
    "#print(predicte)\n",
    "#print(metrics.classification_report(y_test, np.round(predicte)))\n",
    "#for multi-class\n",
    "\n",
    "print(metrics.classification_report(np.argmax(y_test,axis=1), np.argmax(predicte,axis=1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21133333333333335\n",
      "Accuracy: 21.13%\n",
      "[[173 146 128 313 100 140]\n",
      " [ 82 281 136 152 163 186]\n",
      " [157 100 163 192 167 221]\n",
      " [224 156 119 206 155 140]\n",
      " [107 186 171 194 182 160]\n",
      " [178  91 130 223 115 263]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#predicted = np.argmax(predicted, axis=1)\n",
    "#scores= model.evaluate(input_test, y_test,verbose=0)\n",
    "#for binary\n",
    "#print(metrics.accuracy_score(y_test, np.round(predicte)))\n",
    "#conf_mat = metrics.confusion_matrix(y_test, np.round(predicte))\n",
    "#for multi\n",
    "print(metrics.accuracy_score(y_test.argmax(axis=1), predicte.argmax(axis=1)))\n",
    "conf_mat = metrics.confusion_matrix(y_test.argmax(axis=1), predicte.argmax(axis=1))\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "print(conf_mat)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "            xticklabels=['MSA', 'BEI', 'DOH', 'RAB', 'CAI', 'TUN'], yticklabels=['MSA', 'BEI', 'DOH', 'RAB', 'CAI', 'TUN'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n",
    "#for x,y,p in zip(input_test,y_test,predicte):\n",
    " #   print(x,y,p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_convnet(max_features, vectore_d):\n",
    "    input_shape = Input(shape=(max_features, vectore_d))\n",
    "    tower_1 = Conv1D(64, 3, activation='relu')(input_shape)\n",
    "    tower_1 = MaxPooling1D(5)(tower_1)\n",
    "   # tower_1 = Flatten()(tower_1)\n",
    "\n",
    "    tower_2 = Conv1D(64, 4, activation='relu')(input_shape)\n",
    "    tower_2 = MaxPooling1D(4)(tower_2)\n",
    "    #tower_2 = Flatten()(tower_2)\n",
    "\n",
    "    tower_3 = Conv1D(64, 5, activation='relu')(input_shape)\n",
    "    tower_3 = MaxPooling1D(3)(tower_3)\n",
    "    #tower_3 = Flatten()(tower_3)\n",
    "    \n",
    "    tower_4 = LSTM(64,dropout=0.5, recurrent_dropout=0.5, return_sequences = True)(input_shape)\n",
    "    tower_5 = LSTM(32,dropout=0.5, recurrent_dropout=0.5,return_sequences= False)(tower_4)\n",
    "    #tower_4 = Dense(100, activation = 'relu')(tower_4)\n",
    "    \n",
    "    \n",
    "    \n",
    "    merged = layers.concatenate([tower_1, tower_2,tower_3], axis=1)\n",
    "    merged = Flatten()(merged)\n",
    "    \n",
    "    final_merged = layers.concatenate([merged,tower_5],axis = 1)\n",
    "    out = Dense(100,activation='relu')(final_merged)\n",
    "    #out = Dropout(0.3)(out)\n",
    "    out = Dense(30)(out)\n",
    "    out = Dense(1, activation='sigmoid')(out)\n",
    "\n",
    "    model = Model(input_shape, out)\n",
    "    print(model.summary())\n",
    "    from keras.utils.vis_utils import plot_model\n",
    "    plot_model(model, to_file='weights/1CNN.png', show_shapes=True, show_layer_names=True)\n",
    "    \n",
    "    return model\n",
    "create_convnet(max_features, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
