{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import cleaner\n",
    "\n",
    "data_dir = '../data/'\n",
    "#data_dir = '/Users/xabuka/PycharmProjects/ASTD/data/'\n",
    "#pos_file = data_dir+'pos.txt'\n",
    "#neg_file = data_dir+'neg.txt'\n",
    "#no_file = data_dir+'no.txt'\n",
    "train_file_6 = 'MADAR-Corpus-6-train.tsv'\n",
    "dev_file_6 = 'MADAR-Corpus-6-dev.tsv'\n",
    "train_file_26 = 'MADAR-Corpus-26-train.tsv'\n",
    "dev_file_26 = 'MADAR-Corpus-26-dev.tsv'\n",
    "#import csv\n",
    "\n",
    "\n",
    "madar_list =[]\n",
    "sents, labels= [],[]\n",
    "# Firstly read tsv file\n",
    "corpus_type = 'combined/train_set/'\n",
    "corpus_splited = 'combined/train/'\n",
    "folder_path = data_dir+corpus_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_madar(file_name,tag):\n",
    "    sent, label= [],[]\n",
    "    with open(file_name, newline = '') as tsv:                                                                                         \n",
    "        madar_list=[x.strip().split('\\t') for x in tsv]\n",
    "    print(len(madar_list))\n",
    "    for row in madar_list:\n",
    "        \n",
    "        sent.append(row[0]) # save sentence\n",
    "        label.append(row[1]) # related dialect \n",
    "        if tag:\n",
    "            store_set(row,folder_path)\n",
    "        \n",
    "    return sent, label\n",
    "        # store a file inside the dialects folder ----- complete_dataset one_file/dialect\n",
    "        #with open(data_dir+'Dialect6/Dev_set/'+row[1]+'/'+row[1]+'.txt','a+') as write_file:\n",
    "           # write_file.write(row[0]+'\\n') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_set(row,folder_path):\n",
    "    with open(folder_path+row[1]+'/'+row[1]+'.txt','a+') as write_file:\n",
    "        write_file.write(row[0]+'\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_data\n",
    "def save_cleaned_set(folder_path,labels):\n",
    "    for file_name in set(labels):\n",
    "        clean_data = cleaner.clean(folder_path+file_name+'/'+file_name+'.txt')\n",
    "        print(len(clean_data))\n",
    "        with open(folder_path+file_name+'/'+file_name+'_clean.txt','w+') as clean_file:\n",
    "            for x in clean_data:\n",
    "                clean_file.write(x+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to use with ASTD data\n",
    "def save_cleaned_set(file_name,labels):\n",
    "    #for file_name in set(labels):\n",
    "        clean_data = cleaner.clean(file_name)\n",
    "        print(len(clean_data))\n",
    "        with open(data_dir+labels+'_clean.txt','w+') as clean_file:\n",
    "            for x in clean_data:\n",
    "                clean_file.write(x+'\\n')\n",
    "\n",
    "save_cleaned_set(no_file,'no')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creat_splited_files(folder_path_to_read, folder_path_to_write, labels, clean):\n",
    "    #save every line in a file\n",
    "    if clean:\n",
    "        txt = '_clean.txt'\n",
    "    else:\n",
    "        txt = '.txt'\n",
    "    for lang in set(labels):\n",
    "        with open(folder_path_to_read+lang+'/'+lang+txt,'r') as read_file:\n",
    "            for i, line in enumerate(read_file.readlines()):\n",
    "                #print(line)\n",
    "                with open(folder_path_to_write+lang+'/'+lang+'_'+str(i)+'.txt','w+') as write_file:\n",
    "                    write_file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folders(folder_path,labels):\n",
    "    # make dilaects folders\n",
    "    # folder for every dialect\n",
    "    for i in set(labels):\n",
    "        if not os.path.exists(folder_path+i):\n",
    "            os.makedirs(folder_path+i)\n",
    "            #print(folder_path+i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_system_files(folder_path):\n",
    "    #remove DS_store files\n",
    "    for lang in set(labels):\n",
    "        if os.path.exists(folder_path+lang+'/*.DS_store'):#+'/'+lang+'.txt'):\n",
    "            print('yes')\n",
    "            os.remove(folder_path+lang+'/*.DS_store')#+lang+'.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read the tsv file and store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(sents,labels):\n",
    "    small_sents = []\n",
    "    combine_sents = []\n",
    "    small_labels = []\n",
    "    combine_labels = []\n",
    "    for x,y in zip(sents,labels):\n",
    "        if len(x.split())<6:\n",
    "            small_sents.append(x)\n",
    "            small_labels.append(y)\n",
    "    #print(len(small_sents))\n",
    "    for i,sent in enumerate(small_sents):\n",
    "        #print(i)\n",
    "        if i < (len(small_sents)-1):\n",
    "            if small_labels[i] == small_labels[i+1]:\n",
    "                combine_sents.append(sent + ' '+small_sents[i+1])\n",
    "                combine_labels.append(small_labels[i])\n",
    "        \n",
    "        \n",
    "    return combine_sents,combine_labels\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41600\n",
      "41600\n",
      "41600\n"
     ]
    }
   ],
   "source": [
    "sents, labels= read_madar(data_dir+train_file_26,False)\n",
    "\n",
    "\n",
    "print(len(sents))\n",
    "print(len(labels))\n",
    "#print(set(small_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. creat directories for the dialects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_folders(folder_path,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. read the data into set files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41600\n"
     ]
    }
   ],
   "source": [
    "sents, labels= read_madar(data_dir+train_file_26,True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_sents,small_labels = combine(sents,labels)\n",
    "sents = sents + small_sents\n",
    "labels = labels + small_labels\n",
    " \n",
    "for i,label in enumerate(small_labels):\n",
    "    with open(folder_path+label+'/'+label+'.txt','a+') as write_file:\n",
    "        write_file.write(small_sents[i]+'\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. clean the read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading corpus ...\n",
      "200\n",
      "reading corpus ...\n",
      "200\n",
      "reading corpus ...\n",
      "200\n",
      "reading corpus ...\n",
      "200\n",
      "reading corpus ...\n",
      "200\n",
      "reading corpus ...\n",
      "200\n",
      "reading corpus ...\n",
      "200\n",
      "reading corpus ...\n",
      "200\n",
      "reading corpus ...\n",
      "200\n",
      "reading corpus ...\n",
      "200\n",
      "reading corpus ...\n",
      "200\n",
      "reading corpus ...\n",
      "200\n",
      "reading corpus ...\n",
      "200\n",
      "reading corpus ...\n",
      "200\n",
      "reading corpus ...\n",
      "200\n",
      "reading corpus ...\n",
      "200\n",
      "reading corpus ...\n",
      "200\n",
      "reading corpus ...\n",
      "200\n",
      "reading corpus ...\n",
      "200\n",
      "reading corpus ...\n",
      "200\n",
      "reading corpus ...\n",
      "200\n",
      "reading corpus ...\n",
      "200\n",
      "reading corpus ...\n",
      "200\n",
      "reading corpus ...\n",
      "200\n",
      "reading corpus ...\n",
      "200\n",
      "reading corpus ...\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "save_cleaned_set(folder_path,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. creat 26 dialetc folder for splited set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_pre = data_dir + corpus_splited+'pre_clean/'\n",
    "create_folders(folder_path_pre,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_post = data_dir + corpus_splited+'post_clean/'\n",
    "create_folders(folder_path_post,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. save the splited data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_pre = data_dir + corpus_splited+'pre_clean/'\n",
    "creat_splited_files(folder_path,folder_path_pre, labels,clean = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#updates\n",
    "folder_path_post = data_dir + corpus_splited+'post_clean/'\n",
    "creat_splited_files(folder_path, folder_path_post, labels, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
