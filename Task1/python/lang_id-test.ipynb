{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "import keras.optimizers\n",
    "from keras.layers import Embedding,Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams if GPU is available\n",
    "if tf.test.is_gpu_available():\n",
    "    # GPU\n",
    "    BATCH_SIZE = 512  # Number of images used in each iteration\n",
    "    EPOCHS = 12  # Number of passes through entire dataset\n",
    "    \n",
    "# Hyperparams for CPU training\n",
    "else:\n",
    "    # CPU\n",
    "    BATCH_SIZE = 64\n",
    "    EPOCHS = 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALPHABET:\n",
      "ءآأؤإئابةتثجحخدذرزسشصضطظعغفقكلمنهوىي ؟!?¿¡\n",
      "ALPHABET LEN(VOCAB SIZE): 42\n"
     ]
    }
   ],
   "source": [
    "#################\n",
    "# Configuration #\n",
    "#################\n",
    "\n",
    "# dictionary of languages that our classifier will cover\n",
    "LANGUAGES_DICT = {'BEI':0,'MSA':1,'CAI':2,'DOH':3,'RAB':4,'TUN':5}\n",
    "\"\"\"\n",
    "LANGUAGES_DICT = {'ALE':0,'ALG':1,'ALX':2,'AMM':3,'ASW':4,'BAG':5,\n",
    "                'BAS':6,'BEI':7,'BEN':8,'CAI':9,'DAM':10,'DOH':11,\n",
    "                  'FES':12,'JED':13,'JER':14,'KHA':15,'MOS':16,'MSA':17,\n",
    "                  'MUS':18,'RAB':19,'RIY':20,'SAL':21,'SAN':22,'SFX':23,\n",
    "                  'TRI':24,'TUN':25}\n",
    "\"\"\"\n",
    "\n",
    "# Length of cleaned text used for training and prediction - 140 chars\n",
    "MAX_LEN = 250\n",
    "\n",
    "# number of language samples per language that we will extract from source files\n",
    "NUM_SAMPLES = 9000\n",
    "NUM_test = 1000\n",
    "\n",
    "# For reproducibility\n",
    "SEED = 42\n",
    "\n",
    "import support #import define_alphabet\n",
    "# Load the Alphabet\n",
    "alphabet = support.define_alphabet()\n",
    "print('ALPHABET:')\n",
    "print(alphabet[1])\n",
    "\n",
    "VOCAB_SIZE = len(alphabet[1])\n",
    "print('ALPHABET LEN(VOCAB SIZE):', VOCAB_SIZE)\n",
    "\n",
    "# Folders from where load / store the raw, source, cleaned, samples and train_test data\n",
    "data_directory = \"../data/Dialect6\"\n",
    "source_directory = os.path.join(data_directory, 'source')\n",
    "cleaned_directory = os.path.join(data_directory, 'cleaned')\n",
    "source_directory_test = os.path.join(data_directory, 'source_test')\n",
    "cleaned_directory_test =os.path.join(data_directory, 'cleaned_test')\n",
    "samples_directory = os.path.join('/tmp', 'samples')\n",
    "train_test_directory = os.path.join('/tmp', 'train_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. SAMPLE TEXT: \n",
      " الضريبة\n",
      " تفضل الوصل بالشنطة\n",
      " شو بدك تشتري ؟\n",
      " ماري كانت عالوقت للتران بس جورج ما كان\n",
      " فيي حط هالرسالة فيه ؟\n",
      " بدي إشتري طعومة\n",
      " انا مش من هون\n",
      " فيك تفرجيني كيف بيتاكل هيدا ؟\n",
      " هالأوتيل عندو تجهيزات لمؤتمر ؟\n",
      " في شي حدا بيفهم ياباني ؟\n",
      " لسوء الحظ هوي كتير\n",
      "\n",
      "2. REFERENCE ALPHABET: \n",
      " ['ء', 'آ', 'أ', 'ؤ', 'إ', 'ئ', 'ا', 'ب', 'ة', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ف', 'ق', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ى', 'ي', ' ', '؟', '!', '?', '¿', '¡']\n",
      "\n",
      "3. SAMPLE INPUT ROW: \n",
      " [1, 0, 1, 1, 1, 0, 21, 8, 4, 14, 0, 4, 3, 0, 5, 0, 10, 1, 3, 6, 1, 2, 3, 1, 3, 0, 8, 1, 7, 16, 8, 10, 8, 10, 0, 23, 48, 5, 0, 0, 0, 0]\n",
      "\n",
      "4. INPUT SIZE (VOCAB SIZE):  42\n"
     ]
    }
   ],
   "source": [
    "from support import get_sample_text, get_input_row\n",
    "    \n",
    "# let's see if our processing is returning counts\n",
    "# last part calculates also input_size for DNN so this code must be run before DNN is trained\n",
    "path = os.path.join(cleaned_directory, \"BEI_clean.txt\")\n",
    "with open(path, 'r') as f:\n",
    "    content = f.read()\n",
    "    random_index = random.randrange(0,len(content)-2*MAX_LEN)\n",
    "    sample_text = get_sample_text(content,random_index,MAX_LEN)\n",
    "    print (\"1. SAMPLE TEXT: \\n\", sample_text)\n",
    "    print (\"\\n2. REFERENCE ALPHABET: \\n\", alphabet[0])\n",
    "    \n",
    "    sample_input_row = get_input_row(content, random_index, MAX_LEN, alphabet)\n",
    "    print (\"\\n3. SAMPLE INPUT ROW: \\n\",sample_input_row)\n",
    "    \n",
    "    input_size = len(sample_input_row)\n",
    "    if input_size != VOCAB_SIZE:\n",
    "        print(\"Something strange happened!\")\n",
    "        \n",
    "    print (\"\\n4. INPUT SIZE (VOCAB SIZE): \", input_size)\n",
    "    del content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file : ../data/Dialect6/cleaned/BEI_clean.txt\n",
      "267225\n",
      "File size :  0.27 MB  | # possible samples :  6362 | # skip chars : -165\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Processing file : ../data/Dialect6/cleaned/MSA_clean.txt\n",
      "322546\n",
      "File size :  0.32 MB  | # possible samples :  7679 | # skip chars : -160\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Processing file : ../data/Dialect6/cleaned/CAI_clean.txt\n",
      "288771\n",
      "File size :  0.29 MB  | # possible samples :  6875 | # skip chars : -163\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Processing file : ../data/Dialect6/cleaned/DOH_clean.txt\n",
      "263868\n",
      "File size :  0.26 MB  | # possible samples :  6282 | # skip chars : -165\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Processing file : ../data/Dialect6/cleaned/RAB_clean.txt\n",
      "309464\n",
      "File size :  0.31 MB  | # possible samples :  7368 | # skip chars : -161\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Processing file : ../data/Dialect6/cleaned/TUN_clean.txt\n",
      "276387\n",
      "File size :  0.28 MB  | # possible samples :  6580 | # skip chars : -164\n",
      "----------------------------------------------------------------------------------------------------\n",
      "19824\n",
      "{0, 1, 2, 3, 4, 5}\n",
      "Vocab Size :  42\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Samples array size :  (19823, 43)\n",
      "/tmp/samples/lang_samples_42.npz size :  0.52 MB\n"
     ]
    }
   ],
   "source": [
    "# Utility function to return file Bytes size in MB\n",
    "def size_mb(size):\n",
    "    size_mb =  '{:.2f}'.format(size/(1000*1000.0))\n",
    "    return size_mb + \" MB\"\n",
    "\n",
    "# Now we have preprocessing utility functions ready. Let's use them to process each cleaned language file\n",
    "# and turn text data into numerical data samples for our neural network\n",
    "# prepare numpy array\n",
    "sample_data = np.empty((NUM_SAMPLES*len(LANGUAGES_DICT),input_size+1),dtype = np.uint16)\n",
    "lang_seq = 0 # offset for each language data\n",
    "jump_reduce = 0.2 # part of characters removed from jump to avoid passing the end of file\n",
    "counter = 0\n",
    "for lang_code in LANGUAGES_DICT:\n",
    "    start_index = 0\n",
    "    path = os.path.join(cleaned_directory, lang_code+\"_clean.txt\")\n",
    "    with open(path, 'r') as f:\n",
    "        print (\"Processing file : \" + path)\n",
    "        file_content = f.read()\n",
    "        content_length = len(file_content)\n",
    "        print(content_length)\n",
    "        remaining = content_length - MAX_LEN*NUM_SAMPLES\n",
    "        jump = int(((remaining/NUM_SAMPLES)*3)/4)\n",
    "        print (\"File size : \",size_mb(content_length),\\\n",
    "               \" | # possible samples : \",int(content_length/VOCAB_SIZE),\\\n",
    "              \"| # skip chars : \" + str(jump))\n",
    "        for idx in range(NUM_SAMPLES):\n",
    "            if (content_length-start_index < 300):\n",
    "                break\n",
    "            input_row = get_input_row(file_content, start_index, MAX_LEN, alphabet)\n",
    "            #print(idx,' ',start_index,' ',MAX_LEN)\n",
    "            \n",
    "            sample_data[counter,] = input_row + [LANGUAGES_DICT[lang_code]]\n",
    "            counter = counter +1\n",
    "            #print(counter)\n",
    "            start_index += MAX_LEN + jump\n",
    "        del file_content\n",
    "    lang_seq += 1\n",
    "    print (100*\"-\")\n",
    "     \n",
    "# Let's randomy shuffle the data\n",
    "print(counter)\n",
    "new_data = sample_data[:counter-1,]\n",
    "np.random.shuffle(new_data)\n",
    "print(set(new_data[:,-1]))\n",
    "# reference input size\n",
    "print (\"Vocab Size : \",VOCAB_SIZE )\n",
    "print (100*\"-\")\n",
    "print (\"Samples array size : \",new_data.shape )\n",
    "\n",
    "# Create the the sample dirctory if not exists\n",
    "if not os.path.exists(samples_directory):\n",
    "    os.makedirs(samples_directory)\n",
    "\n",
    "# Save compressed sample data to disk\n",
    "path_smpl = os.path.join(samples_directory,\"lang_samples_\"+str(VOCAB_SIZE)+\".npz\")\n",
    "np.savez_compressed(path_smpl,data=new_data)\n",
    "print(path_smpl, \"size : \", size_mb(os.path.getsize(path_smpl)))\n",
    "del sample_data,new_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file : ../data/Dialect6/cleaned_test/BEI_clean.txt\n",
      "File size :  0.03 MB  | # possible samples :  701 | # skip chars : -165\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Processing file : ../data/Dialect6/cleaned_test/MSA_clean.txt\n",
      "File size :  0.04 MB  | # possible samples :  845 | # skip chars : -160\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Processing file : ../data/Dialect6/cleaned_test/CAI_clean.txt\n",
      "File size :  0.03 MB  | # possible samples :  761 | # skip chars : -163\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Processing file : ../data/Dialect6/cleaned_test/DOH_clean.txt\n",
      "File size :  0.03 MB  | # possible samples :  689 | # skip chars : -165\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Processing file : ../data/Dialect6/cleaned_test/RAB_clean.txt\n",
      "File size :  0.03 MB  | # possible samples :  814 | # skip chars : -161\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Processing file : ../data/Dialect6/cleaned_test/TUN_clean.txt\n",
      "File size :  0.03 MB  | # possible samples :  729 | # skip chars : -164\n",
      "----------------------------------------------------------------------------------------------------\n",
      "idx 353\n",
      "counter 2171\n",
      "2171\n",
      "{0, 1, 2, 3, 4, 5}\n",
      "Vocab Size :  42\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Samples array size :  (2171, 43)\n",
      "/tmp/samples/lang_samples_test_42.npz size :  0.06 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sample_data_test = np.empty((NUM_test*len(LANGUAGES_DICT),input_size+1),dtype = np.uint16)\n",
    "#print(set(sample_data_test[:,-1]))\n",
    "lang_seq = 0 # offset for each language data\n",
    "jump_reduce = 0.2 # part of characters removed from jump to avoid passing the end of file\n",
    "counter = 0\n",
    "for lang_code in LANGUAGES_DICT:\n",
    "    start_index = 0\n",
    "    path = os.path.join(cleaned_directory_test, lang_code+\"_clean.txt\")\n",
    "    with open(path, 'r') as f:\n",
    "        print (\"Processing file : \" + path)\n",
    "        file_content = f.read()\n",
    "        content_length = len(file_content)\n",
    "        remaining = content_length - MAX_LEN*NUM_test\n",
    "        jump = int(((remaining/NUM_test)*3)/4)\n",
    "        print (\"File size : \",size_mb(content_length),\\\n",
    "               \" | # possible samples : \",int(content_length/VOCAB_SIZE),\\\n",
    "              \"| # skip chars : \" + str(jump))\n",
    "        for idx in range(NUM_test):\n",
    "            if (content_length-start_index < 300):\n",
    "                break\n",
    "            input_row = get_input_row(file_content, start_index, MAX_LEN, alphabet)\n",
    "            #print(NUM_test*lang_seq+idx)\n",
    "            sample_data_test[counter,] = input_row + [LANGUAGES_DICT[lang_code]]\n",
    "            counter = counter +1\n",
    "\n",
    "            start_index += MAX_LEN + jump\n",
    "        del file_content\n",
    "    lang_seq += 1\n",
    "    print (100*\"-\")\n",
    "print('idx',idx)\n",
    "print('counter',counter)\n",
    "print(len(sample_data_test[:counter,]))\n",
    "new_data = sample_data_test[:counter-1,]\n",
    "    \n",
    "# Let's randomy shuffle the data\n",
    "np.random.shuffle(new_data)\n",
    "print(set(new_data[:,-1]))\n",
    "# reference input size\n",
    "print (\"Vocab Size : \",VOCAB_SIZE )\n",
    "print (100*\"-\")\n",
    "print (\"Samples array size : \",sample_data_test[:counter,].shape )\n",
    "\n",
    "# Create the the sample dirctory if not exists\n",
    "if not os.path.exists(samples_directory):\n",
    "    os.makedirs(samples_directory)\n",
    "\n",
    "# Save compressed sample data to disk\n",
    "path_smpl_test = os.path.join(samples_directory,\"lang_samples_test_\"+str(VOCAB_SIZE)+\".npz\")\n",
    "np.savez_compressed(path_smpl_test,data=new_data)\n",
    "print(path_smpl_test, \"size : \", size_mb(os.path.getsize(path_smpl_test)))\n",
    "del sample_data_test,new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample record : \n",
      " [ 0  0  0  0  0  0 32  7  5 12  0  0  3  3 13  0 11  0  8  6  1  1  3  1\n",
      "  5  1  1  4  2 21 14  9  6 13  0 13 46  2  0  0  0  0  2]\n",
      "\n",
      "Sample language :  CAI\n",
      "\n",
      "Dataset shape (Total_samples, Alphabet): (19823, 43)\n",
      "Language bins count (samples per language): \n",
      "BEI 3141\n",
      "MSA 3581\n",
      "CAI 3316\n",
      "DOH 3101\n",
      "RAB 3474\n",
      "TUN 3210\n"
     ]
    }
   ],
   "source": [
    "# utility function to turn language id into language code\n",
    "def decode_langid(langid):    \n",
    "    for dname, did in LANGUAGES_DICT.items():\n",
    "        if did == langid:\n",
    "            return dname\n",
    "\n",
    "# Loading the data\n",
    "path_smpl = os.path.join(samples_directory,\"lang_samples_\"+str(VOCAB_SIZE)+\".npz\")\n",
    "dt = np.load(path_smpl)['data']\n",
    "\n",
    "# Sanity chech on a random sample\n",
    "random_index = random.randrange(0,dt.shape[0])\n",
    "print (\"Sample record : \\n\",dt[random_index,])\n",
    "print (\"\\nSample language : \",decode_langid(dt[random_index,][VOCAB_SIZE]))\n",
    "\n",
    "# Check if the data have equal share of different languages\n",
    "print (\"\\nDataset shape (Total_samples, Alphabet):\", dt.shape)\n",
    "bins = np.bincount(dt[:,input_size])\n",
    "\n",
    "print (\"Language bins count (samples per language): \") \n",
    "for lang_code in LANGUAGES_DICT: \n",
    "    print (lang_code, bins[LANGUAGES_DICT[lang_code]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample record : \n",
      " [ 0  0  2  0  2  0 22  5  8 14  2  1  3  6  4  0 12  1  2  6  3  0  3  0\n",
      "  6  1  3  1  6  9 12 10  6 15  0 26 45  3  0  0  0  0  5]\n",
      "\n",
      "Sample language :  TUN\n",
      "\n",
      "Dataset shape (Total_samples, Alphabet): (2170, 43)\n",
      "Language bins count (samples per language): \n",
      "BEI 343\n",
      "MSA 392\n",
      "CAI 365\n",
      "DOH 337\n",
      "RAB 381\n",
      "TUN 352\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Loading the data\n",
    "path_smpl_test = os.path.join(samples_directory,\"lang_samples_test_\"+str(VOCAB_SIZE)+\".npz\")\n",
    "dt_test = np.load(path_smpl_test)['data']\n",
    "\n",
    "# Sanity chech on a random sample\n",
    "random_index = random.randrange(0,dt_test.shape[0])\n",
    "print (\"Sample record : \\n\",dt_test[random_index,])\n",
    "print (\"\\nSample language : \",decode_langid(dt_test[random_index,][VOCAB_SIZE]))\n",
    "\n",
    "# Check if the data have equal share of different languages\n",
    "print (\"\\nDataset shape (Total_samples, Alphabet):\", dt_test.shape)\n",
    "bins_test = np.bincount(dt_test[:,input_size])\n",
    "\n",
    "print (\"Language bins count (samples per language): \") \n",
    "for lang_code in LANGUAGES_DICT: \n",
    "    print (lang_code, bins_test[LANGUAGES_DICT[lang_code]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0, 1.0, 2.0, 3.0, 4.0, 5.0}\n",
      "{0.0, 1.0, 2.0, 3.0, 4.0, 5.0}\n",
      "Example data before processing:\n",
      "X : \n",
      " [ 0.  0.  2.  0.  0.  0. 18. 11.  7. 13.  1.  6.  3.  3.  4.  1.  9.  0.\n",
      "  8.  5.  1.  1.  2.  0. 13.  0.  7.  0. 11.  7.  9. 15.  0.  6.  1. 13.\n",
      " 51.  7.  0.  0.  0.  0.]\n",
      "Y : \n",
      " 5.0\n",
      "Example data before processing:\n",
      "X : \n",
      " [ 1.  0.  2.  0.  1.  0. 18.  8.  5. 13.  0.  2.  5.  2. 10.  0.  8.  0.\n",
      "  4.  6.  3.  0.  3.  1.  9.  1.  5.  1. 11. 14. 14. 11.  4. 11.  0. 19.\n",
      " 46.  3.  0.  0.  0.  0.]\n",
      "Y : \n",
      " 0.0\n",
      "X preprocessed shape : (19823, 42)\n",
      "X preprocessed shape : (2170, 42)\n",
      "\n",
      "Example data after processing:\n",
      "X : \n",
      " [ 0.  0.  2.  0.  0.  0. 18. 11.  7. 13.  1.  6.  3.  3.  4.  1.  9.  0.\n",
      "  8.  5.  1.  1.  2.  0. 13.  0.  7.  0. 11.  7.  9. 15.  0.  6.  1. 13.\n",
      " 51.  7.  0.  0.  0.  0.]\n",
      "Y : \n",
      " [0. 0. 0. 0. 0. 1.]\n",
      "\n",
      "Example  tets data after processing:\n",
      "X : \n",
      " [ 1.  0.  2.  0.  1.  0. 18.  8.  5. 13.  0.  2.  5.  2. 10.  0.  8.  0.\n",
      "  4.  6.  3.  0.  3.  1.  9.  1.  5.  1. 11. 14. 14. 11.  4. 11.  0. 19.\n",
      " 46.  3.  0.  0.  0.  0.]\n",
      "Y : \n",
      " [1. 0. 0. 0. 0. 0.]\n",
      "/tmp/train_test/train_test_data_42.npz size :  0.68 MB\n",
      "/tmp/train_test/train_test_data_test_42.npz size :  0.07 MB\n"
     ]
    }
   ],
   "source": [
    "# we need to preprocess data for DNN yet again - scale it \n",
    "# scaling will ensure that our optimization algorithm (variation of gradient descent) will converge well\n",
    "# we need also ensure one-hot econding of target classes for softmax output layer\n",
    "# let's convert datatype before processing to float\n",
    "dt = dt.astype(np.float32)\n",
    "dt_test = dt_test.astype(np.float32)\n",
    "# X and Y split\n",
    "X_train = dt[:, 0:input_size] # Samples\n",
    "Y_train = dt[:, input_size] # The last element is the label\n",
    "print(set(Y_train))\n",
    "X_test = dt_test[:, 0:input_size] # Samples\n",
    "Y_test = dt_test[:, input_size] # The last element is the label\n",
    "print(set(Y_test))\n",
    "del dt,dt_test\n",
    "\n",
    "# Random index to check random sample\n",
    "random_index_train = random.randrange(0,X_train.shape[0])\n",
    "print(\"Example data before processing:\")\n",
    "print(\"X : \\n\", X_train[random_index_train,])\n",
    "print(\"Y : \\n\", Y_train[random_index_train])\n",
    "\n",
    "\n",
    "# Random index to check random sample\n",
    "random_index_test = random.randrange(0,X_test.shape[0])\n",
    "print(\"Example data before processing:\")\n",
    "print(\"X : \\n\", X_test[random_index_test,])\n",
    "print(\"Y : \\n\", Y_test[random_index_test])\n",
    "\n",
    "# X PREPROCESSING\n",
    "# Feature Standardization - Standar scaler will be useful later during DNN prediction\n",
    "standard_scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X = standard_scaler.transform(X_train)   \n",
    "print (\"X preprocessed shape :\", X_train.shape)\n",
    "\n",
    "standard_scaler = preprocessing.StandardScaler().fit(X_test)\n",
    "X = standard_scaler.transform(X_test)   \n",
    "print (\"X preprocessed shape :\", X_test.shape)\n",
    "\n",
    "\n",
    "# Y PREPROCESSINGY \n",
    "# One-hot encoding\n",
    "Y_train = keras.utils.to_categorical(Y_train, num_classes=len(LANGUAGES_DICT))\n",
    "Y_test = keras.utils.to_categorical(Y_test, num_classes=len(LANGUAGES_DICT))\n",
    "\n",
    "\n",
    "# See the sample data\n",
    "print(\"\\nExample data after processing:\")\n",
    "print(\"X : \\n\", X_train[random_index_train,])\n",
    "print(\"Y : \\n\", Y_train[random_index_train])\n",
    "\n",
    "print(\"\\nExample  tets data after processing:\")\n",
    "print(\"X : \\n\", X_test[random_index_test,])\n",
    "print(\"Y : \\n\", Y_test[random_index_test])\n",
    "\n",
    "# Train/test split. Static seed to have comparable results for different runs\n",
    "#X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=SEED)\n",
    "\n",
    "\n",
    "# Create the train / test directory if not extists\n",
    "if not os.path.exists(train_test_directory):\n",
    "    os.makedirs(train_test_directory)\n",
    "\n",
    "# Save compressed train_test data to disk\n",
    "path_tt = os.path.join(train_test_directory,\"train_test_data_\"+str(VOCAB_SIZE)+\".npz\")\n",
    "np.savez_compressed(path_tt,X_train=X_train,Y_train=Y_train)\n",
    "print(path_tt, \"size : \",size_mb(os.path.getsize(path_tt)))\n",
    "\n",
    "\n",
    "path_tt_test = os.path.join(train_test_directory,\"train_test_data_test_\"+str(VOCAB_SIZE)+\".npz\")\n",
    "np.savez_compressed(path_tt_test,X_test=X_test,Y_test=Y_test)\n",
    "print(path_tt_test, \"size : \",size_mb(os.path.getsize(path_tt_test)))\n",
    "\n",
    "\n",
    "del X_train,Y_train,X_test,Y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (19823, 42)\n",
      "Y_train:  (19823, 6)\n",
      "X_test:  (2170, 42)\n",
      "Y_test:  (2170, 6)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load train data first from file\n",
    "path_tt = os.path.join(train_test_directory, \"train_test_data_\"+str(VOCAB_SIZE)+\".npz\")\n",
    "train_test_data = np.load(path_tt)\n",
    "# Load train data first from file\n",
    "path_tt_test = os.path.join(train_test_directory, \"train_test_data_test_\"+str(VOCAB_SIZE)+\".npz\")\n",
    "train_test_data_test = np.load(path_tt_test)\n",
    "\n",
    "# Train Set\n",
    "X_train = train_test_data['X_train']\n",
    "print (\"X_train: \",X_train.shape)\n",
    "Y_train = train_test_data['Y_train']\n",
    "print (\"Y_train: \",Y_train.shape)\n",
    "\n",
    "# Test Set\n",
    "X_test = train_test_data_test['X_test']\n",
    "print (\"X_test: \",X_test.shape)\n",
    "Y_test = train_test_data_test['Y_test']\n",
    "print (\"Y_test: \",Y_test.shape)\n",
    "\n",
    "del train_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_22 (Dense)             (None, 500)               21500     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 300)               150300    \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 6)                 606       \n",
      "=================================================================\n",
      "Total params: 202,506\n",
      "Trainable params: 202,506\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "# Note: glorot_uniform is the Xavier uniform initializer.\n",
    "model.add(Dense(500,input_dim=input_size, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(300, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(LANGUAGES_DICT), kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\"))\n",
    "model_optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=model_optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17840 samples, validate on 1983 samples\n",
      "Epoch 1/1000\n",
      "17840/17840 [==============================] - 2s 112us/step - loss: 0.0489 - acc: 0.9833 - val_loss: 0.3213 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.32134, saving model to weights.best.madar6_1000min.hdf5\n",
      "Epoch 2/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0425 - acc: 0.9858 - val_loss: 0.3205 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.32134 to 0.32053, saving model to weights.best.madar6_1000min.hdf5\n",
      "Epoch 3/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0551 - acc: 0.9825 - val_loss: 0.3252 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.32053\n",
      "Epoch 4/1000\n",
      "17840/17840 [==============================] - 2s 108us/step - loss: 0.0445 - acc: 0.9838 - val_loss: 0.3048 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.32053 to 0.30476, saving model to weights.best.madar6_1000min.hdf5\n",
      "Epoch 5/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0405 - acc: 0.9862 - val_loss: 0.3191 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.30476\n",
      "Epoch 6/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0462 - acc: 0.9845 - val_loss: 0.3033 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.30476 to 0.30331, saving model to weights.best.madar6_1000min.hdf5\n",
      "Epoch 7/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0486 - acc: 0.9839 - val_loss: 0.3325 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.30331\n",
      "Epoch 8/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0466 - acc: 0.9850 - val_loss: 0.3224 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.30331\n",
      "Epoch 9/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0454 - acc: 0.9853 - val_loss: 0.3165 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.30331\n",
      "Epoch 10/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0434 - acc: 0.9849 - val_loss: 0.3220 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.30331\n",
      "Epoch 11/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0452 - acc: 0.9856 - val_loss: 0.3143 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.30331\n",
      "Epoch 12/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0477 - acc: 0.9840 - val_loss: 0.3042 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.30331\n",
      "Epoch 13/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0484 - acc: 0.9839 - val_loss: 0.3129 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.30331\n",
      "Epoch 14/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0462 - acc: 0.9844 - val_loss: 0.3185 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.30331\n",
      "Epoch 15/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0431 - acc: 0.9857 - val_loss: 0.3135 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.30331\n",
      "Epoch 16/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0489 - acc: 0.9827 - val_loss: 0.3221 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.30331\n",
      "Epoch 17/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0461 - acc: 0.9849 - val_loss: 0.3117 - val_acc: 0.9319\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.30331\n",
      "Epoch 18/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0465 - acc: 0.9846 - val_loss: 0.3321 - val_acc: 0.9239\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.30331\n",
      "Epoch 19/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0471 - acc: 0.9836 - val_loss: 0.3133 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.30331\n",
      "Epoch 20/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0448 - acc: 0.9846 - val_loss: 0.3192 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.30331\n",
      "Epoch 21/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0496 - acc: 0.9841 - val_loss: 0.3039 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.30331\n",
      "Epoch 22/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0538 - acc: 0.9840 - val_loss: 0.3135 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.30331\n",
      "Epoch 23/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0392 - acc: 0.9862 - val_loss: 0.3270 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.30331\n",
      "Epoch 24/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0476 - acc: 0.9836 - val_loss: 0.3216 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.30331\n",
      "Epoch 25/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0462 - acc: 0.9855 - val_loss: 0.3161 - val_acc: 0.9324\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.30331\n",
      "Epoch 26/1000\n",
      "17840/17840 [==============================] - 2s 112us/step - loss: 0.0441 - acc: 0.9854 - val_loss: 0.3057 - val_acc: 0.9319\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.30331\n",
      "Epoch 27/1000\n",
      "17840/17840 [==============================] - 2s 111us/step - loss: 0.0510 - acc: 0.9826 - val_loss: 0.3127 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.30331\n",
      "Epoch 28/1000\n",
      "17840/17840 [==============================] - 2s 109us/step - loss: 0.0417 - acc: 0.9859 - val_loss: 0.3332 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.30331\n",
      "Epoch 29/1000\n",
      "17840/17840 [==============================] - 2s 110us/step - loss: 0.0396 - acc: 0.9869 - val_loss: 0.3354 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.30331\n",
      "Epoch 30/1000\n",
      "17840/17840 [==============================] - 2s 111us/step - loss: 0.0493 - acc: 0.9844 - val_loss: 0.3090 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.30331\n",
      "Epoch 31/1000\n",
      "17840/17840 [==============================] - 2s 128us/step - loss: 0.0499 - acc: 0.9841 - val_loss: 0.3145 - val_acc: 0.9249\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.30331\n",
      "Epoch 32/1000\n",
      "17840/17840 [==============================] - 2s 116us/step - loss: 0.0413 - acc: 0.9862 - val_loss: 0.3055 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.30331\n",
      "Epoch 33/1000\n",
      "17840/17840 [==============================] - 2s 123us/step - loss: 0.0482 - acc: 0.9859 - val_loss: 0.3117 - val_acc: 0.9319\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.30331\n",
      "Epoch 34/1000\n",
      "17840/17840 [==============================] - 2s 122us/step - loss: 0.0450 - acc: 0.9861 - val_loss: 0.3067 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.30331\n",
      "Epoch 35/1000\n",
      "17840/17840 [==============================] - 2s 117us/step - loss: 0.0500 - acc: 0.9826 - val_loss: 0.3043 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.30331\n",
      "Epoch 36/1000\n",
      "17840/17840 [==============================] - 2s 118us/step - loss: 0.0460 - acc: 0.9850 - val_loss: 0.2972 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.30331 to 0.29716, saving model to weights.best.madar6_1000min.hdf5\n",
      "Epoch 37/1000\n",
      "17840/17840 [==============================] - 2s 120us/step - loss: 0.0445 - acc: 0.9853 - val_loss: 0.3182 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.29716\n",
      "Epoch 38/1000\n",
      "17840/17840 [==============================] - 2s 112us/step - loss: 0.0445 - acc: 0.9857 - val_loss: 0.3330 - val_acc: 0.9218\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.29716\n",
      "Epoch 39/1000\n",
      "17840/17840 [==============================] - 2s 116us/step - loss: 0.0448 - acc: 0.9852 - val_loss: 0.3208 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.29716\n",
      "Epoch 40/1000\n",
      "17840/17840 [==============================] - 2s 115us/step - loss: 0.0540 - acc: 0.9830 - val_loss: 0.3064 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.29716\n",
      "Epoch 41/1000\n",
      "17840/17840 [==============================] - 2s 116us/step - loss: 0.0446 - acc: 0.9858 - val_loss: 0.3156 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.29716\n",
      "Epoch 42/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0393 - acc: 0.9866 - val_loss: 0.3238 - val_acc: 0.9239\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.29716\n",
      "Epoch 43/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0425 - acc: 0.9857 - val_loss: 0.3241 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.29716\n",
      "Epoch 44/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0388 - acc: 0.9855 - val_loss: 0.3396 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.29716\n",
      "Epoch 45/1000\n",
      "17840/17840 [==============================] - 2s 99us/step - loss: 0.0466 - acc: 0.9852 - val_loss: 0.3317 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.29716\n",
      "Epoch 46/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0454 - acc: 0.9837 - val_loss: 0.3229 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.29716\n",
      "Epoch 47/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0453 - acc: 0.9841 - val_loss: 0.3264 - val_acc: 0.9254\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.29716\n",
      "Epoch 48/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0444 - acc: 0.9847 - val_loss: 0.3312 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.29716\n",
      "Epoch 49/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0434 - acc: 0.9858 - val_loss: 0.3017 - val_acc: 0.9244\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.29716\n",
      "Epoch 50/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0490 - acc: 0.9842 - val_loss: 0.3332 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.29716\n",
      "Epoch 51/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0439 - acc: 0.9850 - val_loss: 0.3030 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.29716\n",
      "Epoch 52/1000\n",
      "17840/17840 [==============================] - 2s 99us/step - loss: 0.0454 - acc: 0.9855 - val_loss: 0.3196 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.29716\n",
      "Epoch 53/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0421 - acc: 0.9858 - val_loss: 0.3331 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.29716\n",
      "Epoch 54/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0469 - acc: 0.9852 - val_loss: 0.3399 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.29716\n",
      "Epoch 55/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0456 - acc: 0.9853 - val_loss: 0.3238 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.29716\n",
      "Epoch 56/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0428 - acc: 0.9859 - val_loss: 0.3066 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.29716\n",
      "Epoch 57/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0423 - acc: 0.9848 - val_loss: 0.3257 - val_acc: 0.9249\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.29716\n",
      "Epoch 58/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0431 - acc: 0.9858 - val_loss: 0.3205 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.29716\n",
      "Epoch 59/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0425 - acc: 0.9862 - val_loss: 0.3405 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.29716\n",
      "Epoch 60/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0430 - acc: 0.9854 - val_loss: 0.3226 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.29716\n",
      "Epoch 61/1000\n",
      "17840/17840 [==============================] - 2s 114us/step - loss: 0.0449 - acc: 0.9840 - val_loss: 0.3415 - val_acc: 0.9249\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.29716\n",
      "Epoch 62/1000\n",
      "17840/17840 [==============================] - 2s 112us/step - loss: 0.0420 - acc: 0.9849 - val_loss: 0.3107 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.29716\n",
      "Epoch 63/1000\n",
      "17840/17840 [==============================] - 2s 112us/step - loss: 0.0485 - acc: 0.9850 - val_loss: 0.3040 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.29716\n",
      "Epoch 64/1000\n",
      "17840/17840 [==============================] - 2s 112us/step - loss: 0.0423 - acc: 0.9861 - val_loss: 0.3198 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.29716\n",
      "Epoch 65/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0474 - acc: 0.9845 - val_loss: 0.3221 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.29716\n",
      "Epoch 66/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0435 - acc: 0.9855 - val_loss: 0.3236 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.29716\n",
      "Epoch 67/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0464 - acc: 0.9853 - val_loss: 0.3058 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.29716\n",
      "Epoch 68/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0419 - acc: 0.9865 - val_loss: 0.3227 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.29716\n",
      "Epoch 69/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0434 - acc: 0.9856 - val_loss: 0.3048 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.29716\n",
      "Epoch 70/1000\n",
      "17840/17840 [==============================] - 2s 124us/step - loss: 0.0450 - acc: 0.9850 - val_loss: 0.3208 - val_acc: 0.9254\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.29716\n",
      "Epoch 71/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0422 - acc: 0.9855 - val_loss: 0.3124 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.29716\n",
      "Epoch 72/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0507 - acc: 0.9831 - val_loss: 0.3274 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.29716\n",
      "Epoch 73/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0465 - acc: 0.9839 - val_loss: 0.3017 - val_acc: 0.9254\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.29716\n",
      "Epoch 74/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0412 - acc: 0.9866 - val_loss: 0.3276 - val_acc: 0.9254\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.29716\n",
      "Epoch 75/1000\n",
      "17840/17840 [==============================] - 2s 109us/step - loss: 0.0470 - acc: 0.9843 - val_loss: 0.3161 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.29716\n",
      "Epoch 76/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0477 - acc: 0.9846 - val_loss: 0.3185 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.29716\n",
      "Epoch 77/1000\n",
      "17840/17840 [==============================] - 2s 108us/step - loss: 0.0404 - acc: 0.9862 - val_loss: 0.3196 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.29716\n",
      "Epoch 78/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0406 - acc: 0.9858 - val_loss: 0.3141 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.29716\n",
      "Epoch 79/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0413 - acc: 0.9868 - val_loss: 0.3259 - val_acc: 0.9254\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.29716\n",
      "Epoch 80/1000\n",
      "17840/17840 [==============================] - 2s 108us/step - loss: 0.0432 - acc: 0.9859 - val_loss: 0.3364 - val_acc: 0.9233\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.29716\n",
      "Epoch 81/1000\n",
      "17840/17840 [==============================] - 2s 108us/step - loss: 0.0454 - acc: 0.9841 - val_loss: 0.3218 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.29716\n",
      "Epoch 82/1000\n",
      "17840/17840 [==============================] - 2s 109us/step - loss: 0.0471 - acc: 0.9848 - val_loss: 0.3150 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.29716\n",
      "Epoch 83/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0436 - acc: 0.9844 - val_loss: 0.3102 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.29716\n",
      "Epoch 84/1000\n",
      "17840/17840 [==============================] - 2s 108us/step - loss: 0.0420 - acc: 0.9848 - val_loss: 0.3144 - val_acc: 0.9259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00084: val_loss did not improve from 0.29716\n",
      "Epoch 85/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0471 - acc: 0.9849 - val_loss: 0.3296 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.29716\n",
      "Epoch 86/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0390 - acc: 0.9867 - val_loss: 0.3223 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.29716\n",
      "Epoch 87/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0503 - acc: 0.9843 - val_loss: 0.3098 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.29716\n",
      "Epoch 88/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0500 - acc: 0.9836 - val_loss: 0.3325 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.29716\n",
      "Epoch 89/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0474 - acc: 0.9852 - val_loss: 0.3044 - val_acc: 0.9319\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.29716\n",
      "Epoch 90/1000\n",
      "17840/17840 [==============================] - 2s 99us/step - loss: 0.0407 - acc: 0.9859 - val_loss: 0.3300 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.29716\n",
      "Epoch 91/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0456 - acc: 0.9851 - val_loss: 0.3145 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.29716\n",
      "Epoch 92/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0407 - acc: 0.9873 - val_loss: 0.3254 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.29716\n",
      "Epoch 93/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0450 - acc: 0.9850 - val_loss: 0.3253 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.29716\n",
      "Epoch 94/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0403 - acc: 0.9860 - val_loss: 0.3451 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.29716\n",
      "Epoch 95/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0471 - acc: 0.9843 - val_loss: 0.3195 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.29716\n",
      "Epoch 96/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0433 - acc: 0.9853 - val_loss: 0.3289 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.29716\n",
      "Epoch 97/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0429 - acc: 0.9857 - val_loss: 0.3344 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.29716\n",
      "Epoch 98/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0443 - acc: 0.9846 - val_loss: 0.3373 - val_acc: 0.9244\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.29716\n",
      "Epoch 99/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0393 - acc: 0.9865 - val_loss: 0.3295 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.29716\n",
      "Epoch 100/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0541 - acc: 0.9826 - val_loss: 0.3141 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.29716\n",
      "Epoch 101/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0499 - acc: 0.9839 - val_loss: 0.3148 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.29716\n",
      "Epoch 102/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0416 - acc: 0.9862 - val_loss: 0.3254 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.29716\n",
      "Epoch 103/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0433 - acc: 0.9862 - val_loss: 0.3265 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.29716\n",
      "Epoch 104/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0440 - acc: 0.9854 - val_loss: 0.3263 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.29716\n",
      "Epoch 105/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0474 - acc: 0.9850 - val_loss: 0.3185 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.29716\n",
      "Epoch 106/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0406 - acc: 0.9861 - val_loss: 0.3090 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.29716\n",
      "Epoch 107/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0456 - acc: 0.9853 - val_loss: 0.3356 - val_acc: 0.9254\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.29716\n",
      "Epoch 108/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0462 - acc: 0.9845 - val_loss: 0.3281 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.29716\n",
      "Epoch 109/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0395 - acc: 0.9873 - val_loss: 0.3200 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.29716\n",
      "Epoch 110/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0432 - acc: 0.9863 - val_loss: 0.3209 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.29716\n",
      "Epoch 111/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0429 - acc: 0.9853 - val_loss: 0.3348 - val_acc: 0.9239\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.29716\n",
      "Epoch 112/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0419 - acc: 0.9866 - val_loss: 0.3330 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.29716\n",
      "Epoch 113/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0431 - acc: 0.9851 - val_loss: 0.3302 - val_acc: 0.9239\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.29716\n",
      "Epoch 114/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0438 - acc: 0.9852 - val_loss: 0.3100 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.29716\n",
      "Epoch 115/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0466 - acc: 0.9847 - val_loss: 0.3199 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.29716\n",
      "Epoch 116/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0387 - acc: 0.9865 - val_loss: 0.3378 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.29716\n",
      "Epoch 117/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0399 - acc: 0.9868 - val_loss: 0.3240 - val_acc: 0.9254\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.29716\n",
      "Epoch 118/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0471 - acc: 0.9848 - val_loss: 0.3276 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.29716\n",
      "Epoch 119/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0406 - acc: 0.9869 - val_loss: 0.3287 - val_acc: 0.9254\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.29716\n",
      "Epoch 120/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0498 - acc: 0.9827 - val_loss: 0.3288 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.29716\n",
      "Epoch 121/1000\n",
      "17840/17840 [==============================] - 2s 108us/step - loss: 0.0490 - acc: 0.9858 - val_loss: 0.3075 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.29716\n",
      "Epoch 122/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0435 - acc: 0.9864 - val_loss: 0.3411 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.29716\n",
      "Epoch 123/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0446 - acc: 0.9850 - val_loss: 0.3209 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.29716\n",
      "Epoch 124/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0423 - acc: 0.9864 - val_loss: 0.3240 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.29716\n",
      "Epoch 125/1000\n",
      "17840/17840 [==============================] - 2s 113us/step - loss: 0.0441 - acc: 0.9854 - val_loss: 0.3297 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.29716\n",
      "Epoch 126/1000\n",
      "17840/17840 [==============================] - 2s 117us/step - loss: 0.0468 - acc: 0.9846 - val_loss: 0.3135 - val_acc: 0.9249\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.29716\n",
      "Epoch 127/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17840/17840 [==============================] - 2s 108us/step - loss: 0.0460 - acc: 0.9850 - val_loss: 0.3207 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.29716\n",
      "Epoch 128/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0447 - acc: 0.9855 - val_loss: 0.3182 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.29716\n",
      "Epoch 129/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0512 - acc: 0.9846 - val_loss: 0.3250 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.29716\n",
      "Epoch 130/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0480 - acc: 0.9853 - val_loss: 0.3077 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.29716\n",
      "Epoch 131/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0422 - acc: 0.9865 - val_loss: 0.3191 - val_acc: 0.9254\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.29716\n",
      "Epoch 132/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0474 - acc: 0.9846 - val_loss: 0.3014 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.29716\n",
      "Epoch 133/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0451 - acc: 0.9858 - val_loss: 0.3155 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.29716\n",
      "Epoch 134/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0432 - acc: 0.9850 - val_loss: 0.3160 - val_acc: 0.9334\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.29716\n",
      "Epoch 135/1000\n",
      "17840/17840 [==============================] - 2s 99us/step - loss: 0.0449 - acc: 0.9854 - val_loss: 0.3302 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.29716\n",
      "Epoch 136/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0415 - acc: 0.9858 - val_loss: 0.3280 - val_acc: 0.9254\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.29716\n",
      "Epoch 137/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0432 - acc: 0.9857 - val_loss: 0.3269 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.29716\n",
      "Epoch 138/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0461 - acc: 0.9850 - val_loss: 0.3256 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.29716\n",
      "Epoch 139/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0441 - acc: 0.9850 - val_loss: 0.3317 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.29716\n",
      "Epoch 140/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0377 - acc: 0.9873 - val_loss: 0.3500 - val_acc: 0.9244\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.29716\n",
      "Epoch 141/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0434 - acc: 0.9868 - val_loss: 0.3388 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.29716\n",
      "Epoch 142/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0437 - acc: 0.9847 - val_loss: 0.3224 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.29716\n",
      "Epoch 143/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0492 - acc: 0.9836 - val_loss: 0.3284 - val_acc: 0.9254\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.29716\n",
      "Epoch 144/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0406 - acc: 0.9863 - val_loss: 0.3251 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.29716\n",
      "Epoch 145/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0407 - acc: 0.9862 - val_loss: 0.3245 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.29716\n",
      "Epoch 146/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0412 - acc: 0.9861 - val_loss: 0.3300 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.29716\n",
      "Epoch 147/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0464 - acc: 0.9858 - val_loss: 0.3195 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.29716\n",
      "Epoch 148/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0452 - acc: 0.9856 - val_loss: 0.3244 - val_acc: 0.9239\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.29716\n",
      "Epoch 149/1000\n",
      "17840/17840 [==============================] - 2s 108us/step - loss: 0.0439 - acc: 0.9849 - val_loss: 0.3239 - val_acc: 0.9244\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.29716\n",
      "Epoch 150/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0433 - acc: 0.9853 - val_loss: 0.3159 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.29716\n",
      "Epoch 151/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0432 - acc: 0.9865 - val_loss: 0.3189 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.29716\n",
      "Epoch 152/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0394 - acc: 0.9860 - val_loss: 0.3463 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.29716\n",
      "Epoch 153/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0418 - acc: 0.9866 - val_loss: 0.3345 - val_acc: 0.9239\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.29716\n",
      "Epoch 154/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0448 - acc: 0.9863 - val_loss: 0.3257 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.29716\n",
      "Epoch 155/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0462 - acc: 0.9853 - val_loss: 0.3151 - val_acc: 0.9249\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.29716\n",
      "Epoch 156/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0469 - acc: 0.9846 - val_loss: 0.3160 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.29716\n",
      "Epoch 157/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0408 - acc: 0.9875 - val_loss: 0.3157 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.29716\n",
      "Epoch 158/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0424 - acc: 0.9861 - val_loss: 0.3308 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.29716\n",
      "Epoch 159/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0448 - acc: 0.9851 - val_loss: 0.3102 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.29716\n",
      "Epoch 160/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0414 - acc: 0.9857 - val_loss: 0.3215 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.29716\n",
      "Epoch 161/1000\n",
      "17840/17840 [==============================] - 2s 116us/step - loss: 0.0384 - acc: 0.9869 - val_loss: 0.3443 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.29716\n",
      "Epoch 162/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0459 - acc: 0.9845 - val_loss: 0.3263 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.29716\n",
      "Epoch 163/1000\n",
      "17840/17840 [==============================] - 2s 114us/step - loss: 0.0423 - acc: 0.9863 - val_loss: 0.3164 - val_acc: 0.9223\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.29716\n",
      "Epoch 164/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0384 - acc: 0.9873 - val_loss: 0.3236 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.29716\n",
      "Epoch 165/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0420 - acc: 0.9864 - val_loss: 0.3112 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.29716\n",
      "Epoch 166/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0454 - acc: 0.9854 - val_loss: 0.3081 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.29716\n",
      "Epoch 167/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0436 - acc: 0.9850 - val_loss: 0.3257 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.29716\n",
      "Epoch 168/1000\n",
      "17840/17840 [==============================] - 2s 110us/step - loss: 0.0417 - acc: 0.9867 - val_loss: 0.3220 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.29716\n",
      "Epoch 169/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0428 - acc: 0.9863 - val_loss: 0.3265 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.29716\n",
      "Epoch 170/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0426 - acc: 0.9852 - val_loss: 0.3171 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.29716\n",
      "Epoch 171/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0398 - acc: 0.9869 - val_loss: 0.3282 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.29716\n",
      "Epoch 172/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0389 - acc: 0.9871 - val_loss: 0.3245 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.29716\n",
      "Epoch 173/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0357 - acc: 0.9877 - val_loss: 0.3493 - val_acc: 0.9244\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.29716\n",
      "Epoch 174/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0418 - acc: 0.9858 - val_loss: 0.3224 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.29716\n",
      "Epoch 175/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0385 - acc: 0.9873 - val_loss: 0.3154 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.29716\n",
      "Epoch 176/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0427 - acc: 0.9865 - val_loss: 0.3364 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.29716\n",
      "Epoch 177/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0422 - acc: 0.9858 - val_loss: 0.3040 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.29716\n",
      "Epoch 178/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0454 - acc: 0.9858 - val_loss: 0.3203 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.29716\n",
      "Epoch 179/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0437 - acc: 0.9860 - val_loss: 0.3177 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.29716\n",
      "Epoch 180/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0451 - acc: 0.9868 - val_loss: 0.3177 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.29716\n",
      "Epoch 181/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0462 - acc: 0.9849 - val_loss: 0.2976 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.29716\n",
      "Epoch 182/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0435 - acc: 0.9855 - val_loss: 0.3018 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.29716\n",
      "Epoch 183/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0431 - acc: 0.9849 - val_loss: 0.3082 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.29716\n",
      "Epoch 184/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0409 - acc: 0.9870 - val_loss: 0.3110 - val_acc: 0.9249\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.29716\n",
      "Epoch 185/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0456 - acc: 0.9853 - val_loss: 0.3069 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.29716\n",
      "Epoch 186/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0510 - acc: 0.9833 - val_loss: 0.2946 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.29716 to 0.29459, saving model to weights.best.madar6_1000min.hdf5\n",
      "Epoch 187/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0419 - acc: 0.9864 - val_loss: 0.3021 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.29459\n",
      "Epoch 188/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0410 - acc: 0.9863 - val_loss: 0.3347 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.29459\n",
      "Epoch 189/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0362 - acc: 0.9876 - val_loss: 0.3234 - val_acc: 0.9249\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.29459\n",
      "Epoch 190/1000\n",
      "17840/17840 [==============================] - 2s 111us/step - loss: 0.0457 - acc: 0.9848 - val_loss: 0.3073 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.29459\n",
      "Epoch 191/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0393 - acc: 0.9869 - val_loss: 0.3417 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.29459\n",
      "Epoch 192/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0487 - acc: 0.9847 - val_loss: 0.3187 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.29459\n",
      "Epoch 193/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0416 - acc: 0.9865 - val_loss: 0.3156 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.29459\n",
      "Epoch 194/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0403 - acc: 0.9865 - val_loss: 0.3243 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.29459\n",
      "Epoch 195/1000\n",
      "17840/17840 [==============================] - 2s 112us/step - loss: 0.0431 - acc: 0.9862 - val_loss: 0.3271 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.29459\n",
      "Epoch 196/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0462 - acc: 0.9851 - val_loss: 0.3112 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.29459\n",
      "Epoch 197/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0393 - acc: 0.9869 - val_loss: 0.3221 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.29459\n",
      "Epoch 198/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0407 - acc: 0.9873 - val_loss: 0.3411 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.29459\n",
      "Epoch 199/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0378 - acc: 0.9874 - val_loss: 0.3157 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.29459\n",
      "Epoch 200/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0439 - acc: 0.9847 - val_loss: 0.3228 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.29459\n",
      "Epoch 201/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0425 - acc: 0.9872 - val_loss: 0.3132 - val_acc: 0.9254\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.29459\n",
      "Epoch 202/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0469 - acc: 0.9847 - val_loss: 0.3183 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.29459\n",
      "Epoch 203/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0430 - acc: 0.9862 - val_loss: 0.3225 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.29459\n",
      "Epoch 204/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0401 - acc: 0.9871 - val_loss: 0.3318 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.29459\n",
      "Epoch 205/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0412 - acc: 0.9863 - val_loss: 0.3540 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.29459\n",
      "Epoch 206/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0464 - acc: 0.9848 - val_loss: 0.3099 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.29459\n",
      "Epoch 207/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0477 - acc: 0.9846 - val_loss: 0.3077 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.29459\n",
      "Epoch 208/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0420 - acc: 0.9867 - val_loss: 0.3283 - val_acc: 0.9249\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.29459\n",
      "Epoch 209/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0387 - acc: 0.9869 - val_loss: 0.3120 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.29459\n",
      "Epoch 210/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0424 - acc: 0.9863 - val_loss: 0.3221 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.29459\n",
      "Epoch 211/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0411 - acc: 0.9874 - val_loss: 0.3290 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.29459\n",
      "Epoch 212/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0449 - acc: 0.9856 - val_loss: 0.3040 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.29459\n",
      "Epoch 213/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0430 - acc: 0.9867 - val_loss: 0.3111 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.29459\n",
      "Epoch 214/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0363 - acc: 0.9876 - val_loss: 0.3296 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.29459\n",
      "Epoch 215/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0387 - acc: 0.9876 - val_loss: 0.3340 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.29459\n",
      "Epoch 216/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0413 - acc: 0.9855 - val_loss: 0.3287 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.29459\n",
      "Epoch 217/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0451 - acc: 0.9855 - val_loss: 0.3173 - val_acc: 0.9324\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.29459\n",
      "Epoch 218/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0448 - acc: 0.9853 - val_loss: 0.3125 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.29459\n",
      "Epoch 219/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0440 - acc: 0.9864 - val_loss: 0.3182 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.29459\n",
      "Epoch 220/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0422 - acc: 0.9853 - val_loss: 0.3379 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.29459\n",
      "Epoch 221/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0429 - acc: 0.9849 - val_loss: 0.3004 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.29459\n",
      "Epoch 222/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0409 - acc: 0.9867 - val_loss: 0.3246 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.29459\n",
      "Epoch 223/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0406 - acc: 0.9867 - val_loss: 0.3022 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.29459\n",
      "Epoch 224/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0409 - acc: 0.9855 - val_loss: 0.3236 - val_acc: 0.9239\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.29459\n",
      "Epoch 225/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0438 - acc: 0.9855 - val_loss: 0.3289 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.29459\n",
      "Epoch 226/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0418 - acc: 0.9862 - val_loss: 0.3258 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.29459\n",
      "Epoch 227/1000\n",
      "17840/17840 [==============================] - 2s 113us/step - loss: 0.0474 - acc: 0.9849 - val_loss: 0.3388 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.29459\n",
      "Epoch 228/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0500 - acc: 0.9830 - val_loss: 0.2999 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.29459\n",
      "Epoch 229/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0478 - acc: 0.9845 - val_loss: 0.3201 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.29459\n",
      "Epoch 230/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0386 - acc: 0.9864 - val_loss: 0.3274 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.29459\n",
      "Epoch 231/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0400 - acc: 0.9878 - val_loss: 0.3330 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.29459\n",
      "Epoch 232/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0374 - acc: 0.9883 - val_loss: 0.3118 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.29459\n",
      "Epoch 233/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0442 - acc: 0.9853 - val_loss: 0.3083 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.29459\n",
      "Epoch 234/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0447 - acc: 0.9867 - val_loss: 0.3030 - val_acc: 0.9319\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.29459\n",
      "Epoch 235/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0414 - acc: 0.9868 - val_loss: 0.3173 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.29459\n",
      "Epoch 236/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0458 - acc: 0.9855 - val_loss: 0.3037 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.29459\n",
      "Epoch 237/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0393 - acc: 0.9867 - val_loss: 0.3246 - val_acc: 0.9319\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.29459\n",
      "Epoch 238/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0451 - acc: 0.9851 - val_loss: 0.3162 - val_acc: 0.9319\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.29459\n",
      "Epoch 239/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0376 - acc: 0.9876 - val_loss: 0.3180 - val_acc: 0.9249\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.29459\n",
      "Epoch 240/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0436 - acc: 0.9857 - val_loss: 0.3179 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.29459\n",
      "Epoch 241/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0476 - acc: 0.9846 - val_loss: 0.2966 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.29459\n",
      "Epoch 242/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0418 - acc: 0.9862 - val_loss: 0.3128 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.29459\n",
      "Epoch 243/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0403 - acc: 0.9867 - val_loss: 0.3150 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.29459\n",
      "Epoch 244/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0411 - acc: 0.9864 - val_loss: 0.3279 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.29459\n",
      "Epoch 245/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0445 - acc: 0.9858 - val_loss: 0.3128 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.29459\n",
      "Epoch 246/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0409 - acc: 0.9861 - val_loss: 0.3172 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.29459\n",
      "Epoch 247/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0398 - acc: 0.9867 - val_loss: 0.3161 - val_acc: 0.9228\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.29459\n",
      "Epoch 248/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0399 - acc: 0.9869 - val_loss: 0.3335 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.29459\n",
      "Epoch 249/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0401 - acc: 0.9874 - val_loss: 0.3271 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.29459\n",
      "Epoch 250/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0401 - acc: 0.9864 - val_loss: 0.3291 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.29459\n",
      "Epoch 251/1000\n",
      "17840/17840 [==============================] - 2s 108us/step - loss: 0.0395 - acc: 0.9874 - val_loss: 0.3154 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.29459\n",
      "Epoch 252/1000\n",
      "17840/17840 [==============================] - 2s 109us/step - loss: 0.0353 - acc: 0.9872 - val_loss: 0.3405 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 0.29459\n",
      "Epoch 253/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0475 - acc: 0.9851 - val_loss: 0.3319 - val_acc: 0.9254\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.29459\n",
      "Epoch 254/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0453 - acc: 0.9849 - val_loss: 0.3344 - val_acc: 0.9233\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.29459\n",
      "Epoch 255/1000\n",
      "17840/17840 [==============================] - 2s 108us/step - loss: 0.0444 - acc: 0.9853 - val_loss: 0.3210 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.29459\n",
      "Epoch 256/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0442 - acc: 0.9852 - val_loss: 0.3059 - val_acc: 0.9254\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.29459\n",
      "Epoch 257/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0410 - acc: 0.9869 - val_loss: 0.3327 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 0.29459\n",
      "Epoch 258/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0390 - acc: 0.9857 - val_loss: 0.3179 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.29459\n",
      "Epoch 259/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0403 - acc: 0.9861 - val_loss: 0.3301 - val_acc: 0.9203\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.29459\n",
      "Epoch 260/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0380 - acc: 0.9864 - val_loss: 0.3366 - val_acc: 0.9239\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.29459\n",
      "Epoch 261/1000\n",
      "17840/17840 [==============================] - 2s 99us/step - loss: 0.0479 - acc: 0.9844 - val_loss: 0.3180 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.29459\n",
      "Epoch 262/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0457 - acc: 0.9849 - val_loss: 0.3032 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.29459\n",
      "Epoch 263/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0413 - acc: 0.9870 - val_loss: 0.3116 - val_acc: 0.9249\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.29459\n",
      "Epoch 264/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0428 - acc: 0.9855 - val_loss: 0.3265 - val_acc: 0.9254\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.29459\n",
      "Epoch 265/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0406 - acc: 0.9865 - val_loss: 0.3204 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.29459\n",
      "Epoch 266/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0402 - acc: 0.9861 - val_loss: 0.3156 - val_acc: 0.9329\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.29459\n",
      "Epoch 267/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0469 - acc: 0.9847 - val_loss: 0.2988 - val_acc: 0.9334\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.29459\n",
      "Epoch 268/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0386 - acc: 0.9874 - val_loss: 0.3273 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.29459\n",
      "Epoch 269/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0421 - acc: 0.9862 - val_loss: 0.3329 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.29459\n",
      "Epoch 270/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0462 - acc: 0.9851 - val_loss: 0.2917 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00270: val_loss improved from 0.29459 to 0.29175, saving model to weights.best.madar6_1000min.hdf5\n",
      "Epoch 271/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0444 - acc: 0.9859 - val_loss: 0.3122 - val_acc: 0.9319\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.29175\n",
      "Epoch 272/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0416 - acc: 0.9866 - val_loss: 0.2984 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.29175\n",
      "Epoch 273/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0372 - acc: 0.9878 - val_loss: 0.3163 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.29175\n",
      "Epoch 274/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0384 - acc: 0.9878 - val_loss: 0.3285 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.29175\n",
      "Epoch 275/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0375 - acc: 0.9871 - val_loss: 0.3108 - val_acc: 0.9329\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.29175\n",
      "Epoch 276/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0408 - acc: 0.9866 - val_loss: 0.3452 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.29175\n",
      "Epoch 277/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0434 - acc: 0.9858 - val_loss: 0.3062 - val_acc: 0.9324\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.29175\n",
      "Epoch 278/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0466 - acc: 0.9852 - val_loss: 0.3309 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.29175\n",
      "Epoch 279/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0501 - acc: 0.9841 - val_loss: 0.3210 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.29175\n",
      "Epoch 280/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0381 - acc: 0.9867 - val_loss: 0.3155 - val_acc: 0.9344\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.29175\n",
      "Epoch 281/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0367 - acc: 0.9885 - val_loss: 0.3269 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.29175\n",
      "Epoch 282/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0397 - acc: 0.9867 - val_loss: 0.3082 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.29175\n",
      "Epoch 283/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0419 - acc: 0.9865 - val_loss: 0.3071 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.29175\n",
      "Epoch 284/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0411 - acc: 0.9867 - val_loss: 0.3310 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.29175\n",
      "Epoch 285/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0323 - acc: 0.9884 - val_loss: 0.3188 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.29175\n",
      "Epoch 286/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0422 - acc: 0.9870 - val_loss: 0.3228 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.29175\n",
      "Epoch 287/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0454 - acc: 0.9857 - val_loss: 0.3225 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.29175\n",
      "Epoch 288/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0404 - acc: 0.9877 - val_loss: 0.3089 - val_acc: 0.9249\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.29175\n",
      "Epoch 289/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0426 - acc: 0.9858 - val_loss: 0.3300 - val_acc: 0.9233\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.29175\n",
      "Epoch 290/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0374 - acc: 0.9874 - val_loss: 0.3203 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.29175\n",
      "Epoch 291/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0476 - acc: 0.9850 - val_loss: 0.3257 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.29175\n",
      "Epoch 292/1000\n",
      "17840/17840 [==============================] - 2s 109us/step - loss: 0.0416 - acc: 0.9860 - val_loss: 0.3215 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.29175\n",
      "Epoch 293/1000\n",
      "17840/17840 [==============================] - 2s 109us/step - loss: 0.0430 - acc: 0.9855 - val_loss: 0.3018 - val_acc: 0.9324\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.29175\n",
      "Epoch 294/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0432 - acc: 0.9863 - val_loss: 0.3097 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.29175\n",
      "Epoch 295/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0380 - acc: 0.9864 - val_loss: 0.3222 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.29175\n",
      "Epoch 296/1000\n",
      "17840/17840 [==============================] - 2s 99us/step - loss: 0.0421 - acc: 0.9865 - val_loss: 0.3254 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.29175\n",
      "Epoch 297/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0435 - acc: 0.9853 - val_loss: 0.3263 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.29175\n",
      "Epoch 298/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0413 - acc: 0.9869 - val_loss: 0.3223 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.29175\n",
      "Epoch 299/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0458 - acc: 0.9849 - val_loss: 0.3175 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 0.29175\n",
      "Epoch 300/1000\n",
      "17840/17840 [==============================] - 2s 98us/step - loss: 0.0389 - acc: 0.9871 - val_loss: 0.3134 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.29175\n",
      "Epoch 301/1000\n",
      "17840/17840 [==============================] - 2s 99us/step - loss: 0.0381 - acc: 0.9880 - val_loss: 0.3083 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.29175\n",
      "Epoch 302/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0402 - acc: 0.9864 - val_loss: 0.3297 - val_acc: 0.9239\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.29175\n",
      "Epoch 303/1000\n",
      "17840/17840 [==============================] - 2s 99us/step - loss: 0.0385 - acc: 0.9866 - val_loss: 0.3280 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.29175\n",
      "Epoch 304/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0440 - acc: 0.9849 - val_loss: 0.3346 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00304: val_loss did not improve from 0.29175\n",
      "Epoch 305/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0401 - acc: 0.9867 - val_loss: 0.3146 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.29175\n",
      "Epoch 306/1000\n",
      "17840/17840 [==============================] - 2s 99us/step - loss: 0.0369 - acc: 0.9884 - val_loss: 0.3142 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00306: val_loss did not improve from 0.29175\n",
      "Epoch 307/1000\n",
      "17840/17840 [==============================] - 2s 99us/step - loss: 0.0397 - acc: 0.9869 - val_loss: 0.3202 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.29175\n",
      "Epoch 308/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0432 - acc: 0.9865 - val_loss: 0.3136 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.29175\n",
      "Epoch 309/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0404 - acc: 0.9862 - val_loss: 0.3247 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.29175\n",
      "Epoch 310/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0407 - acc: 0.9872 - val_loss: 0.3337 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.29175\n",
      "Epoch 311/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0399 - acc: 0.9871 - val_loss: 0.3118 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.29175\n",
      "Epoch 312/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0405 - acc: 0.9854 - val_loss: 0.3130 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.29175\n",
      "Epoch 313/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0362 - acc: 0.9883 - val_loss: 0.3106 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.29175\n",
      "Epoch 314/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0385 - acc: 0.9882 - val_loss: 0.3133 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.29175\n",
      "Epoch 315/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0417 - acc: 0.9866 - val_loss: 0.3419 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.29175\n",
      "Epoch 316/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0441 - acc: 0.9862 - val_loss: 0.3233 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.29175\n",
      "Epoch 317/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0458 - acc: 0.9858 - val_loss: 0.3146 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 0.29175\n",
      "Epoch 318/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0443 - acc: 0.9854 - val_loss: 0.3078 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.29175\n",
      "Epoch 319/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0410 - acc: 0.9860 - val_loss: 0.3241 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.29175\n",
      "Epoch 320/1000\n",
      "17840/17840 [==============================] - 2s 110us/step - loss: 0.0394 - acc: 0.9877 - val_loss: 0.3188 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.29175\n",
      "Epoch 321/1000\n",
      "17840/17840 [==============================] - 2s 110us/step - loss: 0.0419 - acc: 0.9876 - val_loss: 0.3191 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.29175\n",
      "Epoch 322/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0403 - acc: 0.9857 - val_loss: 0.3089 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.29175\n",
      "Epoch 323/1000\n",
      "17840/17840 [==============================] - 2s 109us/step - loss: 0.0336 - acc: 0.9885 - val_loss: 0.3223 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.29175\n",
      "Epoch 324/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0405 - acc: 0.9870 - val_loss: 0.3178 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.29175\n",
      "Epoch 325/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0400 - acc: 0.9864 - val_loss: 0.3074 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.29175\n",
      "Epoch 326/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0440 - acc: 0.9865 - val_loss: 0.3190 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.29175\n",
      "Epoch 327/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0413 - acc: 0.9861 - val_loss: 0.3209 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.29175\n",
      "Epoch 328/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0373 - acc: 0.9867 - val_loss: 0.3332 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.29175\n",
      "Epoch 329/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0443 - acc: 0.9851 - val_loss: 0.3189 - val_acc: 0.9244\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.29175\n",
      "Epoch 330/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0386 - acc: 0.9877 - val_loss: 0.3191 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.29175\n",
      "Epoch 331/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0383 - acc: 0.9877 - val_loss: 0.3317 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.29175\n",
      "Epoch 332/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0389 - acc: 0.9877 - val_loss: 0.3277 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.29175\n",
      "Epoch 333/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0390 - acc: 0.9873 - val_loss: 0.3218 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.29175\n",
      "Epoch 334/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0424 - acc: 0.9848 - val_loss: 0.3148 - val_acc: 0.9319\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.29175\n",
      "Epoch 335/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0382 - acc: 0.9878 - val_loss: 0.3213 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.29175\n",
      "Epoch 336/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0431 - acc: 0.9862 - val_loss: 0.3162 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.29175\n",
      "Epoch 337/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0447 - acc: 0.9859 - val_loss: 0.3165 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.29175\n",
      "Epoch 338/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0399 - acc: 0.9873 - val_loss: 0.3209 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.29175\n",
      "Epoch 339/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0375 - acc: 0.9884 - val_loss: 0.3272 - val_acc: 0.9324\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.29175\n",
      "Epoch 340/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0438 - acc: 0.9855 - val_loss: 0.3116 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.29175\n",
      "Epoch 341/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0372 - acc: 0.9883 - val_loss: 0.3424 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.29175\n",
      "Epoch 342/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0462 - acc: 0.9855 - val_loss: 0.3336 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.29175\n",
      "Epoch 343/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0384 - acc: 0.9869 - val_loss: 0.3341 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.29175\n",
      "Epoch 344/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0420 - acc: 0.9872 - val_loss: 0.3205 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.29175\n",
      "Epoch 345/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0414 - acc: 0.9861 - val_loss: 0.3154 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.29175\n",
      "Epoch 346/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0400 - acc: 0.9872 - val_loss: 0.3314 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.29175\n",
      "Epoch 347/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0448 - acc: 0.9861 - val_loss: 0.3223 - val_acc: 0.9339\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.29175\n",
      "Epoch 348/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0431 - acc: 0.9862 - val_loss: 0.3087 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00348: val_loss did not improve from 0.29175\n",
      "Epoch 349/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0422 - acc: 0.9859 - val_loss: 0.3040 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.29175\n",
      "Epoch 350/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0421 - acc: 0.9855 - val_loss: 0.3071 - val_acc: 0.9319\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.29175\n",
      "Epoch 351/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0420 - acc: 0.9864 - val_loss: 0.3105 - val_acc: 0.9233\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.29175\n",
      "Epoch 352/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0426 - acc: 0.9864 - val_loss: 0.3277 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.29175\n",
      "Epoch 353/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0366 - acc: 0.9879 - val_loss: 0.3267 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.29175\n",
      "Epoch 354/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0368 - acc: 0.9884 - val_loss: 0.3185 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.29175\n",
      "Epoch 355/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0416 - acc: 0.9867 - val_loss: 0.3224 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.29175\n",
      "Epoch 356/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0399 - acc: 0.9859 - val_loss: 0.3141 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.29175\n",
      "Epoch 357/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0432 - acc: 0.9870 - val_loss: 0.3137 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.29175\n",
      "Epoch 358/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0359 - acc: 0.9884 - val_loss: 0.3286 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.29175\n",
      "Epoch 359/1000\n",
      "17840/17840 [==============================] - 2s 108us/step - loss: 0.0397 - acc: 0.9872 - val_loss: 0.3311 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.29175\n",
      "Epoch 360/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0468 - acc: 0.9846 - val_loss: 0.3188 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.29175\n",
      "Epoch 361/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0412 - acc: 0.9862 - val_loss: 0.3246 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.29175\n",
      "Epoch 362/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0400 - acc: 0.9868 - val_loss: 0.3119 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.29175\n",
      "Epoch 363/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0391 - acc: 0.9875 - val_loss: 0.3286 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.29175\n",
      "Epoch 364/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0405 - acc: 0.9859 - val_loss: 0.3237 - val_acc: 0.9254\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.29175\n",
      "Epoch 365/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0451 - acc: 0.9866 - val_loss: 0.3106 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.29175\n",
      "Epoch 366/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0401 - acc: 0.9871 - val_loss: 0.3090 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.29175\n",
      "Epoch 367/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0404 - acc: 0.9872 - val_loss: 0.3241 - val_acc: 0.9228\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.29175\n",
      "Epoch 368/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0314 - acc: 0.9894 - val_loss: 0.3213 - val_acc: 0.9244\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.29175\n",
      "Epoch 369/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0469 - acc: 0.9855 - val_loss: 0.3270 - val_acc: 0.9249\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.29175\n",
      "Epoch 370/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0373 - acc: 0.9874 - val_loss: 0.3355 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.29175\n",
      "Epoch 371/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0392 - acc: 0.9869 - val_loss: 0.3243 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.29175\n",
      "Epoch 372/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0331 - acc: 0.9883 - val_loss: 0.3246 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.29175\n",
      "Epoch 373/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0368 - acc: 0.9883 - val_loss: 0.3274 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.29175\n",
      "Epoch 374/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0482 - acc: 0.9845 - val_loss: 0.3243 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.29175\n",
      "Epoch 375/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0445 - acc: 0.9855 - val_loss: 0.3101 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 0.29175\n",
      "Epoch 376/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0376 - acc: 0.9883 - val_loss: 0.3238 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.29175\n",
      "Epoch 377/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0397 - acc: 0.9874 - val_loss: 0.3167 - val_acc: 0.9319\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.29175\n",
      "Epoch 378/1000\n",
      "17840/17840 [==============================] - 2s 108us/step - loss: 0.0421 - acc: 0.9859 - val_loss: 0.3184 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.29175\n",
      "Epoch 379/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0379 - acc: 0.9881 - val_loss: 0.3241 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.29175\n",
      "Epoch 380/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0366 - acc: 0.9881 - val_loss: 0.3286 - val_acc: 0.9244\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.29175\n",
      "Epoch 381/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0395 - acc: 0.9869 - val_loss: 0.3156 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.29175\n",
      "Epoch 382/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0388 - acc: 0.9865 - val_loss: 0.3038 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.29175\n",
      "Epoch 383/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0405 - acc: 0.9879 - val_loss: 0.3187 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 0.29175\n",
      "Epoch 384/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0421 - acc: 0.9867 - val_loss: 0.3108 - val_acc: 0.9249\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.29175\n",
      "Epoch 385/1000\n",
      "17840/17840 [==============================] - 2s 111us/step - loss: 0.0398 - acc: 0.9874 - val_loss: 0.3027 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.29175\n",
      "Epoch 386/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0371 - acc: 0.9887 - val_loss: 0.3214 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.29175\n",
      "Epoch 387/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0423 - acc: 0.9863 - val_loss: 0.3300 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.29175\n",
      "Epoch 388/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0341 - acc: 0.9882 - val_loss: 0.3223 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.29175\n",
      "Epoch 389/1000\n",
      "17840/17840 [==============================] - 2s 122us/step - loss: 0.0406 - acc: 0.9867 - val_loss: 0.2977 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.29175\n",
      "Epoch 390/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0411 - acc: 0.9859 - val_loss: 0.3079 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.29175\n",
      "Epoch 391/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0347 - acc: 0.9874 - val_loss: 0.3260 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.29175\n",
      "Epoch 392/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0382 - acc: 0.9881 - val_loss: 0.3131 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.29175\n",
      "Epoch 393/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0397 - acc: 0.9876 - val_loss: 0.3125 - val_acc: 0.9213\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.29175\n",
      "Epoch 394/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0400 - acc: 0.9866 - val_loss: 0.3072 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.29175\n",
      "Epoch 395/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0433 - acc: 0.9858 - val_loss: 0.3048 - val_acc: 0.9319\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.29175\n",
      "Epoch 396/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0423 - acc: 0.9875 - val_loss: 0.3367 - val_acc: 0.9254\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.29175\n",
      "Epoch 397/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0406 - acc: 0.9872 - val_loss: 0.2983 - val_acc: 0.9339\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.29175\n",
      "Epoch 398/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0409 - acc: 0.9871 - val_loss: 0.3086 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.29175\n",
      "Epoch 399/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0471 - acc: 0.9862 - val_loss: 0.3075 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 0.29175\n",
      "Epoch 400/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0332 - acc: 0.9900 - val_loss: 0.3158 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.29175\n",
      "Epoch 401/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0415 - acc: 0.9872 - val_loss: 0.3268 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.29175\n",
      "Epoch 402/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0425 - acc: 0.9868 - val_loss: 0.3143 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.29175\n",
      "Epoch 403/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0398 - acc: 0.9868 - val_loss: 0.3333 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.29175\n",
      "Epoch 404/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0432 - acc: 0.9868 - val_loss: 0.3069 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.29175\n",
      "Epoch 405/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0412 - acc: 0.9853 - val_loss: 0.3138 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.29175\n",
      "Epoch 406/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0379 - acc: 0.9878 - val_loss: 0.3207 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.29175\n",
      "Epoch 407/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0445 - acc: 0.9862 - val_loss: 0.3108 - val_acc: 0.9324\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.29175\n",
      "Epoch 408/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0424 - acc: 0.9859 - val_loss: 0.3035 - val_acc: 0.9329\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.29175\n",
      "Epoch 409/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0406 - acc: 0.9863 - val_loss: 0.3231 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.29175\n",
      "Epoch 410/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0362 - acc: 0.9885 - val_loss: 0.3119 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.29175\n",
      "Epoch 411/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0380 - acc: 0.9871 - val_loss: 0.3316 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.29175\n",
      "Epoch 412/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0422 - acc: 0.9872 - val_loss: 0.2988 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 0.29175\n",
      "Epoch 413/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0426 - acc: 0.9866 - val_loss: 0.2985 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.29175\n",
      "Epoch 414/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0353 - acc: 0.9879 - val_loss: 0.3183 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.29175\n",
      "Epoch 415/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0407 - acc: 0.9871 - val_loss: 0.3214 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.29175\n",
      "Epoch 416/1000\n",
      "17840/17840 [==============================] - 2s 108us/step - loss: 0.0390 - acc: 0.9875 - val_loss: 0.3200 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.29175\n",
      "Epoch 417/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0433 - acc: 0.9867 - val_loss: 0.3248 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 0.29175\n",
      "Epoch 418/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0414 - acc: 0.9870 - val_loss: 0.3179 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 0.29175\n",
      "Epoch 419/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0450 - acc: 0.9850 - val_loss: 0.3193 - val_acc: 0.9249\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.29175\n",
      "Epoch 420/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0414 - acc: 0.9876 - val_loss: 0.3127 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.29175\n",
      "Epoch 421/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0404 - acc: 0.9870 - val_loss: 0.3148 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.29175\n",
      "Epoch 422/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0363 - acc: 0.9876 - val_loss: 0.3155 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.29175\n",
      "Epoch 423/1000\n",
      "17840/17840 [==============================] - 2s 99us/step - loss: 0.0349 - acc: 0.9878 - val_loss: 0.3232 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 0.29175\n",
      "Epoch 424/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0405 - acc: 0.9869 - val_loss: 0.3127 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 0.29175\n",
      "Epoch 425/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0375 - acc: 0.9872 - val_loss: 0.3012 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00425: val_loss did not improve from 0.29175\n",
      "Epoch 426/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0393 - acc: 0.9873 - val_loss: 0.3117 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 0.29175\n",
      "Epoch 427/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0389 - acc: 0.9882 - val_loss: 0.3126 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.29175\n",
      "Epoch 428/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0416 - acc: 0.9866 - val_loss: 0.3024 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.29175\n",
      "Epoch 429/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0402 - acc: 0.9871 - val_loss: 0.2905 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00429: val_loss improved from 0.29175 to 0.29054, saving model to weights.best.madar6_1000min.hdf5\n",
      "Epoch 430/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0397 - acc: 0.9867 - val_loss: 0.3181 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.29054\n",
      "Epoch 431/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0472 - acc: 0.9845 - val_loss: 0.3300 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.29054\n",
      "Epoch 432/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0421 - acc: 0.9877 - val_loss: 0.3134 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.29054\n",
      "Epoch 433/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0395 - acc: 0.9872 - val_loss: 0.3121 - val_acc: 0.9329\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 0.29054\n",
      "Epoch 434/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0358 - acc: 0.9882 - val_loss: 0.3071 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.29054\n",
      "Epoch 435/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0429 - acc: 0.9869 - val_loss: 0.3091 - val_acc: 0.9334\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 0.29054\n",
      "Epoch 436/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0414 - acc: 0.9869 - val_loss: 0.3037 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.29054\n",
      "Epoch 437/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0347 - acc: 0.9886 - val_loss: 0.3164 - val_acc: 0.9319\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.29054\n",
      "Epoch 438/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0371 - acc: 0.9879 - val_loss: 0.3076 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 0.29054\n",
      "Epoch 439/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0372 - acc: 0.9874 - val_loss: 0.3159 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.29054\n",
      "Epoch 440/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0423 - acc: 0.9857 - val_loss: 0.3063 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 0.29054\n",
      "Epoch 441/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0397 - acc: 0.9864 - val_loss: 0.3137 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.29054\n",
      "Epoch 442/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0359 - acc: 0.9880 - val_loss: 0.3325 - val_acc: 0.9254\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.29054\n",
      "Epoch 443/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0335 - acc: 0.9885 - val_loss: 0.3139 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.29054\n",
      "Epoch 444/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0412 - acc: 0.9876 - val_loss: 0.3226 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.29054\n",
      "Epoch 445/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0346 - acc: 0.9880 - val_loss: 0.3312 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.29054\n",
      "Epoch 446/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0399 - acc: 0.9871 - val_loss: 0.3457 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.29054\n",
      "Epoch 447/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0438 - acc: 0.9867 - val_loss: 0.3196 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 0.29054\n",
      "Epoch 448/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0426 - acc: 0.9863 - val_loss: 0.3116 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.29054\n",
      "Epoch 449/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0357 - acc: 0.9891 - val_loss: 0.3191 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.29054\n",
      "Epoch 450/1000\n",
      "17840/17840 [==============================] - 2s 113us/step - loss: 0.0420 - acc: 0.9876 - val_loss: 0.3111 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 0.29054\n",
      "Epoch 451/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0436 - acc: 0.9859 - val_loss: 0.2974 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.29054\n",
      "Epoch 452/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0389 - acc: 0.9871 - val_loss: 0.2981 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.29054\n",
      "Epoch 453/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0426 - acc: 0.9863 - val_loss: 0.3057 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 0.29054\n",
      "Epoch 454/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0391 - acc: 0.9874 - val_loss: 0.3081 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.29054\n",
      "Epoch 455/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0373 - acc: 0.9878 - val_loss: 0.3279 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 0.29054\n",
      "Epoch 456/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0353 - acc: 0.9883 - val_loss: 0.3298 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.29054\n",
      "Epoch 457/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0359 - acc: 0.9882 - val_loss: 0.3094 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 0.29054\n",
      "Epoch 458/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0450 - acc: 0.9862 - val_loss: 0.3132 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 0.29054\n",
      "Epoch 459/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0403 - acc: 0.9870 - val_loss: 0.2979 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 0.29054\n",
      "Epoch 460/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0399 - acc: 0.9869 - val_loss: 0.3040 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.29054\n",
      "Epoch 461/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0362 - acc: 0.9882 - val_loss: 0.3221 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.29054\n",
      "Epoch 462/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0401 - acc: 0.9873 - val_loss: 0.2956 - val_acc: 0.9339\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 0.29054\n",
      "Epoch 463/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0352 - acc: 0.9882 - val_loss: 0.3204 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 0.29054\n",
      "Epoch 464/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0372 - acc: 0.9876 - val_loss: 0.3347 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 0.29054\n",
      "Epoch 465/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0399 - acc: 0.9875 - val_loss: 0.2972 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 0.29054\n",
      "Epoch 466/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0380 - acc: 0.9877 - val_loss: 0.3107 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 0.29054\n",
      "Epoch 467/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0391 - acc: 0.9874 - val_loss: 0.3149 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 0.29054\n",
      "Epoch 468/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0380 - acc: 0.9873 - val_loss: 0.3127 - val_acc: 0.9329\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 0.29054\n",
      "Epoch 469/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0382 - acc: 0.9872 - val_loss: 0.3288 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 0.29054\n",
      "Epoch 470/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0468 - acc: 0.9844 - val_loss: 0.3061 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 0.29054\n",
      "Epoch 471/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0396 - acc: 0.9865 - val_loss: 0.3048 - val_acc: 0.9334\n",
      "\n",
      "Epoch 00471: val_loss did not improve from 0.29054\n",
      "Epoch 472/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0414 - acc: 0.9878 - val_loss: 0.3176 - val_acc: 0.9324\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 0.29054\n",
      "Epoch 473/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0346 - acc: 0.9892 - val_loss: 0.3153 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 0.29054\n",
      "Epoch 474/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0318 - acc: 0.9894 - val_loss: 0.3188 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00474: val_loss did not improve from 0.29054\n",
      "Epoch 475/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0387 - acc: 0.9876 - val_loss: 0.3271 - val_acc: 0.9269 -\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 0.29054\n",
      "Epoch 476/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0367 - acc: 0.9881 - val_loss: 0.3213 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 0.29054\n",
      "Epoch 477/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0390 - acc: 0.9874 - val_loss: 0.3290 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 0.29054\n",
      "Epoch 478/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0412 - acc: 0.9860 - val_loss: 0.3227 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.29054\n",
      "Epoch 479/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0454 - acc: 0.9866 - val_loss: 0.3073 - val_acc: 0.9319\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.29054\n",
      "Epoch 480/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0370 - acc: 0.9873 - val_loss: 0.3123 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 0.29054\n",
      "Epoch 481/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0312 - acc: 0.9892 - val_loss: 0.3211 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 0.29054\n",
      "Epoch 482/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0374 - acc: 0.9877 - val_loss: 0.3434 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 0.29054\n",
      "Epoch 483/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0459 - acc: 0.9857 - val_loss: 0.3077 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 0.29054\n",
      "Epoch 484/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0360 - acc: 0.9882 - val_loss: 0.3141 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 0.29054\n",
      "Epoch 485/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0412 - acc: 0.9869 - val_loss: 0.3253 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00485: val_loss did not improve from 0.29054\n",
      "Epoch 486/1000\n",
      "17840/17840 [==============================] - 2s 109us/step - loss: 0.0459 - acc: 0.9859 - val_loss: 0.3273 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.29054\n",
      "Epoch 487/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0390 - acc: 0.9879 - val_loss: 0.3169 - val_acc: 0.9319\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 0.29054\n",
      "Epoch 488/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0391 - acc: 0.9872 - val_loss: 0.3095 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 0.29054\n",
      "Epoch 489/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0440 - acc: 0.9868 - val_loss: 0.3159 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 0.29054\n",
      "Epoch 490/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0392 - acc: 0.9868 - val_loss: 0.3057 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 0.29054\n",
      "Epoch 491/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0395 - acc: 0.9869 - val_loss: 0.3099 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.29054\n",
      "Epoch 492/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0397 - acc: 0.9882 - val_loss: 0.3120 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 0.29054\n",
      "Epoch 493/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0423 - acc: 0.9857 - val_loss: 0.3042 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 0.29054\n",
      "Epoch 494/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0367 - acc: 0.9879 - val_loss: 0.3151 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 0.29054\n",
      "Epoch 495/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0377 - acc: 0.9874 - val_loss: 0.3169 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 0.29054\n",
      "Epoch 496/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0422 - acc: 0.9872 - val_loss: 0.3071 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 0.29054\n",
      "Epoch 497/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0407 - acc: 0.9871 - val_loss: 0.3164 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 0.29054\n",
      "Epoch 498/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0356 - acc: 0.9881 - val_loss: 0.3355 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.29054\n",
      "Epoch 499/1000\n",
      "17840/17840 [==============================] - 2s 110us/step - loss: 0.0395 - acc: 0.9883 - val_loss: 0.3370 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.29054\n",
      "Epoch 500/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0374 - acc: 0.9879 - val_loss: 0.3484 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 0.29054\n",
      "Epoch 501/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0489 - acc: 0.9862 - val_loss: 0.3062 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 0.29054\n",
      "Epoch 502/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0443 - acc: 0.9848 - val_loss: 0.3084 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 0.29054\n",
      "Epoch 503/1000\n",
      "17840/17840 [==============================] - 2s 109us/step - loss: 0.0358 - acc: 0.9879 - val_loss: 0.3153 - val_acc: 0.9274 1s \n",
      "\n",
      "Epoch 00503: val_loss did not improve from 0.29054\n",
      "Epoch 504/1000\n",
      "17840/17840 [==============================] - 2s 109us/step - loss: 0.0346 - acc: 0.9881 - val_loss: 0.3167 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 0.29054\n",
      "Epoch 505/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0418 - acc: 0.9871 - val_loss: 0.3229 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 0.29054\n",
      "Epoch 506/1000\n",
      "17840/17840 [==============================] - 2s 99us/step - loss: 0.0442 - acc: 0.9858 - val_loss: 0.3125 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 0.29054\n",
      "Epoch 507/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0365 - acc: 0.9890 - val_loss: 0.3048 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 0.29054\n",
      "Epoch 508/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0431 - acc: 0.9855 - val_loss: 0.3083 - val_acc: 0.9324\n",
      "\n",
      "Epoch 00508: val_loss did not improve from 0.29054\n",
      "Epoch 509/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0346 - acc: 0.9885 - val_loss: 0.3207 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 0.29054\n",
      "Epoch 510/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0358 - acc: 0.9878 - val_loss: 0.3032 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 0.29054\n",
      "Epoch 511/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0350 - acc: 0.9887 - val_loss: 0.3295 - val_acc: 0.9244\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 0.29054\n",
      "Epoch 512/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0361 - acc: 0.9879 - val_loss: 0.2993 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 0.29054\n",
      "Epoch 513/1000\n",
      "17840/17840 [==============================] - 2s 99us/step - loss: 0.0412 - acc: 0.9872 - val_loss: 0.3204 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 0.29054\n",
      "Epoch 514/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0378 - acc: 0.9874 - val_loss: 0.3323 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 0.29054\n",
      "Epoch 515/1000\n",
      "17840/17840 [==============================] - 2s 110us/step - loss: 0.0404 - acc: 0.9874 - val_loss: 0.3405 - val_acc: 0.9254\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 0.29054\n",
      "Epoch 516/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0429 - acc: 0.9858 - val_loss: 0.3216 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 0.29054\n",
      "Epoch 517/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0405 - acc: 0.9871 - val_loss: 0.3181 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 0.29054\n",
      "Epoch 518/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0423 - acc: 0.9860 - val_loss: 0.3070 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 0.29054\n",
      "Epoch 519/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0337 - acc: 0.9883 - val_loss: 0.3241 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00519: val_loss did not improve from 0.29054\n",
      "Epoch 520/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0336 - acc: 0.9892 - val_loss: 0.3265 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 0.29054\n",
      "Epoch 521/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0375 - acc: 0.9877 - val_loss: 0.3162 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 0.29054\n",
      "Epoch 522/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0373 - acc: 0.9883 - val_loss: 0.3141 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00522: val_loss did not improve from 0.29054\n",
      "Epoch 523/1000\n",
      "17840/17840 [==============================] - 2s 108us/step - loss: 0.0416 - acc: 0.9863 - val_loss: 0.3136 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00523: val_loss did not improve from 0.29054\n",
      "Epoch 524/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0386 - acc: 0.9882 - val_loss: 0.3337 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 0.29054\n",
      "Epoch 525/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0382 - acc: 0.9867 - val_loss: 0.3099 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 0.29054\n",
      "Epoch 526/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0427 - acc: 0.9863 - val_loss: 0.3100 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00526: val_loss did not improve from 0.29054\n",
      "Epoch 527/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0466 - acc: 0.9854 - val_loss: 0.3063 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 0.29054\n",
      "Epoch 528/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0421 - acc: 0.9855 - val_loss: 0.3068 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00528: val_loss did not improve from 0.29054\n",
      "Epoch 529/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0360 - acc: 0.9890 - val_loss: 0.3265 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 0.29054\n",
      "Epoch 530/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0365 - acc: 0.9881 - val_loss: 0.3127 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 0.29054\n",
      "Epoch 531/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0399 - acc: 0.9867 - val_loss: 0.3180 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 0.29054\n",
      "Epoch 532/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0393 - acc: 0.9878 - val_loss: 0.3295 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 0.29054\n",
      "Epoch 533/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0396 - acc: 0.9877 - val_loss: 0.3280 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00533: val_loss did not improve from 0.29054\n",
      "Epoch 534/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0448 - acc: 0.9861 - val_loss: 0.3126 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 0.29054\n",
      "Epoch 535/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0380 - acc: 0.9877 - val_loss: 0.3198 - val_acc: 0.9249\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 0.29054\n",
      "Epoch 536/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0382 - acc: 0.9879 - val_loss: 0.3024 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 0.29054\n",
      "Epoch 537/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0389 - acc: 0.9877 - val_loss: 0.3143 - val_acc: 0.9329\n",
      "\n",
      "Epoch 00537: val_loss did not improve from 0.29054\n",
      "Epoch 538/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0424 - acc: 0.9878 - val_loss: 0.3185 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 0.29054\n",
      "Epoch 539/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0376 - acc: 0.9886 - val_loss: 0.3567 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 0.29054\n",
      "Epoch 540/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0420 - acc: 0.9867 - val_loss: 0.3065 - val_acc: 0.9319\n",
      "\n",
      "Epoch 00540: val_loss did not improve from 0.29054\n",
      "Epoch 541/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0407 - acc: 0.9869 - val_loss: 0.3124 - val_acc: 0.9329\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 0.29054\n",
      "Epoch 542/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0393 - acc: 0.9869 - val_loss: 0.3199 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 0.29054\n",
      "Epoch 543/1000\n",
      "17840/17840 [==============================] - 2s 109us/step - loss: 0.0334 - acc: 0.9901 - val_loss: 0.3250 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 0.29054\n",
      "Epoch 544/1000\n",
      "17840/17840 [==============================] - 2s 108us/step - loss: 0.0368 - acc: 0.9881 - val_loss: 0.3392 - val_acc: 0.9249\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 0.29054\n",
      "Epoch 545/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0389 - acc: 0.9873 - val_loss: 0.3325 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00545: val_loss did not improve from 0.29054\n",
      "Epoch 546/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0422 - acc: 0.9868 - val_loss: 0.3368 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 0.29054\n",
      "Epoch 547/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0410 - acc: 0.9868 - val_loss: 0.3260 - val_acc: 0.9329\n",
      "\n",
      "Epoch 00547: val_loss did not improve from 0.29054\n",
      "Epoch 548/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0386 - acc: 0.9876 - val_loss: 0.3050 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 0.29054\n",
      "Epoch 549/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0402 - acc: 0.9863 - val_loss: 0.3139 - val_acc: 0.9319\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 0.29054\n",
      "Epoch 550/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0400 - acc: 0.9876 - val_loss: 0.3147 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00550: val_loss did not improve from 0.29054\n",
      "Epoch 551/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0408 - acc: 0.9874 - val_loss: 0.3144 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 0.29054\n",
      "Epoch 552/1000\n",
      "17840/17840 [==============================] - 2s 111us/step - loss: 0.0383 - acc: 0.9884 - val_loss: 0.3111 - val_acc: 0.9324\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 0.29054\n",
      "Epoch 553/1000\n",
      "17840/17840 [==============================] - 2s 108us/step - loss: 0.0332 - acc: 0.9890 - val_loss: 0.3362 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 0.29054\n",
      "Epoch 554/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0406 - acc: 0.9868 - val_loss: 0.3400 - val_acc: 0.9249\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 0.29054\n",
      "Epoch 555/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0366 - acc: 0.9881 - val_loss: 0.3288 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 0.29054\n",
      "Epoch 556/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0340 - acc: 0.9896 - val_loss: 0.3263 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 0.29054\n",
      "Epoch 557/1000\n",
      "17840/17840 [==============================] - 2s 99us/step - loss: 0.0378 - acc: 0.9874 - val_loss: 0.3553 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 0.29054\n",
      "Epoch 558/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0366 - acc: 0.9881 - val_loss: 0.3398 - val_acc: 0.9239\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 0.29054\n",
      "Epoch 559/1000\n",
      "17840/17840 [==============================] - 2s 123us/step - loss: 0.0412 - acc: 0.9868 - val_loss: 0.3373 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 0.29054\n",
      "Epoch 560/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0357 - acc: 0.9883 - val_loss: 0.3345 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 0.29054\n",
      "Epoch 561/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0389 - acc: 0.9872 - val_loss: 0.3383 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 0.29054\n",
      "Epoch 562/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0378 - acc: 0.9881 - val_loss: 0.3334 - val_acc: 0.9319\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 0.29054\n",
      "Epoch 563/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0441 - acc: 0.9849 - val_loss: 0.3089 - val_acc: 0.9249\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 0.29054\n",
      "Epoch 564/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0385 - acc: 0.9881 - val_loss: 0.3095 - val_acc: 0.9349\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 0.29054\n",
      "Epoch 565/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0441 - acc: 0.9865 - val_loss: 0.3093 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 0.29054\n",
      "Epoch 566/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0382 - acc: 0.9876 - val_loss: 0.3236 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 0.29054\n",
      "Epoch 567/1000\n",
      "17840/17840 [==============================] - 2s 110us/step - loss: 0.0369 - acc: 0.9878 - val_loss: 0.3214 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 0.29054\n",
      "Epoch 568/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0388 - acc: 0.9879 - val_loss: 0.3100 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 0.29054\n",
      "Epoch 569/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0356 - acc: 0.9887 - val_loss: 0.3338 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 0.29054\n",
      "Epoch 570/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0379 - acc: 0.9877 - val_loss: 0.3315 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 0.29054\n",
      "Epoch 571/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0370 - acc: 0.9876 - val_loss: 0.3347 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 0.29054\n",
      "Epoch 572/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0395 - acc: 0.9872 - val_loss: 0.3357 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 0.29054\n",
      "Epoch 573/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0454 - acc: 0.9846 - val_loss: 0.3198 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 0.29054\n",
      "Epoch 574/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0402 - acc: 0.9870 - val_loss: 0.3203 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00574: val_loss did not improve from 0.29054\n",
      "Epoch 575/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0368 - acc: 0.9886 - val_loss: 0.3263 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 0.29054\n",
      "Epoch 576/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0409 - acc: 0.9867 - val_loss: 0.3034 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 0.29054\n",
      "Epoch 577/1000\n",
      "17840/17840 [==============================] - 2s 111us/step - loss: 0.0347 - acc: 0.9894 - val_loss: 0.3143 - val_acc: 0.9319\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 0.29054\n",
      "Epoch 578/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0376 - acc: 0.9881 - val_loss: 0.3205 - val_acc: 0.9319\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 0.29054\n",
      "Epoch 579/1000\n",
      "17840/17840 [==============================] - 2s 114us/step - loss: 0.0370 - acc: 0.9872 - val_loss: 0.3142 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 0.29054\n",
      "Epoch 580/1000\n",
      "17840/17840 [==============================] - 2s 120us/step - loss: 0.0393 - acc: 0.9874 - val_loss: 0.3240 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 0.29054\n",
      "Epoch 581/1000\n",
      "17840/17840 [==============================] - 2s 109us/step - loss: 0.0497 - acc: 0.9845 - val_loss: 0.3128 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 0.29054\n",
      "Epoch 582/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0373 - acc: 0.9878 - val_loss: 0.3071 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 0.29054\n",
      "Epoch 583/1000\n",
      "17840/17840 [==============================] - 2s 108us/step - loss: 0.0354 - acc: 0.9881 - val_loss: 0.3144 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 0.29054\n",
      "Epoch 584/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0382 - acc: 0.9876 - val_loss: 0.3237 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 0.29054\n",
      "Epoch 585/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0369 - acc: 0.9876 - val_loss: 0.3369 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 0.29054\n",
      "Epoch 586/1000\n",
      "17840/17840 [==============================] - 2s 113us/step - loss: 0.0436 - acc: 0.9858 - val_loss: 0.3307 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 0.29054\n",
      "Epoch 587/1000\n",
      "17840/17840 [==============================] - 2s 111us/step - loss: 0.0415 - acc: 0.9864 - val_loss: 0.3133 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 0.29054\n",
      "Epoch 588/1000\n",
      "17840/17840 [==============================] - 2s 110us/step - loss: 0.0380 - acc: 0.9880 - val_loss: 0.3191 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 0.29054\n",
      "Epoch 589/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0351 - acc: 0.9895 - val_loss: 0.3256 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00589: val_loss did not improve from 0.29054\n",
      "Epoch 590/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0436 - acc: 0.9861 - val_loss: 0.3241 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 0.29054\n",
      "Epoch 591/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0416 - acc: 0.9861 - val_loss: 0.3182 - val_acc: 0.9329\n",
      "\n",
      "Epoch 00591: val_loss did not improve from 0.29054\n",
      "Epoch 592/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0374 - acc: 0.9877 - val_loss: 0.3168 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 0.29054\n",
      "Epoch 593/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0359 - acc: 0.9883 - val_loss: 0.3173 - val_acc: 0.9319\n",
      "\n",
      "Epoch 00593: val_loss did not improve from 0.29054\n",
      "Epoch 594/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0401 - acc: 0.9874 - val_loss: 0.3122 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 0.29054\n",
      "Epoch 595/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0376 - acc: 0.9876 - val_loss: 0.3274 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 0.29054\n",
      "Epoch 596/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0370 - acc: 0.9884 - val_loss: 0.3169 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 0.29054\n",
      "Epoch 597/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0386 - acc: 0.9878 - val_loss: 0.3061 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 0.29054\n",
      "Epoch 598/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0399 - acc: 0.9876 - val_loss: 0.3174 - val_acc: 0.9233\n",
      "\n",
      "Epoch 00598: val_loss did not improve from 0.29054\n",
      "Epoch 599/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0328 - acc: 0.9892 - val_loss: 0.3137 - val_acc: 0.9339\n",
      "\n",
      "Epoch 00599: val_loss did not improve from 0.29054\n",
      "Epoch 600/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0353 - acc: 0.9891 - val_loss: 0.2999 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 0.29054\n",
      "Epoch 601/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0396 - acc: 0.9872 - val_loss: 0.3134 - val_acc: 0.9329\n",
      "\n",
      "Epoch 00601: val_loss did not improve from 0.29054\n",
      "Epoch 602/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0369 - acc: 0.9886 - val_loss: 0.3243 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 0.29054\n",
      "Epoch 603/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0386 - acc: 0.9873 - val_loss: 0.3230 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 0.29054\n",
      "Epoch 604/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0376 - acc: 0.9873 - val_loss: 0.3228 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00604: val_loss did not improve from 0.29054\n",
      "Epoch 605/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0360 - acc: 0.9887 - val_loss: 0.3200 - val_acc: 0.9249\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 0.29054\n",
      "Epoch 606/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0407 - acc: 0.9864 - val_loss: 0.3107 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 0.29054\n",
      "Epoch 607/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0378 - acc: 0.9873 - val_loss: 0.3353 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00607: val_loss did not improve from 0.29054\n",
      "Epoch 608/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0358 - acc: 0.9886 - val_loss: 0.3206 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00608: val_loss did not improve from 0.29054\n",
      "Epoch 609/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0416 - acc: 0.9865 - val_loss: 0.3197 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00609: val_loss did not improve from 0.29054\n",
      "Epoch 610/1000\n",
      "17840/17840 [==============================] - 2s 110us/step - loss: 0.0397 - acc: 0.9867 - val_loss: 0.3165 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00610: val_loss did not improve from 0.29054\n",
      "Epoch 611/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0383 - acc: 0.9867 - val_loss: 0.3094 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00611: val_loss did not improve from 0.29054\n",
      "Epoch 612/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0446 - acc: 0.9863 - val_loss: 0.3053 - val_acc: 0.9319\n",
      "\n",
      "Epoch 00612: val_loss did not improve from 0.29054\n",
      "Epoch 613/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0413 - acc: 0.9874 - val_loss: 0.2965 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 0.29054\n",
      "Epoch 614/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0372 - acc: 0.9879 - val_loss: 0.3122 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00614: val_loss did not improve from 0.29054\n",
      "Epoch 615/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0354 - acc: 0.9885 - val_loss: 0.3231 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 0.29054\n",
      "Epoch 616/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0387 - acc: 0.9870 - val_loss: 0.3231 - val_acc: 0.9344\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 0.29054\n",
      "Epoch 617/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0400 - acc: 0.9877 - val_loss: 0.3021 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 0.29054\n",
      "Epoch 618/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0355 - acc: 0.9880 - val_loss: 0.3344 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00618: val_loss did not improve from 0.29054\n",
      "Epoch 619/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0378 - acc: 0.9885 - val_loss: 0.3284 - val_acc: 0.9319\n",
      "\n",
      "Epoch 00619: val_loss did not improve from 0.29054\n",
      "Epoch 620/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0451 - acc: 0.9852 - val_loss: 0.3069 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00620: val_loss did not improve from 0.29054\n",
      "Epoch 621/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0367 - acc: 0.9877 - val_loss: 0.3199 - val_acc: 0.9233\n",
      "\n",
      "Epoch 00621: val_loss did not improve from 0.29054\n",
      "Epoch 622/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0408 - acc: 0.9874 - val_loss: 0.3039 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00622: val_loss did not improve from 0.29054\n",
      "Epoch 623/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0330 - acc: 0.9889 - val_loss: 0.3076 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00623: val_loss did not improve from 0.29054\n",
      "Epoch 624/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0405 - acc: 0.9871 - val_loss: 0.3159 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00624: val_loss did not improve from 0.29054\n",
      "Epoch 625/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0387 - acc: 0.9871 - val_loss: 0.3275 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00625: val_loss did not improve from 0.29054\n",
      "Epoch 626/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0348 - acc: 0.9878 - val_loss: 0.3313 - val_acc: 0.9233\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 0.29054\n",
      "Epoch 627/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0437 - acc: 0.9871 - val_loss: 0.3123 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 0.29054\n",
      "Epoch 628/1000\n",
      "17840/17840 [==============================] - 2s 110us/step - loss: 0.0372 - acc: 0.9873 - val_loss: 0.3353 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 0.29054\n",
      "Epoch 629/1000\n",
      "17840/17840 [==============================] - 2s 108us/step - loss: 0.0425 - acc: 0.9862 - val_loss: 0.3133 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00629: val_loss did not improve from 0.29054\n",
      "Epoch 630/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0422 - acc: 0.9865 - val_loss: 0.3232 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 0.29054\n",
      "Epoch 631/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0367 - acc: 0.9884 - val_loss: 0.3128 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 0.29054\n",
      "Epoch 632/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0410 - acc: 0.9881 - val_loss: 0.3237 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00632: val_loss did not improve from 0.29054\n",
      "Epoch 633/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0394 - acc: 0.9871 - val_loss: 0.3134 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00633: val_loss did not improve from 0.29054\n",
      "Epoch 634/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0376 - acc: 0.9877 - val_loss: 0.3413 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00634: val_loss did not improve from 0.29054\n",
      "Epoch 635/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0447 - acc: 0.9853 - val_loss: 0.3066 - val_acc: 0.9319\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 0.29054\n",
      "Epoch 636/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0385 - acc: 0.9881 - val_loss: 0.3148 - val_acc: 0.9334\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 0.29054\n",
      "Epoch 637/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0395 - acc: 0.9877 - val_loss: 0.3191 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 0.29054\n",
      "Epoch 638/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0332 - acc: 0.9899 - val_loss: 0.3112 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 0.29054\n",
      "Epoch 639/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0393 - acc: 0.9876 - val_loss: 0.3166 - val_acc: 0.9334\n",
      "\n",
      "Epoch 00639: val_loss did not improve from 0.29054\n",
      "Epoch 640/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0365 - acc: 0.9881 - val_loss: 0.3201 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00640: val_loss did not improve from 0.29054\n",
      "Epoch 641/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0350 - acc: 0.9880 - val_loss: 0.3363 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00641: val_loss did not improve from 0.29054\n",
      "Epoch 642/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0428 - acc: 0.9868 - val_loss: 0.3194 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00642: val_loss did not improve from 0.29054\n",
      "Epoch 643/1000\n",
      "17840/17840 [==============================] - 2s 111us/step - loss: 0.0368 - acc: 0.9884 - val_loss: 0.3175 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00643: val_loss did not improve from 0.29054\n",
      "Epoch 644/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0323 - acc: 0.9886 - val_loss: 0.3296 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00644: val_loss did not improve from 0.29054\n",
      "Epoch 645/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0408 - acc: 0.9861 - val_loss: 0.3221 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00645: val_loss did not improve from 0.29054\n",
      "Epoch 646/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0420 - acc: 0.9867 - val_loss: 0.3262 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00646: val_loss did not improve from 0.29054\n",
      "Epoch 647/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0363 - acc: 0.9885 - val_loss: 0.3410 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00647: val_loss did not improve from 0.29054\n",
      "Epoch 648/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0447 - acc: 0.9867 - val_loss: 0.3166 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00648: val_loss did not improve from 0.29054\n",
      "Epoch 649/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0409 - acc: 0.9868 - val_loss: 0.3245 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00649: val_loss did not improve from 0.29054\n",
      "Epoch 650/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0411 - acc: 0.9876 - val_loss: 0.3039 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 0.29054\n",
      "Epoch 651/1000\n",
      "17840/17840 [==============================] - 2s 110us/step - loss: 0.0328 - acc: 0.9890 - val_loss: 0.3050 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00651: val_loss did not improve from 0.29054\n",
      "Epoch 652/1000\n",
      "17840/17840 [==============================] - 2s 123us/step - loss: 0.0359 - acc: 0.9881 - val_loss: 0.3148 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00652: val_loss did not improve from 0.29054\n",
      "Epoch 653/1000\n",
      "17840/17840 [==============================] - 2s 130us/step - loss: 0.0346 - acc: 0.9890 - val_loss: 0.3172 - val_acc: 0.9319\n",
      "\n",
      "Epoch 00653: val_loss did not improve from 0.29054\n",
      "Epoch 654/1000\n",
      "17840/17840 [==============================] - 2s 116us/step - loss: 0.0338 - acc: 0.9885 - val_loss: 0.3246 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00654: val_loss did not improve from 0.29054\n",
      "Epoch 655/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0374 - acc: 0.9885 - val_loss: 0.3413 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00655: val_loss did not improve from 0.29054\n",
      "Epoch 656/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0377 - acc: 0.9878 - val_loss: 0.3044 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00656: val_loss did not improve from 0.29054\n",
      "Epoch 657/1000\n",
      "17840/17840 [==============================] - 2s 108us/step - loss: 0.0412 - acc: 0.9863 - val_loss: 0.3161 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00657: val_loss did not improve from 0.29054\n",
      "Epoch 658/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0308 - acc: 0.9896 - val_loss: 0.3250 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00658: val_loss did not improve from 0.29054\n",
      "Epoch 659/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0405 - acc: 0.9874 - val_loss: 0.3055 - val_acc: 0.9249\n",
      "\n",
      "Epoch 00659: val_loss did not improve from 0.29054\n",
      "Epoch 660/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0490 - acc: 0.9855 - val_loss: 0.3090 - val_acc: 0.9329\n",
      "\n",
      "Epoch 00660: val_loss did not improve from 0.29054\n",
      "Epoch 661/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0354 - acc: 0.9885 - val_loss: 0.3162 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00661: val_loss did not improve from 0.29054\n",
      "Epoch 662/1000\n",
      "17840/17840 [==============================] - 2s 111us/step - loss: 0.0338 - acc: 0.9884 - val_loss: 0.3274 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00662: val_loss did not improve from 0.29054\n",
      "Epoch 663/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0394 - acc: 0.9879 - val_loss: 0.3263 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00663: val_loss did not improve from 0.29054\n",
      "Epoch 664/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0399 - acc: 0.9869 - val_loss: 0.3092 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00664: val_loss did not improve from 0.29054\n",
      "Epoch 665/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0385 - acc: 0.9875 - val_loss: 0.3166 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00665: val_loss did not improve from 0.29054\n",
      "Epoch 666/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0368 - acc: 0.9876 - val_loss: 0.3019 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 0.29054\n",
      "Epoch 667/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0375 - acc: 0.9893 - val_loss: 0.3202 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00667: val_loss did not improve from 0.29054\n",
      "Epoch 668/1000\n",
      "17840/17840 [==============================] - 2s 108us/step - loss: 0.0419 - acc: 0.9859 - val_loss: 0.3238 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00668: val_loss did not improve from 0.29054\n",
      "Epoch 669/1000\n",
      "17840/17840 [==============================] - 2s 108us/step - loss: 0.0385 - acc: 0.9885 - val_loss: 0.3167 - val_acc: 0.9319\n",
      "\n",
      "Epoch 00669: val_loss did not improve from 0.29054\n",
      "Epoch 670/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0367 - acc: 0.9885 - val_loss: 0.3156 - val_acc: 0.9228\n",
      "\n",
      "Epoch 00670: val_loss did not improve from 0.29054\n",
      "Epoch 671/1000\n",
      "17840/17840 [==============================] - 2s 109us/step - loss: 0.0382 - acc: 0.9884 - val_loss: 0.3095 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00671: val_loss did not improve from 0.29054\n",
      "Epoch 672/1000\n",
      "17840/17840 [==============================] - 2s 110us/step - loss: 0.0391 - acc: 0.9868 - val_loss: 0.3086 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00672: val_loss did not improve from 0.29054\n",
      "Epoch 673/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0357 - acc: 0.9885 - val_loss: 0.3278 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00673: val_loss did not improve from 0.29054\n",
      "Epoch 674/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0379 - acc: 0.9884 - val_loss: 0.3228 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00674: val_loss did not improve from 0.29054\n",
      "Epoch 675/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0382 - acc: 0.9878 - val_loss: 0.3243 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00675: val_loss did not improve from 0.29054\n",
      "Epoch 676/1000\n",
      "17840/17840 [==============================] - 2s 109us/step - loss: 0.0340 - acc: 0.9901 - val_loss: 0.3271 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00676: val_loss did not improve from 0.29054\n",
      "Epoch 677/1000\n",
      "17840/17840 [==============================] - 2s 109us/step - loss: 0.0339 - acc: 0.9895 - val_loss: 0.3332 - val_acc: 0.9319\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 0.29054\n",
      "Epoch 678/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0365 - acc: 0.9877 - val_loss: 0.3294 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00678: val_loss did not improve from 0.29054\n",
      "Epoch 679/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0401 - acc: 0.9873 - val_loss: 0.3221 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00679: val_loss did not improve from 0.29054\n",
      "Epoch 680/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0396 - acc: 0.9869 - val_loss: 0.3383 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00680: val_loss did not improve from 0.29054\n",
      "Epoch 681/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0333 - acc: 0.9887 - val_loss: 0.3264 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00681: val_loss did not improve from 0.29054\n",
      "Epoch 682/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0383 - acc: 0.9869 - val_loss: 0.3432 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00682: val_loss did not improve from 0.29054\n",
      "Epoch 683/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0394 - acc: 0.9878 - val_loss: 0.3165 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00683: val_loss did not improve from 0.29054\n",
      "Epoch 684/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0403 - acc: 0.9883 - val_loss: 0.3224 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00684: val_loss did not improve from 0.29054\n",
      "Epoch 685/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0396 - acc: 0.9868 - val_loss: 0.3038 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00685: val_loss did not improve from 0.29054\n",
      "Epoch 686/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0356 - acc: 0.9886 - val_loss: 0.3358 - val_acc: 0.9249\n",
      "\n",
      "Epoch 00686: val_loss did not improve from 0.29054\n",
      "Epoch 687/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0347 - acc: 0.9886 - val_loss: 0.3355 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00687: val_loss did not improve from 0.29054\n",
      "Epoch 688/1000\n",
      "17840/17840 [==============================] - 2s 123us/step - loss: 0.0376 - acc: 0.9878 - val_loss: 0.3255 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00688: val_loss did not improve from 0.29054\n",
      "Epoch 689/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0391 - acc: 0.9875 - val_loss: 0.3241 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00689: val_loss did not improve from 0.29054\n",
      "Epoch 690/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0374 - acc: 0.9892 - val_loss: 0.3302 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00690: val_loss did not improve from 0.29054\n",
      "Epoch 691/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0323 - acc: 0.9886 - val_loss: 0.3268 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00691: val_loss did not improve from 0.29054\n",
      "Epoch 692/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0370 - acc: 0.9887 - val_loss: 0.3229 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00692: val_loss did not improve from 0.29054\n",
      "Epoch 693/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0372 - acc: 0.9887 - val_loss: 0.3153 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00693: val_loss did not improve from 0.29054\n",
      "Epoch 694/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0369 - acc: 0.9877 - val_loss: 0.3282 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00694: val_loss did not improve from 0.29054\n",
      "Epoch 695/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0402 - acc: 0.9879 - val_loss: 0.3154 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00695: val_loss did not improve from 0.29054\n",
      "Epoch 696/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0360 - acc: 0.9885 - val_loss: 0.3264 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00696: val_loss did not improve from 0.29054\n",
      "Epoch 697/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0392 - acc: 0.9886 - val_loss: 0.3270 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00697: val_loss did not improve from 0.29054\n",
      "Epoch 698/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0419 - acc: 0.9869 - val_loss: 0.3336 - val_acc: 0.9244\n",
      "\n",
      "Epoch 00698: val_loss did not improve from 0.29054\n",
      "Epoch 699/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0399 - acc: 0.9879 - val_loss: 0.3153 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00699: val_loss did not improve from 0.29054\n",
      "Epoch 700/1000\n",
      "17840/17840 [==============================] - 2s 108us/step - loss: 0.0311 - acc: 0.9895 - val_loss: 0.3324 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00700: val_loss did not improve from 0.29054\n",
      "Epoch 701/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0333 - acc: 0.9887 - val_loss: 0.3395 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00701: val_loss did not improve from 0.29054\n",
      "Epoch 702/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0364 - acc: 0.9883 - val_loss: 0.3354 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00702: val_loss did not improve from 0.29054\n",
      "Epoch 703/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0365 - acc: 0.9883 - val_loss: 0.3293 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00703: val_loss did not improve from 0.29054\n",
      "Epoch 704/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0367 - acc: 0.9878 - val_loss: 0.3195 - val_acc: 0.9249\n",
      "\n",
      "Epoch 00704: val_loss did not improve from 0.29054\n",
      "Epoch 705/1000\n",
      "17840/17840 [==============================] - 2s 109us/step - loss: 0.0361 - acc: 0.9883 - val_loss: 0.3224 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00705: val_loss did not improve from 0.29054\n",
      "Epoch 706/1000\n",
      "17840/17840 [==============================] - 2s 113us/step - loss: 0.0416 - acc: 0.9864 - val_loss: 0.3094 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00706: val_loss did not improve from 0.29054\n",
      "Epoch 707/1000\n",
      "17840/17840 [==============================] - 2s 112us/step - loss: 0.0369 - acc: 0.9880 - val_loss: 0.3279 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00707: val_loss did not improve from 0.29054\n",
      "Epoch 708/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0362 - acc: 0.9888 - val_loss: 0.3377 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00708: val_loss did not improve from 0.29054\n",
      "Epoch 709/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0312 - acc: 0.9896 - val_loss: 0.3418 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00709: val_loss did not improve from 0.29054\n",
      "Epoch 710/1000\n",
      "17840/17840 [==============================] - 2s 119us/step - loss: 0.0385 - acc: 0.9868 - val_loss: 0.3396 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00710: val_loss did not improve from 0.29054\n",
      "Epoch 711/1000\n",
      "17840/17840 [==============================] - 2s 109us/step - loss: 0.0352 - acc: 0.9879 - val_loss: 0.3427 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00711: val_loss did not improve from 0.29054\n",
      "Epoch 712/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0366 - acc: 0.9885 - val_loss: 0.3317 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00712: val_loss did not improve from 0.29054\n",
      "Epoch 713/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0404 - acc: 0.9869 - val_loss: 0.3196 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00713: val_loss did not improve from 0.29054\n",
      "Epoch 714/1000\n",
      "17840/17840 [==============================] - 2s 124us/step - loss: 0.0390 - acc: 0.9869 - val_loss: 0.3152 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00714: val_loss did not improve from 0.29054\n",
      "Epoch 715/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0319 - acc: 0.9900 - val_loss: 0.3248 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00715: val_loss did not improve from 0.29054\n",
      "Epoch 716/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0371 - acc: 0.9881 - val_loss: 0.3196 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00716: val_loss did not improve from 0.29054\n",
      "Epoch 717/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0388 - acc: 0.9882 - val_loss: 0.3249 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00717: val_loss did not improve from 0.29054\n",
      "Epoch 718/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0386 - acc: 0.9873 - val_loss: 0.3310 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00718: val_loss did not improve from 0.29054\n",
      "Epoch 719/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0335 - acc: 0.9896 - val_loss: 0.3247 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00719: val_loss did not improve from 0.29054\n",
      "Epoch 720/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0347 - acc: 0.9880 - val_loss: 0.3355 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00720: val_loss did not improve from 0.29054\n",
      "Epoch 721/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0400 - acc: 0.9873 - val_loss: 0.3209 - val_acc: 0.9244\n",
      "\n",
      "Epoch 00721: val_loss did not improve from 0.29054\n",
      "Epoch 722/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0366 - acc: 0.9878 - val_loss: 0.3196 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00722: val_loss did not improve from 0.29054\n",
      "Epoch 723/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0383 - acc: 0.9880 - val_loss: 0.3177 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00723: val_loss did not improve from 0.29054\n",
      "Epoch 724/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0415 - acc: 0.9874 - val_loss: 0.3345 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00724: val_loss did not improve from 0.29054\n",
      "Epoch 725/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0371 - acc: 0.9881 - val_loss: 0.3211 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00725: val_loss did not improve from 0.29054\n",
      "Epoch 726/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0330 - acc: 0.9878 - val_loss: 0.3372 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00726: val_loss did not improve from 0.29054\n",
      "Epoch 727/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0365 - acc: 0.9877 - val_loss: 0.3129 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00727: val_loss did not improve from 0.29054\n",
      "Epoch 728/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0338 - acc: 0.9892 - val_loss: 0.3201 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00728: val_loss did not improve from 0.29054\n",
      "Epoch 729/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0356 - acc: 0.9884 - val_loss: 0.3151 - val_acc: 0.9249\n",
      "\n",
      "Epoch 00729: val_loss did not improve from 0.29054\n",
      "Epoch 730/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0378 - acc: 0.9888 - val_loss: 0.3196 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00730: val_loss did not improve from 0.29054\n",
      "Epoch 731/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0375 - acc: 0.9869 - val_loss: 0.3232 - val_acc: 0.9228\n",
      "\n",
      "Epoch 00731: val_loss did not improve from 0.29054\n",
      "Epoch 732/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0381 - acc: 0.9886 - val_loss: 0.3178 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00732: val_loss did not improve from 0.29054\n",
      "Epoch 733/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0423 - acc: 0.9871 - val_loss: 0.3155 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00733: val_loss did not improve from 0.29054\n",
      "Epoch 734/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0355 - acc: 0.9879 - val_loss: 0.3180 - val_acc: 0.9254\n",
      "\n",
      "Epoch 00734: val_loss did not improve from 0.29054\n",
      "Epoch 735/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0391 - acc: 0.9872 - val_loss: 0.3120 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00735: val_loss did not improve from 0.29054\n",
      "Epoch 736/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0404 - acc: 0.9869 - val_loss: 0.3139 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00736: val_loss did not improve from 0.29054\n",
      "Epoch 737/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0308 - acc: 0.9906 - val_loss: 0.3139 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00737: val_loss did not improve from 0.29054\n",
      "Epoch 738/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0372 - acc: 0.9874 - val_loss: 0.3137 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00738: val_loss did not improve from 0.29054\n",
      "Epoch 739/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0323 - acc: 0.9895 - val_loss: 0.3176 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00739: val_loss did not improve from 0.29054\n",
      "Epoch 740/1000\n",
      "17840/17840 [==============================] - 2s 109us/step - loss: 0.0349 - acc: 0.9892 - val_loss: 0.3288 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00740: val_loss did not improve from 0.29054\n",
      "Epoch 741/1000\n",
      "17840/17840 [==============================] - 2s 111us/step - loss: 0.0366 - acc: 0.9882 - val_loss: 0.3337 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00741: val_loss did not improve from 0.29054\n",
      "Epoch 742/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0393 - acc: 0.9881 - val_loss: 0.3141 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00742: val_loss did not improve from 0.29054\n",
      "Epoch 743/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0352 - acc: 0.9890 - val_loss: 0.3085 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00743: val_loss did not improve from 0.29054\n",
      "Epoch 744/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0354 - acc: 0.9887 - val_loss: 0.3247 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00744: val_loss did not improve from 0.29054\n",
      "Epoch 745/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0318 - acc: 0.9895 - val_loss: 0.3395 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00745: val_loss did not improve from 0.29054\n",
      "Epoch 746/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0405 - acc: 0.9868 - val_loss: 0.3333 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00746: val_loss did not improve from 0.29054\n",
      "Epoch 747/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0422 - acc: 0.9868 - val_loss: 0.3090 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00747: val_loss did not improve from 0.29054\n",
      "Epoch 748/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0365 - acc: 0.9885 - val_loss: 0.3200 - val_acc: 0.9319\n",
      "\n",
      "Epoch 00748: val_loss did not improve from 0.29054\n",
      "Epoch 749/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0360 - acc: 0.9892 - val_loss: 0.3327 - val_acc: 0.9254\n",
      "\n",
      "Epoch 00749: val_loss did not improve from 0.29054\n",
      "Epoch 750/1000\n",
      "17840/17840 [==============================] - 2s 109us/step - loss: 0.0403 - acc: 0.9877 - val_loss: 0.3385 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00750: val_loss did not improve from 0.29054\n",
      "Epoch 751/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0438 - acc: 0.9871 - val_loss: 0.3101 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00751: val_loss did not improve from 0.29054\n",
      "Epoch 752/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0404 - acc: 0.9886 - val_loss: 0.3104 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00752: val_loss did not improve from 0.29054\n",
      "Epoch 753/1000\n",
      "17840/17840 [==============================] - 2s 109us/step - loss: 0.0359 - acc: 0.9883 - val_loss: 0.3159 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00753: val_loss did not improve from 0.29054\n",
      "Epoch 754/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0355 - acc: 0.9889 - val_loss: 0.3186 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00754: val_loss did not improve from 0.29054\n",
      "Epoch 755/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0333 - acc: 0.9886 - val_loss: 0.3298 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00755: val_loss did not improve from 0.29054\n",
      "Epoch 756/1000\n",
      "17840/17840 [==============================] - 2s 108us/step - loss: 0.0394 - acc: 0.9876 - val_loss: 0.3350 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00756: val_loss did not improve from 0.29054\n",
      "Epoch 757/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0402 - acc: 0.9877 - val_loss: 0.3186 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00757: val_loss did not improve from 0.29054\n",
      "Epoch 758/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0341 - acc: 0.9892 - val_loss: 0.3360 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00758: val_loss did not improve from 0.29054\n",
      "Epoch 759/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0336 - acc: 0.9891 - val_loss: 0.3132 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00759: val_loss did not improve from 0.29054\n",
      "Epoch 760/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0418 - acc: 0.9871 - val_loss: 0.2986 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00760: val_loss did not improve from 0.29054\n",
      "Epoch 761/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0374 - acc: 0.9878 - val_loss: 0.3233 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00761: val_loss did not improve from 0.29054\n",
      "Epoch 762/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0397 - acc: 0.9874 - val_loss: 0.3191 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00762: val_loss did not improve from 0.29054\n",
      "Epoch 763/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0399 - acc: 0.9864 - val_loss: 0.3212 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00763: val_loss did not improve from 0.29054\n",
      "Epoch 764/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0373 - acc: 0.9873 - val_loss: 0.3216 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00764: val_loss did not improve from 0.29054\n",
      "Epoch 765/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0381 - acc: 0.9878 - val_loss: 0.3134 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00765: val_loss did not improve from 0.29054\n",
      "Epoch 766/1000\n",
      "17840/17840 [==============================] - 2s 109us/step - loss: 0.0326 - acc: 0.9897 - val_loss: 0.3332 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00766: val_loss did not improve from 0.29054\n",
      "Epoch 767/1000\n",
      "17840/17840 [==============================] - 2s 113us/step - loss: 0.0372 - acc: 0.9892 - val_loss: 0.3135 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00767: val_loss did not improve from 0.29054\n",
      "Epoch 768/1000\n",
      "17840/17840 [==============================] - 2s 124us/step - loss: 0.0414 - acc: 0.9872 - val_loss: 0.3241 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00768: val_loss did not improve from 0.29054\n",
      "Epoch 769/1000\n",
      "17840/17840 [==============================] - 2s 124us/step - loss: 0.0385 - acc: 0.9885 - val_loss: 0.3253 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00769: val_loss did not improve from 0.29054\n",
      "Epoch 770/1000\n",
      "17840/17840 [==============================] - 2s 131us/step - loss: 0.0384 - acc: 0.9879 - val_loss: 0.3234 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00770: val_loss did not improve from 0.29054\n",
      "Epoch 771/1000\n",
      "17840/17840 [==============================] - 2s 126us/step - loss: 0.0344 - acc: 0.9891 - val_loss: 0.3086 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00771: val_loss did not improve from 0.29054\n",
      "Epoch 772/1000\n",
      "17840/17840 [==============================] - 3s 141us/step - loss: 0.0407 - acc: 0.9878 - val_loss: 0.3066 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00772: val_loss did not improve from 0.29054\n",
      "Epoch 773/1000\n",
      "17840/17840 [==============================] - 2s 112us/step - loss: 0.0354 - acc: 0.9888 - val_loss: 0.3183 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00773: val_loss did not improve from 0.29054\n",
      "Epoch 774/1000\n",
      "17840/17840 [==============================] - 2s 112us/step - loss: 0.0361 - acc: 0.9886 - val_loss: 0.3262 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00774: val_loss did not improve from 0.29054\n",
      "Epoch 775/1000\n",
      "17840/17840 [==============================] - 2s 123us/step - loss: 0.0367 - acc: 0.9874 - val_loss: 0.3119 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00775: val_loss did not improve from 0.29054\n",
      "Epoch 776/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0388 - acc: 0.9868 - val_loss: 0.2939 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00776: val_loss did not improve from 0.29054\n",
      "Epoch 777/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0354 - acc: 0.9884 - val_loss: 0.3238 - val_acc: 0.9254\n",
      "\n",
      "Epoch 00777: val_loss did not improve from 0.29054\n",
      "Epoch 778/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0375 - acc: 0.9870 - val_loss: 0.3081 - val_acc: 0.9334\n",
      "\n",
      "Epoch 00778: val_loss did not improve from 0.29054\n",
      "Epoch 779/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0347 - acc: 0.9897 - val_loss: 0.3316 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00779: val_loss did not improve from 0.29054\n",
      "Epoch 780/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0354 - acc: 0.9889 - val_loss: 0.3261 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00780: val_loss did not improve from 0.29054\n",
      "Epoch 781/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0352 - acc: 0.9879 - val_loss: 0.3169 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00781: val_loss did not improve from 0.29054\n",
      "Epoch 782/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0355 - acc: 0.9891 - val_loss: 0.3351 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00782: val_loss did not improve from 0.29054\n",
      "Epoch 783/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0367 - acc: 0.9881 - val_loss: 0.3157 - val_acc: 0.9228\n",
      "\n",
      "Epoch 00783: val_loss did not improve from 0.29054\n",
      "Epoch 784/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0373 - acc: 0.9880 - val_loss: 0.3261 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00784: val_loss did not improve from 0.29054\n",
      "Epoch 785/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0419 - acc: 0.9877 - val_loss: 0.3312 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00785: val_loss did not improve from 0.29054\n",
      "Epoch 786/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0392 - acc: 0.9885 - val_loss: 0.3272 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00786: val_loss did not improve from 0.29054\n",
      "Epoch 787/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0360 - acc: 0.9877 - val_loss: 0.3419 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00787: val_loss did not improve from 0.29054\n",
      "Epoch 788/1000\n",
      "17840/17840 [==============================] - 2s 108us/step - loss: 0.0348 - acc: 0.9896 - val_loss: 0.3315 - val_acc: 0.9239\n",
      "\n",
      "Epoch 00788: val_loss did not improve from 0.29054\n",
      "Epoch 789/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0395 - acc: 0.9871 - val_loss: 0.3357 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00789: val_loss did not improve from 0.29054\n",
      "Epoch 790/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0399 - acc: 0.9873 - val_loss: 0.3262 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00790: val_loss did not improve from 0.29054\n",
      "Epoch 791/1000\n",
      "17840/17840 [==============================] - 2s 109us/step - loss: 0.0372 - acc: 0.9887 - val_loss: 0.3206 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00791: val_loss did not improve from 0.29054\n",
      "Epoch 792/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0333 - acc: 0.9896 - val_loss: 0.3137 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00792: val_loss did not improve from 0.29054\n",
      "Epoch 793/1000\n",
      "17840/17840 [==============================] - 2s 109us/step - loss: 0.0447 - acc: 0.9862 - val_loss: 0.3196 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00793: val_loss did not improve from 0.29054\n",
      "Epoch 794/1000\n",
      "17840/17840 [==============================] - 2s 109us/step - loss: 0.0369 - acc: 0.9871 - val_loss: 0.3235 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00794: val_loss did not improve from 0.29054\n",
      "Epoch 795/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0346 - acc: 0.9888 - val_loss: 0.3187 - val_acc: 0.9324\n",
      "\n",
      "Epoch 00795: val_loss did not improve from 0.29054\n",
      "Epoch 796/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0327 - acc: 0.9900 - val_loss: 0.3219 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00796: val_loss did not improve from 0.29054\n",
      "Epoch 797/1000\n",
      "17840/17840 [==============================] - 2s 108us/step - loss: 0.0407 - acc: 0.9856 - val_loss: 0.3260 - val_acc: 0.9254\n",
      "\n",
      "Epoch 00797: val_loss did not improve from 0.29054\n",
      "Epoch 798/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0405 - acc: 0.9883 - val_loss: 0.3261 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00798: val_loss did not improve from 0.29054\n",
      "Epoch 799/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0348 - acc: 0.9885 - val_loss: 0.3310 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00799: val_loss did not improve from 0.29054\n",
      "Epoch 800/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0422 - acc: 0.9877 - val_loss: 0.3373 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00800: val_loss did not improve from 0.29054\n",
      "Epoch 801/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0316 - acc: 0.9886 - val_loss: 0.3294 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00801: val_loss did not improve from 0.29054\n",
      "Epoch 802/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0359 - acc: 0.9884 - val_loss: 0.3175 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00802: val_loss did not improve from 0.29054\n",
      "Epoch 803/1000\n",
      "17840/17840 [==============================] - 2s 110us/step - loss: 0.0352 - acc: 0.9886 - val_loss: 0.3330 - val_acc: 0.9249\n",
      "\n",
      "Epoch 00803: val_loss did not improve from 0.29054\n",
      "Epoch 804/1000\n",
      "17840/17840 [==============================] - 2s 114us/step - loss: 0.0370 - acc: 0.9887 - val_loss: 0.3131 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00804: val_loss did not improve from 0.29054\n",
      "Epoch 805/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0347 - acc: 0.9889 - val_loss: 0.3089 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00805: val_loss did not improve from 0.29054\n",
      "Epoch 806/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0360 - acc: 0.9884 - val_loss: 0.3069 - val_acc: 0.9365\n",
      "\n",
      "Epoch 00806: val_loss did not improve from 0.29054\n",
      "Epoch 807/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0362 - acc: 0.9881 - val_loss: 0.3164 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00807: val_loss did not improve from 0.29054\n",
      "Epoch 808/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0353 - acc: 0.9886 - val_loss: 0.3341 - val_acc: 0.9254\n",
      "\n",
      "Epoch 00808: val_loss did not improve from 0.29054\n",
      "Epoch 809/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0422 - acc: 0.9868 - val_loss: 0.3284 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00809: val_loss did not improve from 0.29054\n",
      "Epoch 810/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0348 - acc: 0.9895 - val_loss: 0.3356 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00810: val_loss did not improve from 0.29054\n",
      "Epoch 811/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0336 - acc: 0.9896 - val_loss: 0.3145 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00811: val_loss did not improve from 0.29054\n",
      "Epoch 812/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0408 - acc: 0.9866 - val_loss: 0.3067 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00812: val_loss did not improve from 0.29054\n",
      "Epoch 813/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0327 - acc: 0.9887 - val_loss: 0.3383 - val_acc: 0.9228\n",
      "\n",
      "Epoch 00813: val_loss did not improve from 0.29054\n",
      "Epoch 814/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0375 - acc: 0.9882 - val_loss: 0.3280 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00814: val_loss did not improve from 0.29054\n",
      "Epoch 815/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0388 - acc: 0.9874 - val_loss: 0.3148 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00815: val_loss did not improve from 0.29054\n",
      "Epoch 816/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0390 - acc: 0.9878 - val_loss: 0.3379 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00816: val_loss did not improve from 0.29054\n",
      "Epoch 817/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0349 - acc: 0.9882 - val_loss: 0.3474 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00817: val_loss did not improve from 0.29054\n",
      "Epoch 818/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0348 - acc: 0.9878 - val_loss: 0.3319 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00818: val_loss did not improve from 0.29054\n",
      "Epoch 819/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0398 - acc: 0.9882 - val_loss: 0.3168 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00819: val_loss did not improve from 0.29054\n",
      "Epoch 820/1000\n",
      "17840/17840 [==============================] - 2s 108us/step - loss: 0.0410 - acc: 0.9871 - val_loss: 0.3277 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00820: val_loss did not improve from 0.29054\n",
      "Epoch 821/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0314 - acc: 0.9896 - val_loss: 0.3089 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00821: val_loss did not improve from 0.29054\n",
      "Epoch 822/1000\n",
      "17840/17840 [==============================] - 2s 108us/step - loss: 0.0353 - acc: 0.9893 - val_loss: 0.3479 - val_acc: 0.9244\n",
      "\n",
      "Epoch 00822: val_loss did not improve from 0.29054\n",
      "Epoch 823/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0334 - acc: 0.9901 - val_loss: 0.3299 - val_acc: 0.9319\n",
      "\n",
      "Epoch 00823: val_loss did not improve from 0.29054\n",
      "Epoch 824/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0385 - acc: 0.9885 - val_loss: 0.3531 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00824: val_loss did not improve from 0.29054\n",
      "Epoch 825/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0388 - acc: 0.9875 - val_loss: 0.3572 - val_acc: 0.9223\n",
      "\n",
      "Epoch 00825: val_loss did not improve from 0.29054\n",
      "Epoch 826/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0394 - acc: 0.9871 - val_loss: 0.3342 - val_acc: 0.9254\n",
      "\n",
      "Epoch 00826: val_loss did not improve from 0.29054\n",
      "Epoch 827/1000\n",
      "17840/17840 [==============================] - 2s 112us/step - loss: 0.0314 - acc: 0.9892 - val_loss: 0.3295 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00827: val_loss did not improve from 0.29054\n",
      "Epoch 828/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0340 - acc: 0.9885 - val_loss: 0.3430 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00828: val_loss did not improve from 0.29054\n",
      "Epoch 829/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0360 - acc: 0.9889 - val_loss: 0.3315 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00829: val_loss did not improve from 0.29054\n",
      "Epoch 830/1000\n",
      "17840/17840 [==============================] - 2s 108us/step - loss: 0.0397 - acc: 0.9873 - val_loss: 0.3149 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00830: val_loss did not improve from 0.29054\n",
      "Epoch 831/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0348 - acc: 0.9881 - val_loss: 0.3239 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00831: val_loss did not improve from 0.29054\n",
      "Epoch 832/1000\n",
      "17840/17840 [==============================] - 2s 113us/step - loss: 0.0383 - acc: 0.9879 - val_loss: 0.3274 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00832: val_loss did not improve from 0.29054\n",
      "Epoch 833/1000\n",
      "17840/17840 [==============================] - 2s 116us/step - loss: 0.0367 - acc: 0.9869 - val_loss: 0.3164 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00833: val_loss did not improve from 0.29054\n",
      "Epoch 834/1000\n",
      "17840/17840 [==============================] - 2s 121us/step - loss: 0.0393 - acc: 0.9878 - val_loss: 0.3145 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00834: val_loss did not improve from 0.29054\n",
      "Epoch 835/1000\n",
      "17840/17840 [==============================] - 2s 109us/step - loss: 0.0384 - acc: 0.9881 - val_loss: 0.3363 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00835: val_loss did not improve from 0.29054\n",
      "Epoch 836/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0358 - acc: 0.9899 - val_loss: 0.3358 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00836: val_loss did not improve from 0.29054\n",
      "Epoch 837/1000\n",
      "17840/17840 [==============================] - 2s 109us/step - loss: 0.0383 - acc: 0.9872 - val_loss: 0.3221 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00837: val_loss did not improve from 0.29054\n",
      "Epoch 838/1000\n",
      "17840/17840 [==============================] - 2s 108us/step - loss: 0.0308 - acc: 0.9903 - val_loss: 0.3365 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00838: val_loss did not improve from 0.29054\n",
      "Epoch 839/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0359 - acc: 0.9893 - val_loss: 0.3300 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00839: val_loss did not improve from 0.29054\n",
      "Epoch 840/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0337 - acc: 0.9889 - val_loss: 0.3213 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00840: val_loss did not improve from 0.29054\n",
      "Epoch 841/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0386 - acc: 0.9867 - val_loss: 0.3446 - val_acc: 0.9244\n",
      "\n",
      "Epoch 00841: val_loss did not improve from 0.29054\n",
      "Epoch 842/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0407 - acc: 0.9863 - val_loss: 0.3271 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00842: val_loss did not improve from 0.29054\n",
      "Epoch 843/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0362 - acc: 0.9878 - val_loss: 0.3165 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00843: val_loss did not improve from 0.29054\n",
      "Epoch 844/1000\n",
      "17840/17840 [==============================] - 2s 108us/step - loss: 0.0403 - acc: 0.9876 - val_loss: 0.3252 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00844: val_loss did not improve from 0.29054\n",
      "Epoch 845/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0345 - acc: 0.9885 - val_loss: 0.3277 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00845: val_loss did not improve from 0.29054\n",
      "Epoch 846/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0310 - acc: 0.9899 - val_loss: 0.3349 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00846: val_loss did not improve from 0.29054\n",
      "Epoch 847/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0345 - acc: 0.9897 - val_loss: 0.3330 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00847: val_loss did not improve from 0.29054\n",
      "Epoch 848/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0368 - acc: 0.9877 - val_loss: 0.3267 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00848: val_loss did not improve from 0.29054\n",
      "Epoch 849/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0370 - acc: 0.9890 - val_loss: 0.3214 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00849: val_loss did not improve from 0.29054\n",
      "Epoch 850/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0345 - acc: 0.9886 - val_loss: 0.3300 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00850: val_loss did not improve from 0.29054\n",
      "Epoch 851/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0354 - acc: 0.9893 - val_loss: 0.3350 - val_acc: 0.9319\n",
      "\n",
      "Epoch 00851: val_loss did not improve from 0.29054\n",
      "Epoch 852/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0348 - acc: 0.9882 - val_loss: 0.3425 - val_acc: 0.9319\n",
      "\n",
      "Epoch 00852: val_loss did not improve from 0.29054\n",
      "Epoch 853/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0401 - acc: 0.9874 - val_loss: 0.3365 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00853: val_loss did not improve from 0.29054\n",
      "Epoch 854/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0427 - acc: 0.9862 - val_loss: 0.3296 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00854: val_loss did not improve from 0.29054\n",
      "Epoch 855/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0343 - acc: 0.9892 - val_loss: 0.3337 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00855: val_loss did not improve from 0.29054\n",
      "Epoch 856/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0400 - acc: 0.9877 - val_loss: 0.3027 - val_acc: 0.9355\n",
      "\n",
      "Epoch 00856: val_loss did not improve from 0.29054\n",
      "Epoch 857/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0473 - acc: 0.9851 - val_loss: 0.3089 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00857: val_loss did not improve from 0.29054\n",
      "Epoch 858/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0435 - acc: 0.9879 - val_loss: 0.3210 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00858: val_loss did not improve from 0.29054\n",
      "Epoch 859/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0286 - acc: 0.9916 - val_loss: 0.3203 - val_acc: 0.9329\n",
      "\n",
      "Epoch 00859: val_loss did not improve from 0.29054\n",
      "Epoch 860/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0296 - acc: 0.9908 - val_loss: 0.3267 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00860: val_loss did not improve from 0.29054\n",
      "Epoch 861/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0322 - acc: 0.9897 - val_loss: 0.3292 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00861: val_loss did not improve from 0.29054\n",
      "Epoch 862/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0391 - acc: 0.9871 - val_loss: 0.3164 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00862: val_loss did not improve from 0.29054\n",
      "Epoch 863/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0355 - acc: 0.9887 - val_loss: 0.3403 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00863: val_loss did not improve from 0.29054\n",
      "Epoch 864/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0329 - acc: 0.9905 - val_loss: 0.3291 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00864: val_loss did not improve from 0.29054\n",
      "Epoch 865/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0373 - acc: 0.9877 - val_loss: 0.3428 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00865: val_loss did not improve from 0.29054\n",
      "Epoch 866/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0406 - acc: 0.9875 - val_loss: 0.3372 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00866: val_loss did not improve from 0.29054\n",
      "Epoch 867/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0442 - acc: 0.9873 - val_loss: 0.3150 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00867: val_loss did not improve from 0.29054\n",
      "Epoch 868/1000\n",
      "17840/17840 [==============================] - 2s 129us/step - loss: 0.0364 - acc: 0.9889 - val_loss: 0.3260 - val_acc: 0.9324\n",
      "\n",
      "Epoch 00868: val_loss did not improve from 0.29054\n",
      "Epoch 869/1000\n",
      "17840/17840 [==============================] - 2s 115us/step - loss: 0.0355 - acc: 0.9885 - val_loss: 0.3375 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00869: val_loss did not improve from 0.29054\n",
      "Epoch 870/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0335 - acc: 0.9890 - val_loss: 0.3381 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00870: val_loss did not improve from 0.29054\n",
      "Epoch 871/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0371 - acc: 0.9889 - val_loss: 0.3225 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00871: val_loss did not improve from 0.29054\n",
      "Epoch 872/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0365 - acc: 0.9881 - val_loss: 0.3150 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00872: val_loss did not improve from 0.29054\n",
      "Epoch 873/1000\n",
      "17840/17840 [==============================] - 2s 109us/step - loss: 0.0378 - acc: 0.9880 - val_loss: 0.3227 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00873: val_loss did not improve from 0.29054\n",
      "Epoch 874/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0351 - acc: 0.9892 - val_loss: 0.3245 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00874: val_loss did not improve from 0.29054\n",
      "Epoch 875/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0340 - acc: 0.9890 - val_loss: 0.3364 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00875: val_loss did not improve from 0.29054\n",
      "Epoch 876/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0358 - acc: 0.9881 - val_loss: 0.3333 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00876: val_loss did not improve from 0.29054\n",
      "Epoch 877/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0357 - acc: 0.9891 - val_loss: 0.3352 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00877: val_loss did not improve from 0.29054\n",
      "Epoch 878/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0380 - acc: 0.9878 - val_loss: 0.3105 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00878: val_loss did not improve from 0.29054\n",
      "Epoch 879/1000\n",
      "17840/17840 [==============================] - 2s 109us/step - loss: 0.0323 - acc: 0.9895 - val_loss: 0.3204 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00879: val_loss did not improve from 0.29054\n",
      "Epoch 880/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0343 - acc: 0.9881 - val_loss: 0.3346 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00880: val_loss did not improve from 0.29054\n",
      "Epoch 881/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0338 - acc: 0.9897 - val_loss: 0.3303 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00881: val_loss did not improve from 0.29054\n",
      "Epoch 882/1000\n",
      "17840/17840 [==============================] - 2s 109us/step - loss: 0.0454 - acc: 0.9860 - val_loss: 0.3070 - val_acc: 0.9319\n",
      "\n",
      "Epoch 00882: val_loss did not improve from 0.29054\n",
      "Epoch 883/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0334 - acc: 0.9888 - val_loss: 0.3147 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00883: val_loss did not improve from 0.29054\n",
      "Epoch 884/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0357 - acc: 0.9892 - val_loss: 0.3118 - val_acc: 0.9324\n",
      "\n",
      "Epoch 00884: val_loss did not improve from 0.29054\n",
      "Epoch 885/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0379 - acc: 0.9874 - val_loss: 0.3355 - val_acc: 0.9239\n",
      "\n",
      "Epoch 00885: val_loss did not improve from 0.29054\n",
      "Epoch 886/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0385 - acc: 0.9884 - val_loss: 0.3260 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00886: val_loss did not improve from 0.29054\n",
      "Epoch 887/1000\n",
      "17840/17840 [==============================] - 2s 99us/step - loss: 0.0341 - acc: 0.9888 - val_loss: 0.3196 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00887: val_loss did not improve from 0.29054\n",
      "Epoch 888/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0338 - acc: 0.9882 - val_loss: 0.3046 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00888: val_loss did not improve from 0.29054\n",
      "Epoch 889/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0325 - acc: 0.9893 - val_loss: 0.3279 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00889: val_loss did not improve from 0.29054\n",
      "Epoch 890/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0317 - acc: 0.9902 - val_loss: 0.3424 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00890: val_loss did not improve from 0.29054\n",
      "Epoch 891/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0434 - acc: 0.9864 - val_loss: 0.3405 - val_acc: 0.9244\n",
      "\n",
      "Epoch 00891: val_loss did not improve from 0.29054\n",
      "Epoch 892/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0393 - acc: 0.9877 - val_loss: 0.3273 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00892: val_loss did not improve from 0.29054\n",
      "Epoch 893/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0427 - acc: 0.9868 - val_loss: 0.3152 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00893: val_loss did not improve from 0.29054\n",
      "Epoch 894/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0330 - acc: 0.9890 - val_loss: 0.3176 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00894: val_loss did not improve from 0.29054\n",
      "Epoch 895/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0352 - acc: 0.9896 - val_loss: 0.3296 - val_acc: 0.9324\n",
      "\n",
      "Epoch 00895: val_loss did not improve from 0.29054\n",
      "Epoch 896/1000\n",
      "17840/17840 [==============================] - 2s 110us/step - loss: 0.0386 - acc: 0.9872 - val_loss: 0.3221 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00896: val_loss did not improve from 0.29054\n",
      "Epoch 897/1000\n",
      "17840/17840 [==============================] - 2s 110us/step - loss: 0.0334 - acc: 0.9895 - val_loss: 0.3271 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00897: val_loss did not improve from 0.29054\n",
      "Epoch 898/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0368 - acc: 0.9887 - val_loss: 0.3203 - val_acc: 0.9254\n",
      "\n",
      "Epoch 00898: val_loss did not improve from 0.29054\n",
      "Epoch 899/1000\n",
      "17840/17840 [==============================] - 2s 109us/step - loss: 0.0435 - acc: 0.9859 - val_loss: 0.3284 - val_acc: 0.9249\n",
      "\n",
      "Epoch 00899: val_loss did not improve from 0.29054\n",
      "Epoch 900/1000\n",
      "17840/17840 [==============================] - 2s 109us/step - loss: 0.0385 - acc: 0.9878 - val_loss: 0.3220 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00900: val_loss did not improve from 0.29054\n",
      "Epoch 901/1000\n",
      "17840/17840 [==============================] - 2s 111us/step - loss: 0.0298 - acc: 0.9904 - val_loss: 0.3189 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00901: val_loss did not improve from 0.29054\n",
      "Epoch 902/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0371 - acc: 0.9881 - val_loss: 0.3174 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00902: val_loss did not improve from 0.29054\n",
      "Epoch 903/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0368 - acc: 0.9876 - val_loss: 0.3230 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00903: val_loss did not improve from 0.29054\n",
      "Epoch 904/1000\n",
      "17840/17840 [==============================] - 2s 109us/step - loss: 0.0396 - acc: 0.9876 - val_loss: 0.3163 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00904: val_loss did not improve from 0.29054\n",
      "Epoch 905/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0363 - acc: 0.9883 - val_loss: 0.3126 - val_acc: 0.9324\n",
      "\n",
      "Epoch 00905: val_loss did not improve from 0.29054\n",
      "Epoch 906/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0348 - acc: 0.9891 - val_loss: 0.3226 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00906: val_loss did not improve from 0.29054\n",
      "Epoch 907/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0313 - acc: 0.9900 - val_loss: 0.3285 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00907: val_loss did not improve from 0.29054\n",
      "Epoch 908/1000\n",
      "17840/17840 [==============================] - 2s 108us/step - loss: 0.0365 - acc: 0.9890 - val_loss: 0.3251 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00908: val_loss did not improve from 0.29054\n",
      "Epoch 909/1000\n",
      "17840/17840 [==============================] - 2s 108us/step - loss: 0.0417 - acc: 0.9881 - val_loss: 0.3305 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00909: val_loss did not improve from 0.29054\n",
      "Epoch 910/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0370 - acc: 0.9888 - val_loss: 0.3233 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00910: val_loss did not improve from 0.29054\n",
      "Epoch 911/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0337 - acc: 0.9895 - val_loss: 0.3281 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00911: val_loss did not improve from 0.29054\n",
      "Epoch 912/1000\n",
      "17840/17840 [==============================] - 2s 111us/step - loss: 0.0315 - acc: 0.9896 - val_loss: 0.3096 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00912: val_loss did not improve from 0.29054\n",
      "Epoch 913/1000\n",
      "17840/17840 [==============================] - 2s 110us/step - loss: 0.0381 - acc: 0.9879 - val_loss: 0.3172 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00913: val_loss did not improve from 0.29054\n",
      "Epoch 914/1000\n",
      "17840/17840 [==============================] - 2s 110us/step - loss: 0.0331 - acc: 0.9889 - val_loss: 0.3254 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00914: val_loss did not improve from 0.29054\n",
      "Epoch 915/1000\n",
      "17840/17840 [==============================] - 2s 112us/step - loss: 0.0395 - acc: 0.9873 - val_loss: 0.3270 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00915: val_loss did not improve from 0.29054\n",
      "Epoch 916/1000\n",
      "17840/17840 [==============================] - 2s 109us/step - loss: 0.0284 - acc: 0.9913 - val_loss: 0.3429 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00916: val_loss did not improve from 0.29054\n",
      "Epoch 917/1000\n",
      "17840/17840 [==============================] - 2s 110us/step - loss: 0.0360 - acc: 0.9894 - val_loss: 0.3264 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00917: val_loss did not improve from 0.29054\n",
      "Epoch 918/1000\n",
      "17840/17840 [==============================] - 2s 112us/step - loss: 0.0314 - acc: 0.9900 - val_loss: 0.3314 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00918: val_loss did not improve from 0.29054\n",
      "Epoch 919/1000\n",
      "17840/17840 [==============================] - 2s 112us/step - loss: 0.0377 - acc: 0.9885 - val_loss: 0.3228 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00919: val_loss did not improve from 0.29054\n",
      "Epoch 920/1000\n",
      "17840/17840 [==============================] - 2s 109us/step - loss: 0.0320 - acc: 0.9891 - val_loss: 0.3355 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00920: val_loss did not improve from 0.29054\n",
      "Epoch 921/1000\n",
      "17840/17840 [==============================] - 2s 109us/step - loss: 0.0360 - acc: 0.9883 - val_loss: 0.3183 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00921: val_loss did not improve from 0.29054\n",
      "Epoch 922/1000\n",
      "17840/17840 [==============================] - 2s 112us/step - loss: 0.0342 - acc: 0.9895 - val_loss: 0.3361 - val_acc: 0.9329\n",
      "\n",
      "Epoch 00922: val_loss did not improve from 0.29054\n",
      "Epoch 923/1000\n",
      "17840/17840 [==============================] - 2s 108us/step - loss: 0.0384 - acc: 0.9879 - val_loss: 0.3305 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00923: val_loss did not improve from 0.29054\n",
      "Epoch 924/1000\n",
      "17840/17840 [==============================] - 2s 112us/step - loss: 0.0365 - acc: 0.9879 - val_loss: 0.3237 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00924: val_loss did not improve from 0.29054\n",
      "Epoch 925/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0317 - acc: 0.9891 - val_loss: 0.3381 - val_acc: 0.9339\n",
      "\n",
      "Epoch 00925: val_loss did not improve from 0.29054\n",
      "Epoch 926/1000\n",
      "17840/17840 [==============================] - 2s 100us/step - loss: 0.0344 - acc: 0.9888 - val_loss: 0.3347 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00926: val_loss did not improve from 0.29054\n",
      "Epoch 927/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0399 - acc: 0.9877 - val_loss: 0.3246 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00927: val_loss did not improve from 0.29054\n",
      "Epoch 928/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0356 - acc: 0.9882 - val_loss: 0.3245 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00928: val_loss did not improve from 0.29054\n",
      "Epoch 929/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0385 - acc: 0.9885 - val_loss: 0.3229 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00929: val_loss did not improve from 0.29054\n",
      "Epoch 930/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0394 - acc: 0.9876 - val_loss: 0.3087 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00930: val_loss did not improve from 0.29054\n",
      "Epoch 931/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0334 - acc: 0.9889 - val_loss: 0.3349 - val_acc: 0.9244\n",
      "\n",
      "Epoch 00931: val_loss did not improve from 0.29054\n",
      "Epoch 932/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0323 - acc: 0.9900 - val_loss: 0.3309 - val_acc: 0.9239\n",
      "\n",
      "Epoch 00932: val_loss did not improve from 0.29054\n",
      "Epoch 933/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0319 - acc: 0.9902 - val_loss: 0.3189 - val_acc: 0.9339\n",
      "\n",
      "Epoch 00933: val_loss did not improve from 0.29054\n",
      "Epoch 934/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0359 - acc: 0.9888 - val_loss: 0.3228 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00934: val_loss did not improve from 0.29054\n",
      "Epoch 935/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0378 - acc: 0.9877 - val_loss: 0.3382 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00935: val_loss did not improve from 0.29054\n",
      "Epoch 936/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0391 - acc: 0.9877 - val_loss: 0.3195 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00936: val_loss did not improve from 0.29054\n",
      "Epoch 937/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0360 - acc: 0.9880 - val_loss: 0.3104 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00937: val_loss did not improve from 0.29054\n",
      "Epoch 938/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0364 - acc: 0.9879 - val_loss: 0.3187 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00938: val_loss did not improve from 0.29054\n",
      "Epoch 939/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0382 - acc: 0.9883 - val_loss: 0.3368 - val_acc: 0.9239\n",
      "\n",
      "Epoch 00939: val_loss did not improve from 0.29054\n",
      "Epoch 940/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0340 - acc: 0.9891 - val_loss: 0.3440 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00940: val_loss did not improve from 0.29054\n",
      "Epoch 941/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0312 - acc: 0.9895 - val_loss: 0.3385 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00941: val_loss did not improve from 0.29054\n",
      "Epoch 942/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0284 - acc: 0.9902 - val_loss: 0.3325 - val_acc: 0.9324\n",
      "\n",
      "Epoch 00942: val_loss did not improve from 0.29054\n",
      "Epoch 943/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0345 - acc: 0.9895 - val_loss: 0.3325 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00943: val_loss did not improve from 0.29054\n",
      "Epoch 944/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0347 - acc: 0.9886 - val_loss: 0.3442 - val_acc: 0.9254\n",
      "\n",
      "Epoch 00944: val_loss did not improve from 0.29054\n",
      "Epoch 945/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0338 - acc: 0.9889 - val_loss: 0.3333 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00945: val_loss did not improve from 0.29054\n",
      "Epoch 946/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0330 - acc: 0.9890 - val_loss: 0.3309 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00946: val_loss did not improve from 0.29054\n",
      "Epoch 947/1000\n",
      "17840/17840 [==============================] - 2s 125us/step - loss: 0.0370 - acc: 0.9879 - val_loss: 0.3371 - val_acc: 0.9249\n",
      "\n",
      "Epoch 00947: val_loss did not improve from 0.29054\n",
      "Epoch 948/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0309 - acc: 0.9900 - val_loss: 0.3386 - val_acc: 0.9319\n",
      "\n",
      "Epoch 00948: val_loss did not improve from 0.29054\n",
      "Epoch 949/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0384 - acc: 0.9883 - val_loss: 0.3393 - val_acc: 0.9244\n",
      "\n",
      "Epoch 00949: val_loss did not improve from 0.29054\n",
      "Epoch 950/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0351 - acc: 0.9895 - val_loss: 0.3489 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00950: val_loss did not improve from 0.29054\n",
      "Epoch 951/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0353 - acc: 0.9892 - val_loss: 0.3328 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00951: val_loss did not improve from 0.29054\n",
      "Epoch 952/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0431 - acc: 0.9860 - val_loss: 0.3162 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00952: val_loss did not improve from 0.29054\n",
      "Epoch 953/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0398 - acc: 0.9876 - val_loss: 0.3179 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00953: val_loss did not improve from 0.29054\n",
      "Epoch 954/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0339 - acc: 0.9885 - val_loss: 0.3255 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00954: val_loss did not improve from 0.29054\n",
      "Epoch 955/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0362 - acc: 0.9889 - val_loss: 0.3303 - val_acc: 0.9254\n",
      "\n",
      "Epoch 00955: val_loss did not improve from 0.29054\n",
      "Epoch 956/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0310 - acc: 0.9900 - val_loss: 0.3416 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00956: val_loss did not improve from 0.29054\n",
      "Epoch 957/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0303 - acc: 0.9903 - val_loss: 0.3311 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00957: val_loss did not improve from 0.29054\n",
      "Epoch 958/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0328 - acc: 0.9891 - val_loss: 0.3407 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00958: val_loss did not improve from 0.29054\n",
      "Epoch 959/1000\n",
      "17840/17840 [==============================] - 2s 110us/step - loss: 0.0403 - acc: 0.9867 - val_loss: 0.3237 - val_acc: 0.9314\n",
      "\n",
      "Epoch 00959: val_loss did not improve from 0.29054\n",
      "Epoch 960/1000\n",
      "17840/17840 [==============================] - 2s 116us/step - loss: 0.0335 - acc: 0.9891 - val_loss: 0.3177 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00960: val_loss did not improve from 0.29054\n",
      "Epoch 961/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0394 - acc: 0.9876 - val_loss: 0.3250 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00961: val_loss did not improve from 0.29054\n",
      "Epoch 962/1000\n",
      "17840/17840 [==============================] - 2s 116us/step - loss: 0.0429 - acc: 0.9874 - val_loss: 0.3143 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00962: val_loss did not improve from 0.29054\n",
      "Epoch 963/1000\n",
      "17840/17840 [==============================] - 2s 110us/step - loss: 0.0395 - acc: 0.9884 - val_loss: 0.3300 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00963: val_loss did not improve from 0.29054\n",
      "Epoch 964/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0335 - acc: 0.9897 - val_loss: 0.3509 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00964: val_loss did not improve from 0.29054\n",
      "Epoch 965/1000\n",
      "17840/17840 [==============================] - 2s 108us/step - loss: 0.0371 - acc: 0.9890 - val_loss: 0.3273 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00965: val_loss did not improve from 0.29054\n",
      "Epoch 966/1000\n",
      "17840/17840 [==============================] - 2s 107us/step - loss: 0.0328 - acc: 0.9883 - val_loss: 0.3198 - val_acc: 0.9239\n",
      "\n",
      "Epoch 00966: val_loss did not improve from 0.29054\n",
      "Epoch 967/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0344 - acc: 0.9886 - val_loss: 0.3401 - val_acc: 0.9249\n",
      "\n",
      "Epoch 00967: val_loss did not improve from 0.29054\n",
      "Epoch 968/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0353 - acc: 0.9886 - val_loss: 0.3275 - val_acc: 0.9264\n",
      "\n",
      "Epoch 00968: val_loss did not improve from 0.29054\n",
      "Epoch 969/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0354 - acc: 0.9879 - val_loss: 0.3233 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00969: val_loss did not improve from 0.29054\n",
      "Epoch 970/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0469 - acc: 0.9862 - val_loss: 0.3199 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00970: val_loss did not improve from 0.29054\n",
      "Epoch 971/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0375 - acc: 0.9883 - val_loss: 0.3166 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00971: val_loss did not improve from 0.29054\n",
      "Epoch 972/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0346 - acc: 0.9892 - val_loss: 0.3253 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00972: val_loss did not improve from 0.29054\n",
      "Epoch 973/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0326 - acc: 0.9899 - val_loss: 0.3346 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00973: val_loss did not improve from 0.29054\n",
      "Epoch 974/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0405 - acc: 0.9883 - val_loss: 0.3247 - val_acc: 0.9329\n",
      "\n",
      "Epoch 00974: val_loss did not improve from 0.29054\n",
      "Epoch 975/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0361 - acc: 0.9879 - val_loss: 0.3236 - val_acc: 0.9254\n",
      "\n",
      "Epoch 00975: val_loss did not improve from 0.29054\n",
      "Epoch 976/1000\n",
      "17840/17840 [==============================] - 2s 101us/step - loss: 0.0358 - acc: 0.9878 - val_loss: 0.3225 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00976: val_loss did not improve from 0.29054\n",
      "Epoch 977/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0322 - acc: 0.9899 - val_loss: 0.3353 - val_acc: 0.9249\n",
      "\n",
      "Epoch 00977: val_loss did not improve from 0.29054\n",
      "Epoch 978/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0409 - acc: 0.9869 - val_loss: 0.3417 - val_acc: 0.9244\n",
      "\n",
      "Epoch 00978: val_loss did not improve from 0.29054\n",
      "Epoch 979/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0354 - acc: 0.9904 - val_loss: 0.3011 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00979: val_loss did not improve from 0.29054\n",
      "Epoch 980/1000\n",
      "17840/17840 [==============================] - 2s 102us/step - loss: 0.0393 - acc: 0.9876 - val_loss: 0.3145 - val_acc: 0.9319\n",
      "\n",
      "Epoch 00980: val_loss did not improve from 0.29054\n",
      "Epoch 981/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0393 - acc: 0.9877 - val_loss: 0.3295 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00981: val_loss did not improve from 0.29054\n",
      "Epoch 982/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0334 - acc: 0.9887 - val_loss: 0.3414 - val_acc: 0.9254\n",
      "\n",
      "Epoch 00982: val_loss did not improve from 0.29054\n",
      "Epoch 983/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0329 - acc: 0.9890 - val_loss: 0.3316 - val_acc: 0.9244\n",
      "\n",
      "Epoch 00983: val_loss did not improve from 0.29054\n",
      "Epoch 984/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0322 - acc: 0.9896 - val_loss: 0.3307 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00984: val_loss did not improve from 0.29054\n",
      "Epoch 985/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0413 - acc: 0.9877 - val_loss: 0.3310 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00985: val_loss did not improve from 0.29054\n",
      "Epoch 986/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0307 - acc: 0.9901 - val_loss: 0.3340 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00986: val_loss did not improve from 0.29054\n",
      "Epoch 987/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0342 - acc: 0.9886 - val_loss: 0.3457 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00987: val_loss did not improve from 0.29054\n",
      "Epoch 988/1000\n",
      "17840/17840 [==============================] - 2s 103us/step - loss: 0.0369 - acc: 0.9877 - val_loss: 0.3410 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00988: val_loss did not improve from 0.29054\n",
      "Epoch 989/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0394 - acc: 0.9881 - val_loss: 0.3369 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00989: val_loss did not improve from 0.29054\n",
      "Epoch 990/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0401 - acc: 0.9887 - val_loss: 0.3161 - val_acc: 0.9334\n",
      "\n",
      "Epoch 00990: val_loss did not improve from 0.29054\n",
      "Epoch 991/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0341 - acc: 0.9891 - val_loss: 0.3226 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00991: val_loss did not improve from 0.29054\n",
      "Epoch 992/1000\n",
      "17840/17840 [==============================] - 2s 105us/step - loss: 0.0345 - acc: 0.9886 - val_loss: 0.3352 - val_acc: 0.9274\n",
      "\n",
      "Epoch 00992: val_loss did not improve from 0.29054\n",
      "Epoch 993/1000\n",
      "17840/17840 [==============================] - 2s 104us/step - loss: 0.0362 - acc: 0.9892 - val_loss: 0.3224 - val_acc: 0.9324\n",
      "\n",
      "Epoch 00993: val_loss did not improve from 0.29054\n",
      "Epoch 994/1000\n",
      "17840/17840 [==============================] - 2s 111us/step - loss: 0.0348 - acc: 0.9883 - val_loss: 0.3163 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00994: val_loss did not improve from 0.29054\n",
      "Epoch 995/1000\n",
      "17840/17840 [==============================] - 2s 111us/step - loss: 0.0350 - acc: 0.9888 - val_loss: 0.3109 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00995: val_loss did not improve from 0.29054\n",
      "Epoch 996/1000\n",
      "17840/17840 [==============================] - 2s 108us/step - loss: 0.0350 - acc: 0.9886 - val_loss: 0.3252 - val_acc: 0.9289\n",
      "\n",
      "Epoch 00996: val_loss did not improve from 0.29054\n",
      "Epoch 997/1000\n",
      "17840/17840 [==============================] - 2s 108us/step - loss: 0.0394 - acc: 0.9876 - val_loss: 0.3228 - val_acc: 0.9279\n",
      "\n",
      "Epoch 00997: val_loss did not improve from 0.29054\n",
      "Epoch 998/1000\n",
      "17840/17840 [==============================] - 2s 111us/step - loss: 0.0342 - acc: 0.9886 - val_loss: 0.3474 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00998: val_loss did not improve from 0.29054\n",
      "Epoch 999/1000\n",
      "17840/17840 [==============================] - 2s 111us/step - loss: 0.0357 - acc: 0.9888 - val_loss: 0.3155 - val_acc: 0.9294\n",
      "\n",
      "Epoch 00999: val_loss did not improve from 0.29054\n",
      "Epoch 1000/1000\n",
      "17840/17840 [==============================] - 2s 106us/step - loss: 0.0392 - acc: 0.9868 - val_loss: 0.3236 - val_acc: 0.9249\n",
      "\n",
      "Epoch 01000: val_loss did not improve from 0.29054\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "# Tensorboard\n",
    "tensorboard = TensorBoard(log_dir=\"run\")\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "filepath=\"weights.best.madar6_1000min.hdf5\"\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "#callbacks_list = [checkpoint]\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# let's fit the data\n",
    "# history variable will help us to plot results later\n",
    "history = model.fit(X_train,Y_train,\n",
    "                  epochs=1000,\n",
    "                  validation_split=0.1,\n",
    "                  batch_size=BATCH_SIZE,\n",
    "                  callbacks=callbacks_list,#[tensorboard],\n",
    "                  shuffle=True#,\n",
    "                  #verbose=2\n",
    "                   )\n",
    "\n",
    "\n",
    "model.save_weights(filepath) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "plt.clf()   # clear figure\n",
    "acc_values = history_dict['acc']\n",
    "val_acc_values = history_dict['val_acc']\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2170/2170 [==============================] - 0s 71us/step\n",
      "acc: 91.15%\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(filepath)\n",
    "scores = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92       343\n",
      "           1       0.97      0.98      0.98       392\n",
      "           2       0.87      0.86      0.87       365\n",
      "           3       0.90      0.87      0.88       337\n",
      "           4       0.94      0.93      0.94       381\n",
      "           5       0.90      0.87      0.88       352\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      2170\n",
      "   macro avg       0.91      0.91      0.91      2170\n",
      "weighted avg       0.91      0.91      0.91      2170\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# and now we will prepare data for scikit-learn confusion matrix and classification report\n",
    "Y_pred = model.predict_classes(X_test)\n",
    "Y_pred = keras.utils.to_categorical(Y_pred, num_classes=len(LANGUAGES_DICT))\n",
    "LABELS =  list(LANGUAGES_DICT.keys())\n",
    "# Plot confusion matrix \n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from support import print_confusion_matrix\n",
    "\n",
    "cnf_matrix = confusion_matrix(np.argmax(Y_pred,axis=1), np.argmax(Y_test,axis=1))\n",
    "_ = print_confusion_matrix(cnf_matrix, LABELS)\n",
    "print(classification_report(np.argmax(Y_test,axis=1), np.argmax(Y_pred,axis=1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eed5b3877a84d86942966dd96c4e9a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Textarea(value='', description='TEXT', placeholder='ازيك عامل ايه انا سألت عنك كتير اوي'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact_manual\n",
    "from ipywidgets import widgets\n",
    "from support import clean_text\n",
    "\n",
    "\n",
    "def get_prediction(TEXT):\n",
    "    if len(TEXT) < MAX_LEN:\n",
    "        print(\"Text has to be at least {} chars long, but it is {}/{}\".format(MAX_LEN, len(TEXT), MAX_LEN))\n",
    "        return(-1)\n",
    "    # Data cleaning\n",
    "    print(\" it is {}/{}\".format(len(TEXT), MAX_LEN))\n",
    "    cleaned_text = clean_text(TEXT)\n",
    "    \n",
    "    # Get the MAX_LEN char\n",
    "    input_row = get_input_row(cleaned_text, 0, MAX_LEN, alphabet)\n",
    "    \n",
    "    # Data preprocessing (Standardization)\n",
    "    test_array = standard_scaler.transform([input_row])\n",
    "    \n",
    "    raw_score = model.predict(test_array)\n",
    "    pred_idx= np.argmax(raw_score, axis=1)[0]\n",
    "    score = raw_score[0][pred_idx]*100\n",
    "    \n",
    "    # Prediction\n",
    "    prediction = LABELS[model.predict_classes(test_array)[0]]\n",
    "    print('TEXT:', TEXT, '\\nPREDICTION:', prediction.upper(), '\\nSCORE:', score)\n",
    "\n",
    "interact_manual(get_prediction, TEXT=widgets.Textarea(placeholder='ازيك عامل ايه انا سألت عنك كتير اوي'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
